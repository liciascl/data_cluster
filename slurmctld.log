[2024-01-31T13:53:45.673] error: chdir(/var/log): Permission denied
[2024-01-31T13:53:45.673] error: Configured MailProg is invalid
[2024-01-31T13:53:45.673] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T13:53:45.685] No memory enforcing mechanism configured.
[2024-01-31T13:53:45.688] error: Could not open node state file /var/spool/slurmctld/node_state: No such file or directory
[2024-01-31T13:53:45.688] error: NOTE: Trying backup state save file. Information may be lost!
[2024-01-31T13:53:45.688] No node state file (/var/spool/slurmctld/node_state.old) to recover
[2024-01-31T13:53:45.688] error: Could not open job state file /var/spool/slurmctld/job_state: No such file or directory
[2024-01-31T13:53:45.688] error: NOTE: Trying backup state save file. Jobs may be lost!
[2024-01-31T13:53:45.688] No job state file (/var/spool/slurmctld/job_state.old) to recover
[2024-01-31T13:53:45.688] select/cons_tres: select_p_node_init: select/cons_tres SelectTypeParameters not specified, using default value: CR_Core_Memory
[2024-01-31T13:53:45.688] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T13:53:45.688] error: Could not open reservation state file /var/spool/slurmctld/resv_state: No such file or directory
[2024-01-31T13:53:45.688] error: NOTE: Trying backup state save file. Reservations may be lost
[2024-01-31T13:53:45.688] No reservation state file (/var/spool/slurmctld/resv_state.old) to recover
[2024-01-31T13:53:45.688] error: Could not open trigger state file /var/spool/slurmctld/trigger_state: No such file or directory
[2024-01-31T13:53:45.688] error: NOTE: Trying backup state save file. Triggers may be lost!
[2024-01-31T13:53:45.688] No trigger state file (/var/spool/slurmctld/trigger_state.old) to recover
[2024-01-31T13:53:45.688] read_slurm_conf: backup_controller not specified
[2024-01-31T13:53:45.688] Reinitializing job accounting state
[2024-01-31T13:53:45.688] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure
[2024-01-31T13:53:45.688] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T13:53:45.688] Running as primary controller
[2024-01-31T13:53:45.688] No parameter for mcs plugin, default values set
[2024-01-31T13:53:45.688] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T13:54:45.829] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-01-31T13:58:45.402] error: Nodes compute[00-03] not responding
[2024-01-31T13:58:45.402] error: Could not open job state file /var/spool/slurmctld/job_state: No such file or directory
[2024-01-31T13:58:45.402] error: NOTE: Trying backup state save file. Jobs may be lost!
[2024-01-31T13:58:45.402] No job state file (/var/spool/slurmctld/job_state.old) found
[2024-01-31T14:03:45.129] error: Nodes compute[00-03] not responding
[2024-01-31T14:08:45.802] error: Nodes compute[00-03] not responding
[2024-01-31T14:13:45.470] error: Nodes compute[00-03] not responding
[2024-01-31T14:18:45.230] error: Nodes compute[00-03] not responding
[2024-01-31T14:23:45.924] error: Nodes compute[00-03] not responding
[2024-01-31T14:28:45.636] error: Nodes compute[00-03] not responding
[2024-01-31T14:33:45.298] error: Nodes compute[00-03] not responding
[2024-01-31T14:38:45.007] error: Nodes compute[00-03] not responding
[2024-01-31T14:43:45.654] error: Nodes compute[00-03] not responding
[2024-01-31T14:48:45.231] error: Nodes compute[00-03] not responding
[2024-01-31T14:53:45.971] error: Nodes compute[00-03] not responding
[2024-01-31T14:58:45.695] error: Nodes compute[00-03] not responding
[2024-01-31T15:03:45.429] error: Nodes compute[00-03] not responding
[2024-01-31T15:05:07.188] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:05:07.235] Saving all slurm state
[2024-01-31T15:05:07.266] error: chdir(/var/log): Permission denied
[2024-01-31T15:05:07.266] error: Configured MailProg is invalid
[2024-01-31T15:05:07.266] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:05:07.268] No memory enforcing mechanism configured.
[2024-01-31T15:05:07.269] Recovered state of 4 nodes
[2024-01-31T15:05:07.269] Recovered information about 0 jobs
[2024-01-31T15:05:07.269] select/cons_tres: select_p_node_init: select/cons_tres SelectTypeParameters not specified, using default value: CR_Core_Memory
[2024-01-31T15:05:07.269] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:05:07.269] Recovered state of 0 reservations
[2024-01-31T15:05:07.269] read_slurm_conf: backup_controller not specified
[2024-01-31T15:05:07.269] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure
[2024-01-31T15:05:07.269] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:05:07.269] Running as primary controller
[2024-01-31T15:05:07.269] No parameter for mcs plugin, default values set
[2024-01-31T15:05:07.269] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:06:07.405] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-01-31T15:10:07.931] error: Nodes compute[00-03] not responding
[2024-01-31T15:15:07.672] error: Nodes compute[00-03] not responding
[2024-01-31T15:19:24.128] Node compute03 now responding
[2024-01-31T15:19:24.128] error: Setting node compute03 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:19:24.128] drain_nodes: node compute03 state set to DRAIN
[2024-01-31T15:19:24.128] error: _slurm_rpc_node_registration node=compute03: Invalid argument
[2024-01-31T15:19:24.128] Node compute02 now responding
[2024-01-31T15:19:24.128] error: Setting node compute02 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:19:24.128] drain_nodes: node compute02 state set to DRAIN
[2024-01-31T15:19:24.128] error: _slurm_rpc_node_registration node=compute02: Invalid argument
[2024-01-31T15:19:24.128] Node compute00 now responding
[2024-01-31T15:19:24.128] error: Setting node compute00 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:19:24.128] drain_nodes: node compute00 state set to DRAIN
[2024-01-31T15:19:24.128] error: _slurm_rpc_node_registration node=compute00: Invalid argument
[2024-01-31T15:19:24.128] Node compute01 now responding
[2024-01-31T15:19:24.128] error: Setting node compute01 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:19:24.128] drain_nodes: node compute01 state set to DRAIN
[2024-01-31T15:19:24.128] error: _slurm_rpc_node_registration node=compute01: Invalid argument
[2024-01-31T15:20:07.318] error: Nodes compute[00-03] not responding
[2024-01-31T15:24:29.631] error: _slurm_rpc_node_registration node=compute03: Invalid argument
[2024-01-31T15:24:29.632] error: _slurm_rpc_node_registration node=compute00: Invalid argument
[2024-01-31T15:24:29.644] error: _slurm_rpc_node_registration node=compute01: Invalid argument
[2024-01-31T15:24:29.650] error: _slurm_rpc_node_registration node=compute02: Invalid argument
[2024-01-31T15:24:58.562] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:24:58.618] Saving all slurm state
[2024-01-31T15:24:58.649] error: chdir(/var/log): Permission denied
[2024-01-31T15:24:58.649] error: Configured MailProg is invalid
[2024-01-31T15:24:58.649] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:24:58.651] No memory enforcing mechanism configured.
[2024-01-31T15:24:58.651] Recovered state of 4 nodes
[2024-01-31T15:24:58.651] Recovered information about 0 jobs
[2024-01-31T15:24:58.651] select/cons_tres: select_p_node_init: select/cons_tres SelectTypeParameters not specified, using default value: CR_Core_Memory
[2024-01-31T15:24:58.651] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:24:58.652] Recovered state of 0 reservations
[2024-01-31T15:24:58.652] read_slurm_conf: backup_controller not specified
[2024-01-31T15:24:58.652] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure
[2024-01-31T15:24:58.652] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:24:58.652] Running as primary controller
[2024-01-31T15:24:58.652] No parameter for mcs plugin, default values set
[2024-01-31T15:24:58.652] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:25:02.666] error: Setting node compute01 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:25:02.666] error: _slurm_rpc_node_registration node=compute01: Invalid argument
[2024-01-31T15:25:02.666] error: Setting node compute02 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:25:02.666] error: _slurm_rpc_node_registration node=compute02: Invalid argument
[2024-01-31T15:25:02.666] error: Setting node compute03 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:25:02.666] error: _slurm_rpc_node_registration node=compute03: Invalid argument
[2024-01-31T15:25:02.668] error: Setting node compute00 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:25:02.668] error: _slurm_rpc_node_registration node=compute00: Invalid argument
[2024-01-31T15:25:58.795] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-01-31T15:26:38.135] error: _slurm_rpc_node_registration node=compute00: Invalid argument
[2024-01-31T15:26:38.145] error: _slurm_rpc_node_registration node=compute01: Invalid argument
[2024-01-31T15:26:38.146] error: _slurm_rpc_node_registration node=compute02: Invalid argument
[2024-01-31T15:26:38.148] error: _slurm_rpc_node_registration node=compute03: Invalid argument
[2024-01-31T15:28:25.703] killing old slurmctld[31050]
[2024-01-31T15:28:25.704] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:28:25.740] Saving all slurm state
[2024-01-31T15:28:25.769] error: chdir(/var/log): Permission denied
[2024-01-31T15:28:25.769] error: Configured MailProg is invalid
[2024-01-31T15:28:25.769] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:28:25.771] No memory enforcing mechanism configured.
[2024-01-31T15:28:25.772] Recovered state of 4 nodes
[2024-01-31T15:28:25.772] Recovered information about 0 jobs
[2024-01-31T15:28:25.772] select/cons_tres: select_p_node_init: select/cons_tres SelectTypeParameters not specified, using default value: CR_Core_Memory
[2024-01-31T15:28:25.772] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:28:25.772] Recovered state of 0 reservations
[2024-01-31T15:28:25.772] read_slurm_conf: backup_controller not specified
[2024-01-31T15:28:25.772] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure
[2024-01-31T15:28:25.772] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:28:25.772] Running as primary controller
[2024-01-31T15:28:25.772] No parameter for mcs plugin, default values set
[2024-01-31T15:28:25.772] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:28:29.785] error: Setting node compute02 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:28:29.785] error: _slurm_rpc_node_registration node=compute02: Invalid argument
[2024-01-31T15:28:29.785] error: Setting node compute03 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:28:29.785] error: _slurm_rpc_node_registration node=compute03: Invalid argument
[2024-01-31T15:28:29.786] error: Setting node compute01 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:28:29.786] error: _slurm_rpc_node_registration node=compute01: Invalid argument
[2024-01-31T15:28:29.786] error: Setting node compute00 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:28:29.786] error: _slurm_rpc_node_registration node=compute00: Invalid argument
[2024-01-31T15:28:35.364] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:28:35.394] Saving all slurm state
[2024-01-31T15:28:49.198] error: chdir(/var/log): Permission denied
[2024-01-31T15:28:49.198] error: Configured MailProg is invalid
[2024-01-31T15:28:49.198] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:28:49.200] No memory enforcing mechanism configured.
[2024-01-31T15:28:49.221] Recovered state of 4 nodes
[2024-01-31T15:28:49.221] Recovered information about 0 jobs
[2024-01-31T15:28:49.221] select/cons_tres: select_p_node_init: select/cons_tres SelectTypeParameters not specified, using default value: CR_Core_Memory
[2024-01-31T15:28:49.221] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:28:49.221] Recovered state of 0 reservations
[2024-01-31T15:28:49.221] read_slurm_conf: backup_controller not specified
[2024-01-31T15:28:49.221] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure
[2024-01-31T15:28:49.221] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:28:49.221] Running as primary controller
[2024-01-31T15:28:49.221] No parameter for mcs plugin, default values set
[2024-01-31T15:28:49.221] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:28:53.234] error: Setting node compute02 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:28:53.234] error: _slurm_rpc_node_registration node=compute02: Invalid argument
[2024-01-31T15:28:53.234] error: Setting node compute03 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:28:53.234] error: _slurm_rpc_node_registration node=compute03: Invalid argument
[2024-01-31T15:28:53.235] error: Setting node compute01 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:28:53.235] error: _slurm_rpc_node_registration node=compute01: Invalid argument
[2024-01-31T15:28:53.235] error: Setting node compute00 state to INVAL with reason:Low socket*core*thread count, Low CPUs
[2024-01-31T15:28:53.235] error: _slurm_rpc_node_registration node=compute00: Invalid argument
[2024-01-31T15:29:49.360] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-01-31T15:32:03.475] killing old slurmctld[31174]
[2024-01-31T15:32:03.475] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:32:03.572] Saving all slurm state
[2024-01-31T15:32:03.602] error: chdir(/var/log): Permission denied
[2024-01-31T15:32:03.602] error: Configured MailProg is invalid
[2024-01-31T15:32:03.602] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:32:03.604] No memory enforcing mechanism configured.
[2024-01-31T15:32:03.605] Recovered state of 4 nodes
[2024-01-31T15:32:03.605] Recovered information about 0 jobs
[2024-01-31T15:32:03.605] select/cons_tres: select_p_node_init: select/cons_tres SelectTypeParameters not specified, using default value: CR_Core_Memory
[2024-01-31T15:32:03.605] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:32:03.605] Recovered state of 0 reservations
[2024-01-31T15:32:03.605] read_slurm_conf: backup_controller not specified
[2024-01-31T15:32:03.605] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure
[2024-01-31T15:32:03.605] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:32:03.605] Running as primary controller
[2024-01-31T15:32:03.605] No parameter for mcs plugin, default values set
[2024-01-31T15:32:03.605] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:32:06.406] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:32:06.412] Saving all slurm state
[2024-01-31T15:32:09.397] error: chdir(/var/log): Permission denied
[2024-01-31T15:32:09.397] error: Configured MailProg is invalid
[2024-01-31T15:32:09.397] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:32:09.399] No memory enforcing mechanism configured.
[2024-01-31T15:32:09.420] Recovered state of 4 nodes
[2024-01-31T15:32:09.420] Recovered information about 0 jobs
[2024-01-31T15:32:09.420] select/cons_tres: select_p_node_init: select/cons_tres SelectTypeParameters not specified, using default value: CR_Core_Memory
[2024-01-31T15:32:09.420] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:32:09.420] Recovered state of 0 reservations
[2024-01-31T15:32:09.420] read_slurm_conf: backup_controller not specified
[2024-01-31T15:32:09.420] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure
[2024-01-31T15:32:09.420] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:32:09.420] Running as primary controller
[2024-01-31T15:32:09.421] No parameter for mcs plugin, default values set
[2024-01-31T15:32:09.421] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:32:13.435] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:32:13.435] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:32:13.435] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:32:13.435] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:32:44.327] Processing Reconfiguration Request
[2024-01-31T15:32:44.328] No memory enforcing mechanism configured.
[2024-01-31T15:32:44.328] restoring original state of nodes
[2024-01-31T15:32:44.328] select/cons_tres: select_p_node_init: select/cons_tres SelectTypeParameters not specified, using default value: CR_Core_Memory
[2024-01-31T15:32:44.328] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:32:44.328] read_slurm_conf: backup_controller not specified
[2024-01-31T15:32:44.328] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure
[2024-01-31T15:32:44.328] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 1 partitions
[2024-01-31T15:32:44.328] No parameter for mcs plugin, default values set
[2024-01-31T15:32:44.328] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:32:44.328] _slurm_rpc_reconfigure_controller: completed usec=1008
[2024-01-31T15:32:44.498] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-01-31T15:36:53.801] update_node: node compute00 state set to IDLE
[2024-01-31T15:36:54.090] Node compute00 now responding
[2024-01-31T15:36:59.107] update_node: node compute01 state set to IDLE
[2024-01-31T15:37:00.105] Node compute01 now responding
[2024-01-31T15:37:03.187] update_node: node compute02 state set to IDLE
[2024-01-31T15:37:04.112] Node compute02 now responding
[2024-01-31T15:37:06.482] update_node: node compute03 state set to IDLE
[2024-01-31T15:37:07.120] Node compute03 now responding
[2024-01-31T15:49:23.944] killing old slurmctld[31242]
[2024-01-31T15:49:23.944] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:49:23.980] Saving all slurm state
[2024-01-31T15:49:24.008] error: chdir(/var/log): Permission denied
[2024-01-31T15:49:24.008] error: Configured MailProg is invalid
[2024-01-31T15:49:24.008] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:49:24.010] No memory enforcing mechanism configured.
[2024-01-31T15:49:24.010] Recovered state of 4 nodes
[2024-01-31T15:49:24.010] Recovered information about 0 jobs
[2024-01-31T15:49:24.010] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T15:49:24.010] Recovered state of 0 reservations
[2024-01-31T15:49:24.010] read_slurm_conf: backup_controller not specified
[2024-01-31T15:49:24.010] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-01-31T15:49:24.010] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T15:49:24.010] Running as primary controller
[2024-01-31T15:49:24.010] No parameter for mcs plugin, default values set
[2024-01-31T15:49:24.010] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:49:28.025] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:49:28.025] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:49:28.025] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:49:28.026] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:49:29.022] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-01-31T15:49:30.380] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:49:30.425] Saving all slurm state
[2024-01-31T15:49:37.939] error: chdir(/var/log): Permission denied
[2024-01-31T15:49:37.939] error: Configured MailProg is invalid
[2024-01-31T15:49:37.939] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:49:37.941] No memory enforcing mechanism configured.
[2024-01-31T15:49:37.962] Recovered state of 4 nodes
[2024-01-31T15:49:37.962] Recovered information about 0 jobs
[2024-01-31T15:49:37.962] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T15:49:37.962] Recovered state of 0 reservations
[2024-01-31T15:49:37.962] read_slurm_conf: backup_controller not specified
[2024-01-31T15:49:37.962] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-01-31T15:49:37.962] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T15:49:37.962] Running as primary controller
[2024-01-31T15:49:37.962] No parameter for mcs plugin, default values set
[2024-01-31T15:49:37.962] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:49:41.976] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:49:41.976] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:49:41.976] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:49:41.978] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:49:42.974] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-01-31T15:50:31.260] killing old slurmctld[31464]
[2024-01-31T15:50:31.260] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:50:31.281] Saving all slurm state
[2024-01-31T15:50:31.308] error: chdir(/var/log): Permission denied
[2024-01-31T15:50:31.308] error: Configured MailProg is invalid
[2024-01-31T15:50:31.308] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:50:31.309] No memory enforcing mechanism configured.
[2024-01-31T15:50:31.311] Recovered state of 4 nodes
[2024-01-31T15:50:31.311] Recovered information about 0 jobs
[2024-01-31T15:50:31.311] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T15:50:31.311] Recovered state of 0 reservations
[2024-01-31T15:50:31.311] read_slurm_conf: backup_controller not specified
[2024-01-31T15:50:31.311] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-01-31T15:50:31.311] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T15:50:31.311] Running as primary controller
[2024-01-31T15:50:31.311] No parameter for mcs plugin, default values set
[2024-01-31T15:50:31.311] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:50:34.893] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T15:50:34.920] Saving all slurm state
[2024-01-31T15:50:42.154] error: chdir(/var/log): Permission denied
[2024-01-31T15:50:42.154] error: Configured MailProg is invalid
[2024-01-31T15:50:42.154] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T15:50:42.156] No memory enforcing mechanism configured.
[2024-01-31T15:50:42.157] Recovered state of 4 nodes
[2024-01-31T15:50:42.157] Recovered information about 0 jobs
[2024-01-31T15:50:42.157] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T15:50:42.157] Recovered state of 0 reservations
[2024-01-31T15:50:42.157] read_slurm_conf: backup_controller not specified
[2024-01-31T15:50:42.157] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-01-31T15:50:42.157] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T15:50:42.157] Running as primary controller
[2024-01-31T15:50:42.157] No parameter for mcs plugin, default values set
[2024-01-31T15:50:42.157] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T15:50:46.171] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:50:46.171] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:50:46.172] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:50:46.172] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T15:50:47.169] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-01-31T16:22:26.884] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T16:22:26.884] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T16:22:26.884] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T16:22:26.885] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T16:55:46.969] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T16:55:46.970] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T16:55:46.970] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T16:55:46.971] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T17:29:06.891] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T17:29:06.891] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T17:29:06.891] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T17:29:06.892] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T18:02:26.092] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T18:02:26.092] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T18:02:26.093] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T18:02:26.094] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T18:35:46.189] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T18:35:46.189] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T18:35:46.190] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T18:35:46.191] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T19:09:06.147] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T19:09:06.147] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T19:09:06.147] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T19:09:06.148] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T19:42:26.164] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T19:42:26.165] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T19:42:26.165] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T19:42:26.167] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T20:07:21.141] Terminate signal (SIGINT or SIGTERM) received
[2024-01-31T20:07:21.208] Saving all slurm state
[2024-01-31T20:07:52.198] error: chdir(/var/log): Permission denied
[2024-01-31T20:07:52.198] error: Configured MailProg is invalid
[2024-01-31T20:07:52.199] slurmctld version 22.05.11 started on cluster cluster
[2024-01-31T20:07:52.215] No memory enforcing mechanism configured.
[2024-01-31T20:07:52.218] Recovered state of 4 nodes
[2024-01-31T20:07:52.218] Recovered information about 0 jobs
[2024-01-31T20:07:52.218] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T20:07:52.218] Recovered state of 0 reservations
[2024-01-31T20:07:52.218] read_slurm_conf: backup_controller not specified
[2024-01-31T20:07:52.218] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-01-31T20:07:52.218] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-01-31T20:07:52.218] Running as primary controller
[2024-01-31T20:07:52.218] No parameter for mcs plugin, default values set
[2024-01-31T20:07:52.218] mcs: MCSParameters = (null). ondemand set.
[2024-01-31T20:07:56.236] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T20:07:56.236] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T20:07:56.236] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T20:07:56.237] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T20:07:57.230] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-01-31T20:39:36.991] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T20:39:36.992] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T20:39:36.992] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T20:39:36.993] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T21:12:56.965] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T21:12:56.965] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T21:12:56.965] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T21:12:56.966] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T21:46:16.030] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T21:46:16.030] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T21:46:16.030] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T21:46:16.032] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T22:19:36.126] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T22:19:36.126] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T22:19:36.126] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T22:19:36.127] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T22:52:56.273] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T22:52:56.273] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T22:52:56.273] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T22:52:56.274] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T23:26:16.283] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T23:26:16.284] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T23:26:16.284] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T23:26:16.285] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T23:59:36.302] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T23:59:36.302] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T23:59:36.302] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-01-31T23:59:36.303] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T00:32:56.261] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T00:32:56.261] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T00:32:56.262] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T00:32:56.262] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T01:06:16.288] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T01:06:16.289] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T01:06:16.289] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T01:06:16.290] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T01:39:36.377] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T01:39:36.377] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T01:39:36.377] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T01:39:36.379] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T02:12:56.408] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T02:12:56.409] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T02:12:56.409] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T02:12:56.411] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T02:46:16.411] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T02:46:16.411] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T02:46:16.411] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T02:46:16.413] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T03:19:36.590] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T03:19:36.590] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T03:19:36.590] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T03:19:36.591] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T03:52:56.605] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T03:52:56.605] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T03:52:56.605] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T03:52:56.607] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T04:26:16.581] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T04:26:16.582] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T04:26:16.582] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T04:26:16.583] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T04:59:36.603] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T04:59:36.603] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T04:59:36.603] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T04:59:36.604] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T05:32:56.651] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T05:32:56.651] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T05:32:56.651] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T05:32:56.651] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T06:06:16.635] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T06:06:16.635] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T06:06:16.635] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T06:06:16.636] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T06:39:36.830] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T06:39:36.830] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T06:39:36.830] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T06:39:36.831] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T07:12:56.968] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T07:12:56.968] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T07:12:56.968] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T07:12:56.969] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T07:46:16.976] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T07:46:16.976] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T07:46:16.976] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T07:46:16.977] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T08:19:36.920] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T08:19:36.920] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T08:19:36.920] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T08:19:36.921] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T08:52:56.912] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T08:52:56.912] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T08:52:56.912] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T08:52:56.912] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T09:26:16.011] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T09:26:16.012] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T09:26:16.012] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T09:26:16.012] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T09:59:37.032] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T09:59:37.032] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T09:59:37.032] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T09:59:37.032] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T10:32:57.109] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T10:32:57.110] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T10:32:57.110] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T10:32:57.111] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T11:06:17.115] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T11:06:17.116] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T11:06:17.116] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T11:06:17.117] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T11:39:37.283] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T11:39:37.283] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T11:39:37.284] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T11:39:37.284] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:12:57.345] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:12:57.345] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:12:57.345] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:12:57.346] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:22:36.604] sched: _slurm_rpc_allocate_resources JobId=1 NodeList=compute[00-01] usec=187
[2024-02-01T12:28:25.204] _job_complete: JobId=1 WEXITSTATUS 1
[2024-02-01T12:28:25.204] _job_complete: JobId=1 done
[2024-02-01T12:29:37.616] Node compute01 now responding
[2024-02-01T12:29:37.616] Node compute00 now responding
[2024-02-01T12:29:52.646] Resending TERMINATE_JOB request JobId=1 Nodelist=compute[00-01]
[2024-02-01T12:31:17.842] Node compute01 now responding
[2024-02-01T12:31:17.842] Node compute00 now responding
[2024-02-01T12:32:52.058] error: Nodes compute[00-01] not responding
[2024-02-01T12:32:56.310] sched: _slurm_rpc_allocate_resources JobId=2 NodeList=(null) usec=122
[2024-02-01T12:32:57.101] Node compute01 now responding
[2024-02-01T12:32:57.101] Node compute00 now responding
[2024-02-01T12:34:01.801] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=1 uid 1000
[2024-02-01T12:34:01.801] error: Security violation, REQUEST_KILL_JOB RPC for JobId=1 from uid 1000
[2024-02-01T12:34:01.801] _slurm_rpc_kill_job: job_str_signal() uid=1000 JobId=1 sig=9 returned: Access/permission denied
[2024-02-01T12:34:14.304] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=1 uid 0
[2024-02-01T12:34:16.704] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=2 uid 0
[2024-02-01T12:34:37.331] Node compute01 now responding
[2024-02-01T12:34:37.331] Node compute00 now responding
[2024-02-01T12:34:55.660] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=1 uid 0
[2024-02-01T12:36:17.562] Node compute01 now responding
[2024-02-01T12:36:17.562] Node compute00 now responding
[2024-02-01T12:37:12.660] Invalid node state transition requested for node compute00 from=COMPLETING* to=RESUME
[2024-02-01T12:37:12.660] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-02-01T12:37:52.780] error: Nodes compute[00-01] not responding
[2024-02-01T12:37:53.022] Invalid node state transition requested for node compute01 from=COMPLETING* to=RESUME
[2024-02-01T12:37:53.022] _slurm_rpc_update_node for compute01: Invalid node state specified
[2024-02-01T12:37:57.823] Node compute01 now responding
[2024-02-01T12:37:57.823] Node compute00 now responding
[2024-02-01T12:38:55.710] Invalid node state transition requested for node compute00 from=COMPLETING* to=RESUME
[2024-02-01T12:38:55.710] Invalid node state transition requested for node compute01 from=COMPLETING* to=RESUME
[2024-02-01T12:38:55.710] _slurm_rpc_update_node for compute[00-01]: Invalid node state specified
[2024-02-01T12:39:37.060] Node compute01 now responding
[2024-02-01T12:39:37.060] Node compute00 now responding
[2024-02-01T12:41:17.298] Node compute01 now responding
[2024-02-01T12:41:17.298] Node compute00 now responding
[2024-02-01T12:42:04.034] update_node: node compute00 reason set to: Destravando o no
[2024-02-01T12:42:04.034] update_node: node compute00 state set to DRAINING*
[2024-02-01T12:42:09.149] update_node: node compute01 reason set to: Destravando o no
[2024-02-01T12:42:09.149] update_node: node compute01 state set to DRAINING*
[2024-02-01T12:42:27.223] update_node: node compute01 state set to IDLE
[2024-02-01T12:42:27.465] Node compute01 now responding
[2024-02-01T12:42:27.465] Node compute00 now responding
[2024-02-01T12:42:27.466] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:42:32.282] update_node: node compute00 state set to IDLE
[2024-02-01T12:42:32.476] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:42:32.476] Node compute00 now responding
[2024-02-01T12:42:52.516] error: Nodes compute[00-01] not responding
[2024-02-01T12:42:54.568] update_node: node compute00 state set to IDLE
[2024-02-01T12:42:57.542] update_node: node compute01 state set to IDLE
[2024-02-01T12:43:39.113] Node compute01 now responding
[2024-02-01T12:44:12.730] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:44:12.730] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:44:12.730] Node compute00 now responding
[2024-02-01T12:44:12.731] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:44:52.913] cleanup_completing: JobId=1 completion process took 987 seconds
[2024-02-01T12:44:57.081] Invalid node state transition requested for node compute00 from=IDLE to=RESUME
[2024-02-01T12:44:57.081] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-02-01T12:45:18.557] Invalid node state transition requested for node compute00 from=IDLE to=RESUME
[2024-02-01T12:45:18.557] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-02-01T12:46:04.813] sched: _slurm_rpc_allocate_resources JobId=3 NodeList=(null) usec=124
[2024-02-01T12:47:52.238] error: Nodes compute[00-01] not responding
[2024-02-01T12:56:18.595] _job_complete: JobId=3 WTERMSIG 126
[2024-02-01T12:56:18.595] _job_complete: JobId=3 cancelled by interactive user
[2024-02-01T12:56:18.595] _job_complete: JobId=3 done
[2024-02-01T12:58:28.458] killing old slurmctld[1789]
[2024-02-01T12:58:28.458] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T12:58:28.548] Saving all slurm state
[2024-02-01T12:58:28.577] error: chdir(/var/log): Permission denied
[2024-02-01T12:58:28.577] error: Configured MailProg is invalid
[2024-02-01T12:58:28.577] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T12:58:28.579] No memory enforcing mechanism configured.
[2024-02-01T12:58:28.580] Recovered state of 4 nodes
[2024-02-01T12:58:28.580] Recovered JobId=3 Assoc=0
[2024-02-01T12:58:28.580] Recovered information about 1 jobs
[2024-02-01T12:58:28.580] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T12:58:28.580] Recovered state of 0 reservations
[2024-02-01T12:58:28.580] read_slurm_conf: backup_controller not specified
[2024-02-01T12:58:28.580] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T12:58:28.580] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T12:58:28.580] Running as primary controller
[2024-02-01T12:58:28.580] No parameter for mcs plugin, default values set
[2024-02-01T12:58:28.580] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T12:58:31.651] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T12:58:31.685] Saving all slurm state
[2024-02-01T12:58:35.764] error: chdir(/var/log): Permission denied
[2024-02-01T12:58:35.764] error: Configured MailProg is invalid
[2024-02-01T12:58:35.765] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T12:58:35.766] No memory enforcing mechanism configured.
[2024-02-01T12:58:35.783] Recovered state of 4 nodes
[2024-02-01T12:58:35.783] Recovered JobId=3 Assoc=0
[2024-02-01T12:58:35.783] Recovered information about 1 jobs
[2024-02-01T12:58:35.783] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T12:58:35.783] Recovered state of 0 reservations
[2024-02-01T12:58:35.783] read_slurm_conf: backup_controller not specified
[2024-02-01T12:58:35.783] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T12:58:35.783] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T12:58:35.783] Running as primary controller
[2024-02-01T12:58:35.783] No parameter for mcs plugin, default values set
[2024-02-01T12:58:35.783] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T12:58:39.797] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:58:39.797] error: Setting node compute01 state to INVAL with reason:Low RealMemory (reported:15691 < 100.00% of configured:16000)
[2024-02-01T12:58:39.797] drain_nodes: node compute01 state set to DRAIN
[2024-02-01T12:58:39.797] error: _slurm_rpc_node_registration node=compute01: Invalid argument
[2024-02-01T12:58:39.797] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:58:39.798] error: Setting node compute03 state to INVAL with reason:Low RealMemory (reported:15691 < 100.00% of configured:16000)
[2024-02-01T12:58:39.798] drain_nodes: node compute03 state set to DRAIN
[2024-02-01T12:58:39.798] error: _slurm_rpc_node_registration node=compute03: Invalid argument
[2024-02-01T12:58:39.798] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:58:39.798] error: Setting node compute02 state to INVAL with reason:Low RealMemory (reported:15691 < 100.00% of configured:16000)
[2024-02-01T12:58:39.798] drain_nodes: node compute02 state set to DRAIN
[2024-02-01T12:58:39.798] error: _slurm_rpc_node_registration node=compute02: Invalid argument
[2024-02-01T12:58:39.798] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T12:58:39.798] error: Setting node compute00 state to INVAL with reason:Low RealMemory (reported:15691 < 100.00% of configured:16000)
[2024-02-01T12:58:39.798] drain_nodes: node compute00 state set to DRAIN
[2024-02-01T12:58:39.798] error: _slurm_rpc_node_registration node=compute00: Invalid argument
[2024-02-01T12:58:57.510] Invalid node state transition requested for node compute00 from=INVAL to=RESUME
[2024-02-01T12:58:57.510] Invalid node state transition requested for node compute01 from=INVAL to=RESUME
[2024-02-01T12:58:57.510] Invalid node state transition requested for node compute02 from=INVAL to=RESUME
[2024-02-01T12:58:57.510] Invalid node state transition requested for node compute03 from=INVAL to=RESUME
[2024-02-01T12:58:57.510] _slurm_rpc_update_node for compute[00-03]: Invalid node state specified
[2024-02-01T12:58:57.837] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-01T12:59:07.714] Invalid node state transition requested for node compute00 from=INVAL to=RESUME
[2024-02-01T12:59:07.714] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-02-01T13:01:17.950] killing old slurmctld[10441]
[2024-02-01T13:01:17.950] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T13:01:17.972] Saving all slurm state
[2024-02-01T13:01:17.999] error: chdir(/var/log): Permission denied
[2024-02-01T13:01:17.999] error: Configured MailProg is invalid
[2024-02-01T13:01:17.999] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T13:01:18.001] No memory enforcing mechanism configured.
[2024-02-01T13:01:18.001] Recovered state of 4 nodes
[2024-02-01T13:01:18.002] Recovered JobId=3 Assoc=0
[2024-02-01T13:01:18.002] Recovered information about 1 jobs
[2024-02-01T13:01:18.002] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T13:01:18.002] Recovered state of 0 reservations
[2024-02-01T13:01:18.002] read_slurm_conf: backup_controller not specified
[2024-02-01T13:01:18.002] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T13:01:18.002] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T13:01:18.002] Running as primary controller
[2024-02-01T13:01:18.002] No parameter for mcs plugin, default values set
[2024-02-01T13:01:18.002] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T13:01:20.879] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T13:01:20.908] Saving all slurm state
[2024-02-01T13:01:24.676] error: chdir(/var/log): Permission denied
[2024-02-01T13:01:24.676] error: Configured MailProg is invalid
[2024-02-01T13:01:24.677] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T13:01:24.678] No memory enforcing mechanism configured.
[2024-02-01T13:01:24.695] Recovered state of 4 nodes
[2024-02-01T13:01:24.695] Recovered JobId=3 Assoc=0
[2024-02-01T13:01:24.695] Recovered information about 1 jobs
[2024-02-01T13:01:24.695] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T13:01:24.695] Recovered state of 0 reservations
[2024-02-01T13:01:24.695] read_slurm_conf: backup_controller not specified
[2024-02-01T13:01:24.695] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T13:01:24.695] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T13:01:24.695] Running as primary controller
[2024-02-01T13:01:24.695] No parameter for mcs plugin, default values set
[2024-02-01T13:01:24.695] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T13:01:28.707] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:01:28.707] error: Setting node compute02 state to INVAL with reason:Low RealMemory (reported:15691 < 100.00% of configured:16000)
[2024-02-01T13:01:28.707] error: _slurm_rpc_node_registration node=compute02: Invalid argument
[2024-02-01T13:01:28.708] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:01:28.708] error: Setting node compute01 state to INVAL with reason:Low RealMemory (reported:15691 < 100.00% of configured:16000)
[2024-02-01T13:01:28.708] error: _slurm_rpc_node_registration node=compute01: Invalid argument
[2024-02-01T13:01:28.708] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:01:28.708] error: Setting node compute03 state to INVAL with reason:Low RealMemory (reported:15691 < 100.00% of configured:16000)
[2024-02-01T13:01:28.708] error: _slurm_rpc_node_registration node=compute03: Invalid argument
[2024-02-01T13:01:28.708] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:01:28.708] error: Setting node compute00 state to INVAL with reason:Low RealMemory (reported:15691 < 100.00% of configured:16000)
[2024-02-01T13:01:28.708] error: _slurm_rpc_node_registration node=compute00: Invalid argument
[2024-02-01T13:02:24.832] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-01T13:02:58.728] killing old slurmctld[10518]
[2024-02-01T13:02:58.728] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T13:02:58.814] Saving all slurm state
[2024-02-01T13:02:58.843] error: chdir(/var/log): Permission denied
[2024-02-01T13:02:58.843] error: Configured MailProg is invalid
[2024-02-01T13:02:58.843] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T13:02:58.845] No memory enforcing mechanism configured.
[2024-02-01T13:02:58.846] Recovered state of 4 nodes
[2024-02-01T13:02:58.846] Recovered information about 0 jobs
[2024-02-01T13:02:58.846] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T13:02:58.847] Recovered state of 0 reservations
[2024-02-01T13:02:58.847] read_slurm_conf: backup_controller not specified
[2024-02-01T13:02:58.847] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T13:02:58.847] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T13:02:58.847] Running as primary controller
[2024-02-01T13:02:58.847] No parameter for mcs plugin, default values set
[2024-02-01T13:02:58.847] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T13:03:02.861] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:03:02.861] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:03:02.861] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:03:02.863] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:03:05.062] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T13:03:05.161] Saving all slurm state
[2024-02-01T13:03:10.136] error: chdir(/var/log): Permission denied
[2024-02-01T13:03:10.136] error: Configured MailProg is invalid
[2024-02-01T13:03:10.136] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T13:03:10.138] No memory enforcing mechanism configured.
[2024-02-01T13:03:10.159] Recovered state of 4 nodes
[2024-02-01T13:03:10.159] Recovered information about 0 jobs
[2024-02-01T13:03:10.159] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T13:03:10.159] Recovered state of 0 reservations
[2024-02-01T13:03:10.159] read_slurm_conf: backup_controller not specified
[2024-02-01T13:03:10.159] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T13:03:10.159] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T13:03:10.159] Running as primary controller
[2024-02-01T13:03:10.159] No parameter for mcs plugin, default values set
[2024-02-01T13:03:10.159] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T13:03:14.175] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:03:14.175] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:03:14.175] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:03:14.175] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-01T13:04:10.303] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-01T13:04:27.004] update_node: node compute00 state set to IDLE
[2024-02-01T13:04:27.004] update_node: node compute01 state set to IDLE
[2024-02-01T13:04:27.004] update_node: node compute02 state set to IDLE
[2024-02-01T13:04:27.004] update_node: node compute03 state set to IDLE
[2024-02-01T13:04:27.351] Node compute02 now responding
[2024-02-01T13:04:27.351] Node compute00 now responding
[2024-02-01T13:04:27.351] Node compute03 now responding
[2024-02-01T13:04:27.351] Node compute01 now responding
[2024-02-01T13:04:42.607] sched: _slurm_rpc_allocate_resources JobId=4 NodeList=(null) usec=189
[2024-02-01T13:04:54.046] _job_complete: JobId=4 WTERMSIG 126
[2024-02-01T13:04:54.046] _job_complete: JobId=4 cancelled by interactive user
[2024-02-01T13:04:54.046] _job_complete: JobId=4 done
[2024-02-01T13:04:54.047] _slurm_rpc_complete_job_allocation: JobId=4 error Job/step already completing or completed
[2024-02-01T13:05:07.963] sched: _slurm_rpc_allocate_resources JobId=5 NodeList=(null) usec=142
[2024-02-01T13:05:16.722] _job_complete: JobId=5 WTERMSIG 126
[2024-02-01T13:05:16.722] _job_complete: JobId=5 cancelled by interactive user
[2024-02-01T13:05:16.722] _job_complete: JobId=5 done
[2024-02-01T13:05:16.722] _slurm_rpc_complete_job_allocation: JobId=5 error Job/step already completing or completed
[2024-02-01T13:06:32.685] sched: _slurm_rpc_allocate_resources JobId=6 NodeList=(null) usec=123
[2024-02-01T13:07:14.325] _job_complete: JobId=6 WTERMSIG 126
[2024-02-01T13:07:14.325] _job_complete: JobId=6 cancelled by interactive user
[2024-02-01T13:07:14.325] _job_complete: JobId=6 done
[2024-02-01T13:07:14.326] _slurm_rpc_complete_job_allocation: JobId=6 error Job/step already completing or completed
[2024-02-01T13:14:53.990] sched: _slurm_rpc_allocate_resources JobId=7 NodeList=compute00 usec=198
[2024-02-01T13:14:54.038] _job_complete: JobId=7 WEXITSTATUS 2
[2024-02-01T13:14:54.038] _job_complete: JobId=7 done
[2024-02-01T13:22:08.579] _slurm_rpc_submit_batch_job: JobId=8 InitPrio=4294901755 usec=156
[2024-02-01T13:22:08.955] sched: Allocate JobId=8 NodeList=compute[00-01] #CPUs=4 Partition=normal
[2024-02-01T13:22:08.985] _job_complete: JobId=8 WTERMSIG 53
[2024-02-01T13:22:08.985] _job_complete: JobId=8 done
[2024-02-01T13:24:03.070] _slurm_rpc_submit_batch_job: JobId=9 InitPrio=4294901754 usec=203
[2024-02-01T13:24:03.246] sched: Allocate JobId=9 NodeList=compute[00-01] #CPUs=4 Partition=normal
[2024-02-01T13:24:03.275] _job_complete: JobId=9 WTERMSIG 53
[2024-02-01T13:24:03.275] _job_complete: JobId=9 done
[2024-02-01T13:25:00.577] _slurm_rpc_submit_batch_job: JobId=10 InitPrio=4294901753 usec=165
[2024-02-01T13:25:01.380] sched: Allocate JobId=10 NodeList=compute[00-01] #CPUs=4 Partition=normal
[2024-02-01T13:25:01.410] _job_complete: JobId=10 WTERMSIG 53
[2024-02-01T13:25:01.410] _job_complete: JobId=10 done
[2024-02-01T13:26:47.282] _slurm_rpc_submit_batch_job: JobId=11 InitPrio=4294901752 usec=163
[2024-02-01T13:26:47.323] sched/backfill: _start_job: Started JobId=11 in normal on compute[00-03]
[2024-02-01T13:26:47.354] _job_complete: JobId=11 WTERMSIG 53
[2024-02-01T13:26:47.354] _job_complete: JobId=11 done
[2024-02-01T15:02:34.341] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T15:02:34.413] Saving all slurm state
[2024-02-01T15:03:04.849] error: chdir(/var/log): Permission denied
[2024-02-01T15:03:04.850] error: Configured MailProg is invalid
[2024-02-01T15:03:04.850] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T15:03:04.866] No memory enforcing mechanism configured.
[2024-02-01T15:03:04.869] Recovered state of 4 nodes
[2024-02-01T15:03:04.869] Recovered information about 0 jobs
[2024-02-01T15:03:04.869] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T15:03:04.869] Recovered state of 0 reservations
[2024-02-01T15:03:04.870] read_slurm_conf: backup_controller not specified
[2024-02-01T15:03:04.870] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T15:03:04.870] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T15:03:04.870] Running as primary controller
[2024-02-01T15:03:04.870] No parameter for mcs plugin, default values set
[2024-02-01T15:03:04.870] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T15:04:04.452] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-01T15:08:05.000] error: Nodes compute[00-03] not responding
[2024-02-01T15:08:08.031] error: Nodes compute[00-03] not responding, setting DOWN
[2024-02-01T15:15:47.898] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T15:15:47.922] Saving all slurm state
[2024-02-01T15:16:11.082] error: chdir(/var/log): Permission denied
[2024-02-01T15:16:11.082] error: Configured MailProg is invalid
[2024-02-01T15:16:11.082] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T15:16:11.120] No memory enforcing mechanism configured.
[2024-02-01T15:16:11.123] Recovered state of 4 nodes
[2024-02-01T15:16:11.123] Down nodes: compute[00-03]
[2024-02-01T15:16:11.123] Recovered information about 0 jobs
[2024-02-01T15:16:11.123] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T15:16:11.124] Recovered state of 0 reservations
[2024-02-01T15:16:11.124] read_slurm_conf: backup_controller not specified
[2024-02-01T15:16:11.124] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T15:16:11.124] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T15:16:11.124] Running as primary controller
[2024-02-01T15:16:11.124] No parameter for mcs plugin, default values set
[2024-02-01T15:16:11.124] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T15:17:11.384] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-01T15:21:25.690] Node compute03 now responding
[2024-02-01T15:21:25.690] node compute03 returned to service
[2024-02-01T15:21:26.029] Node compute02 now responding
[2024-02-01T15:21:26.029] node compute02 returned to service
[2024-02-01T15:21:26.029] Node compute01 now responding
[2024-02-01T15:21:26.029] node compute01 returned to service
[2024-02-01T15:21:26.030] Node compute00 now responding
[2024-02-01T15:21:26.030] node compute00 returned to service
[2024-02-01T15:27:33.137] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T15:27:33.191] Saving all slurm state
[2024-02-01T15:27:57.948] error: chdir(/var/log): Permission denied
[2024-02-01T15:27:57.950] error: Configured MailProg is invalid
[2024-02-01T15:27:57.950] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T15:27:57.965] No memory enforcing mechanism configured.
[2024-02-01T15:27:57.968] Recovered state of 4 nodes
[2024-02-01T15:27:57.968] Recovered information about 0 jobs
[2024-02-01T15:27:57.968] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T15:27:57.968] Recovered state of 0 reservations
[2024-02-01T15:27:57.968] read_slurm_conf: backup_controller not specified
[2024-02-01T15:27:57.968] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T15:27:57.968] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T15:27:57.968] Running as primary controller
[2024-02-01T15:27:57.968] No parameter for mcs plugin, default values set
[2024-02-01T15:27:57.968] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T15:28:02.979] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-01T15:50:28.503] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T15:50:28.587] Saving all slurm state
[2024-02-01T15:50:53.922] error: chdir(/var/log): Permission denied
[2024-02-01T15:50:53.924] error: Configured MailProg is invalid
[2024-02-01T15:50:53.924] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T15:50:53.940] No memory enforcing mechanism configured.
[2024-02-01T15:50:53.943] Recovered state of 4 nodes
[2024-02-01T15:50:53.943] Recovered information about 0 jobs
[2024-02-01T15:50:53.943] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T15:50:53.944] Recovered state of 0 reservations
[2024-02-01T15:50:53.944] read_slurm_conf: backup_controller not specified
[2024-02-01T15:50:53.944] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T15:50:53.944] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T15:50:53.944] Running as primary controller
[2024-02-01T15:50:53.944] No parameter for mcs plugin, default values set
[2024-02-01T15:50:53.944] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T15:50:58.955] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-01T16:42:51.256] Terminate signal (SIGINT or SIGTERM) received
[2024-02-01T16:42:51.290] Saving all slurm state
[2024-02-01T16:43:14.382] error: chdir(/var/log): Permission denied
[2024-02-01T16:43:14.382] error: Configured MailProg is invalid
[2024-02-01T16:43:14.382] slurmctld version 22.05.11 started on cluster cluster
[2024-02-01T16:43:14.397] No memory enforcing mechanism configured.
[2024-02-01T16:43:14.400] Recovered state of 4 nodes
[2024-02-01T16:43:14.400] Recovered information about 0 jobs
[2024-02-01T16:43:14.401] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T16:43:14.401] Recovered state of 0 reservations
[2024-02-01T16:43:14.401] read_slurm_conf: backup_controller not specified
[2024-02-01T16:43:14.401] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-01T16:43:14.401] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-01T16:43:14.401] Running as primary controller
[2024-02-01T16:43:14.401] No parameter for mcs plugin, default values set
[2024-02-01T16:43:14.401] mcs: MCSParameters = (null). ondemand set.
[2024-02-01T16:43:19.412] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-02T09:34:53.626] Terminate signal (SIGINT or SIGTERM) received
[2024-02-02T09:34:53.715] Saving all slurm state
[2024-02-02T09:35:14.988] error: chdir(/var/log): Permission denied
[2024-02-02T09:35:14.988] error: Configured MailProg is invalid
[2024-02-02T09:35:14.989] slurmctld version 22.05.11 started on cluster cluster
[2024-02-02T09:35:15.004] No memory enforcing mechanism configured.
[2024-02-02T09:35:15.007] Recovered state of 4 nodes
[2024-02-02T09:35:15.007] Recovered information about 0 jobs
[2024-02-02T09:35:15.007] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T09:35:15.007] Recovered state of 0 reservations
[2024-02-02T09:35:15.008] read_slurm_conf: backup_controller not specified
[2024-02-02T09:35:15.008] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-02T09:35:15.008] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T09:35:15.008] Running as primary controller
[2024-02-02T09:35:15.008] No parameter for mcs plugin, default values set
[2024-02-02T09:35:15.008] mcs: MCSParameters = (null). ondemand set.
[2024-02-02T09:35:20.019] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-02T09:47:35.635] Terminate signal (SIGINT or SIGTERM) received
[2024-02-02T09:47:35.683] Saving all slurm state
[2024-02-02T09:48:01.007] error: chdir(/var/log): Permission denied
[2024-02-02T09:48:01.007] error: Configured MailProg is invalid
[2024-02-02T09:48:01.007] slurmctld version 22.05.11 started on cluster cluster
[2024-02-02T09:48:01.023] No memory enforcing mechanism configured.
[2024-02-02T09:48:01.026] Recovered state of 4 nodes
[2024-02-02T09:48:01.026] Recovered information about 0 jobs
[2024-02-02T09:48:01.026] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T09:48:01.026] Recovered state of 0 reservations
[2024-02-02T09:48:01.026] read_slurm_conf: backup_controller not specified
[2024-02-02T09:48:01.026] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-02T09:48:01.026] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T09:48:01.026] Running as primary controller
[2024-02-02T09:48:01.027] No parameter for mcs plugin, default values set
[2024-02-02T09:48:01.027] mcs: MCSParameters = (null). ondemand set.
[2024-02-02T09:48:06.037] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-02T11:36:25.133] error: Nodes compute00 not responding, setting DOWN
[2024-02-02T11:38:01.310] error: Nodes compute01 not responding
[2024-02-02T11:41:25.827] error: Nodes compute01 not responding, setting DOWN
[2024-02-02T12:03:01.818] error: Nodes compute[02-03] not responding
[2024-02-02T12:03:05.854] error: Nodes compute[02-03] not responding, setting DOWN
[2024-02-02T12:44:59.905] update_node: node compute00 state set to IDLE
[2024-02-02T12:45:16.292] update_node: node compute01 state set to IDLE
[2024-02-02T12:45:16.292] update_node: node compute02 state set to IDLE
[2024-02-02T12:45:16.292] update_node: node compute03 state set to IDLE
[2024-02-02T12:46:45.767] Node compute02 now responding
[2024-02-02T12:46:45.769] Node compute01 now responding
[2024-02-02T12:46:45.770] Node compute03 now responding
[2024-02-02T12:46:45.773] Node compute00 now responding
[2024-02-02T12:48:01.238] error: Nodes compute[00-03] not responding
[2024-02-02T12:48:04.816] _slurm_rpc_submit_batch_job: JobId=12 InitPrio=4294901759 usec=153
[2024-02-02T12:49:15.289] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=12 uid 0
[2024-02-02T12:49:31.780] _slurm_rpc_submit_batch_job: JobId=13 InitPrio=4294901758 usec=177
[2024-02-02T12:49:32.475] sched: Allocate JobId=13 NodeList=compute00 #CPUs=1 Partition=express
[2024-02-02T12:49:32.506] _job_complete: JobId=13 WTERMSIG 53
[2024-02-02T12:49:32.506] _job_complete: JobId=13 done
[2024-02-02T12:50:20.979] _slurm_rpc_submit_batch_job: JobId=14 InitPrio=4294901757 usec=132
[2024-02-02T12:50:21.592] sched: Allocate JobId=14 NodeList=compute00 #CPUs=1 Partition=express
[2024-02-02T12:50:21.624] _job_complete: JobId=14 WTERMSIG 53
[2024-02-02T12:50:21.624] _job_complete: JobId=14 done
[2024-02-02T12:51:45.167] _slurm_rpc_submit_batch_job: JobId=15 InitPrio=4294901756 usec=165
[2024-02-02T12:51:45.199] sched/backfill: _start_job: Started JobId=15 in express on compute00
[2024-02-02T12:51:45.230] _job_complete: JobId=15 WTERMSIG 53
[2024-02-02T12:51:45.230] _job_complete: JobId=15 done
[2024-02-02T13:16:02.442] _slurm_rpc_submit_batch_job: JobId=16 InitPrio=4294901755 usec=161
[2024-02-02T13:16:03.436] sched/backfill: _start_job: Started JobId=16 in express on compute00
[2024-02-02T13:16:03.469] _job_complete: JobId=16 WTERMSIG 53
[2024-02-02T13:16:03.469] _job_complete: JobId=16 done
[2024-02-02T13:16:24.278] _slurm_rpc_submit_batch_job: JobId=17 InitPrio=4294901754 usec=138
[2024-02-02T13:16:24.387] sched: Allocate JobId=17 NodeList=compute00 #CPUs=1 Partition=express
[2024-02-02T13:16:24.417] _job_complete: JobId=17 WTERMSIG 53
[2024-02-02T13:16:24.417] _job_complete: JobId=17 done
[2024-02-02T13:19:10.646] _slurm_rpc_submit_batch_job: JobId=18 InitPrio=4294901753 usec=136
[2024-02-02T13:19:10.796] sched: Allocate JobId=18 NodeList=compute00 #CPUs=1 Partition=express
[2024-02-02T13:19:10.827] _job_complete: JobId=18 WTERMSIG 53
[2024-02-02T13:19:10.827] _job_complete: JobId=18 done
[2024-02-02T13:23:22.008] _slurm_rpc_submit_batch_job: JobId=19 InitPrio=4294901752 usec=136
[2024-02-02T13:23:22.400] sched: Allocate JobId=19 NodeList=compute00 #CPUs=1 Partition=express
[2024-02-02T13:23:22.432] _job_complete: JobId=19 WTERMSIG 53
[2024-02-02T13:23:22.432] _job_complete: JobId=19 done
[2024-02-02T13:24:41.368] _slurm_rpc_submit_batch_job: JobId=20 InitPrio=4294901751 usec=137
[2024-02-02T13:24:41.492] sched/backfill: _start_job: Started JobId=20 in normal on compute[00-03]
[2024-02-02T13:24:41.525] _job_complete: JobId=20 WTERMSIG 53
[2024-02-02T13:24:41.525] _job_complete: JobId=20 done
[2024-02-02T13:28:21.262] Terminate signal (SIGINT or SIGTERM) received
[2024-02-02T13:28:21.315] Saving all slurm state
[2024-02-02T13:28:21.345] fatal: mkdir(/var/spool/slurm/ctld): No such file or directory
[2024-02-02T13:28:49.256] fatal: mkdir(/var/spool/slurm/ctld): No such file or directory
[2024-02-02T13:29:13.435] fatal: mkdir(/var/spool/slurm/ctld): No such file or directory
[2024-02-02T13:33:07.828] fatal: mkdir(/var/spool/slurm/ctld): No such file or directory
[2024-02-02T13:34:04.753] error: chdir(/var/log): Permission denied
[2024-02-02T13:34:04.753] error: Configured MailProg is invalid
[2024-02-02T13:34:04.753] slurmctld version 22.05.11 started on cluster cluster
[2024-02-02T13:34:04.755] No memory enforcing mechanism configured.
[2024-02-02T13:34:04.777] Recovered state of 4 nodes
[2024-02-02T13:34:04.777] Recovered JobId=19 Assoc=0
[2024-02-02T13:34:04.777] Recovered JobId=20 Assoc=0
[2024-02-02T13:34:04.777] Recovered information about 2 jobs
[2024-02-02T13:34:04.777] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T13:34:04.777] Recovered state of 0 reservations
[2024-02-02T13:34:04.777] read_slurm_conf: backup_controller not specified
[2024-02-02T13:34:04.777] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-02T13:34:04.777] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T13:34:04.777] Running as primary controller
[2024-02-02T13:34:04.777] No parameter for mcs plugin, default values set
[2024-02-02T13:34:04.777] mcs: MCSParameters = (null). ondemand set.
[2024-02-02T13:34:07.922] killing old slurmctld[7049]
[2024-02-02T13:34:07.922] Terminate signal (SIGINT or SIGTERM) received
[2024-02-02T13:34:07.984] Saving all slurm state
[2024-02-02T13:34:08.014] error: chdir(/var/log): Permission denied
[2024-02-02T13:34:08.014] error: Configured MailProg is invalid
[2024-02-02T13:34:08.014] slurmctld version 22.05.11 started on cluster cluster
[2024-02-02T13:34:08.015] No memory enforcing mechanism configured.
[2024-02-02T13:34:08.016] Recovered state of 4 nodes
[2024-02-02T13:34:08.016] Recovered JobId=19 Assoc=0
[2024-02-02T13:34:08.016] Recovered JobId=20 Assoc=0
[2024-02-02T13:34:08.016] Recovered information about 2 jobs
[2024-02-02T13:34:08.017] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T13:34:08.017] Recovered state of 0 reservations
[2024-02-02T13:34:08.017] read_slurm_conf: backup_controller not specified
[2024-02-02T13:34:08.017] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-02T13:34:08.017] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T13:34:08.017] Running as primary controller
[2024-02-02T13:34:08.017] No parameter for mcs plugin, default values set
[2024-02-02T13:34:08.017] mcs: MCSParameters = (null). ondemand set.
[2024-02-02T13:34:12.033] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T13:34:12.034] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T13:34:12.034] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T13:34:12.035] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T13:34:13.029] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-02T13:34:39.670] Terminate signal (SIGINT or SIGTERM) received
[2024-02-02T13:34:39.693] Saving all slurm state
[2024-02-02T13:34:54.364] error: chdir(/var/log): Permission denied
[2024-02-02T13:34:54.364] error: Configured MailProg is invalid
[2024-02-02T13:34:54.364] slurmctld version 22.05.11 started on cluster cluster
[2024-02-02T13:34:54.366] No memory enforcing mechanism configured.
[2024-02-02T13:34:54.388] Recovered state of 4 nodes
[2024-02-02T13:34:54.388] Recovered JobId=19 Assoc=0
[2024-02-02T13:34:54.388] Recovered JobId=20 Assoc=0
[2024-02-02T13:34:54.388] Recovered information about 2 jobs
[2024-02-02T13:34:54.388] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T13:34:54.388] Recovered state of 0 reservations
[2024-02-02T13:34:54.388] read_slurm_conf: backup_controller not specified
[2024-02-02T13:34:54.388] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-02T13:34:54.388] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T13:34:54.388] Running as primary controller
[2024-02-02T13:34:54.388] No parameter for mcs plugin, default values set
[2024-02-02T13:34:54.388] mcs: MCSParameters = (null). ondemand set.
[2024-02-02T13:34:58.404] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T13:34:58.404] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T13:34:58.404] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T13:34:58.405] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T13:34:59.400] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-02T13:35:46.882] _slurm_rpc_submit_batch_job: JobId=21 InitPrio=4294901750 usec=21165
[2024-02-02T13:35:47.515] sched: Allocate JobId=21 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T13:35:47.546] _job_complete: JobId=21 WTERMSIG 53
[2024-02-02T13:35:47.546] _job_complete: JobId=21 done
[2024-02-02T13:36:05.688] _slurm_rpc_submit_batch_job: JobId=22 InitPrio=4294901749 usec=145
[2024-02-02T13:36:06.557] sched: Allocate JobId=22 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T13:36:06.587] _job_complete: JobId=22 WTERMSIG 53
[2024-02-02T13:36:06.587] _job_complete: JobId=22 done
[2024-02-02T13:40:28.942] _slurm_rpc_submit_batch_job: JobId=23 InitPrio=4294901748 usec=136
[2024-02-02T13:40:29.209] sched: Allocate JobId=23 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T13:40:29.243] _job_complete: JobId=23 WTERMSIG 53
[2024-02-02T13:40:29.243] _job_complete: JobId=23 done
[2024-02-02T13:52:00.615] _slurm_rpc_submit_batch_job: JobId=24 InitPrio=4294901747 usec=148
[2024-02-02T13:52:00.895] sched: Allocate JobId=24 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T13:52:00.928] _job_complete: JobId=24 WTERMSIG 53
[2024-02-02T13:52:00.928] _job_complete: JobId=24 done
[2024-02-02T13:59:05.880] _slurm_rpc_submit_batch_job: JobId=25 InitPrio=4294901746 usec=142
[2024-02-02T13:59:05.932] sched: Allocate JobId=25 NodeList=compute00 #CPUs=1 Partition=express
[2024-02-02T13:59:05.966] _job_complete: JobId=25 WTERMSIG 53
[2024-02-02T13:59:05.966] _job_complete: JobId=25 done
[2024-02-02T13:59:31.033] _slurm_rpc_submit_batch_job: JobId=26 InitPrio=4294901745 usec=171
[2024-02-02T13:59:31.993] sched: Allocate JobId=26 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T13:59:32.078] _job_complete: JobId=26 WEXITSTATUS 0
[2024-02-02T13:59:32.078] _job_complete: JobId=26 done
[2024-02-02T14:05:04.078] _slurm_rpc_submit_batch_job: JobId=27 InitPrio=4294901744 usec=144
[2024-02-02T14:05:04.818] sched: Allocate JobId=27 NodeList=compute00 #CPUs=1 Partition=express
[2024-02-02T14:05:04.911] _job_complete: JobId=27 WEXITSTATUS 0
[2024-02-02T14:05:04.911] _job_complete: JobId=27 done
[2024-02-02T14:06:38.039] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T14:06:38.039] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T14:06:38.039] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T14:06:38.039] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T14:19:54.824] error: Nodes compute[00-01,03] not responding
[2024-02-02T14:19:58.858] error: Nodes compute[00-01,03] not responding, setting DOWN
[2024-02-02T14:24:54.573] error: Nodes compute02 not responding
[2024-02-02T14:24:58.607] error: Nodes compute02 not responding, setting DOWN
[2024-02-02T14:51:49.039] _slurm_rpc_submit_batch_job: JobId=28 InitPrio=4294901743 usec=150
[2024-02-02T14:52:02.803] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=28 uid 1001
[2024-02-02T14:53:33.945] update_node: node compute00 state set to IDLE
[2024-02-02T14:53:33.945] update_node: node compute01 state set to IDLE
[2024-02-02T14:53:33.945] update_node: node compute02 state set to IDLE
[2024-02-02T14:53:33.945] update_node: node compute03 state set to IDLE
[2024-02-02T14:54:10.227] Node compute03 now responding
[2024-02-02T14:54:10.228] Node compute02 now responding
[2024-02-02T14:54:10.234] Node compute01 now responding
[2024-02-02T14:54:10.234] Node compute00 now responding
[2024-02-02T14:54:45.095] _slurm_rpc_submit_batch_job: JobId=29 InitPrio=4294901742 usec=139
[2024-02-02T14:54:46.030] sched: Allocate JobId=29 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T14:54:54.050] error: Nodes compute[00-03] not responding
[2024-02-02T14:54:56.114] _job_complete: JobId=29 WEXITSTATUS 0
[2024-02-02T14:54:56.114] _job_complete: JobId=29 done
[2024-02-02T14:58:05.084] _slurm_rpc_submit_batch_job: JobId=30 InitPrio=4294901741 usec=20750
[2024-02-02T14:58:05.523] sched: Allocate JobId=30 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T14:58:15.612] _job_complete: JobId=30 WEXITSTATUS 0
[2024-02-02T14:58:15.612] _job_complete: JobId=30 done
[2024-02-02T15:10:03.092] _slurm_rpc_submit_batch_job: JobId=31 InitPrio=4294901740 usec=141
[2024-02-02T15:10:03.266] sched: Allocate JobId=31 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T15:10:13.356] _job_complete: JobId=31 WEXITSTATUS 0
[2024-02-02T15:10:13.356] _job_complete: JobId=31 done
[2024-02-02T15:13:04.294] _slurm_rpc_submit_batch_job: JobId=32 InitPrio=4294901739 usec=134
[2024-02-02T15:13:04.436] sched/backfill: _start_job: Started JobId=32 in normal on compute[00-03]
[2024-02-02T15:13:14.538] _job_complete: JobId=32 WEXITSTATUS 0
[2024-02-02T15:13:14.538] _job_complete: JobId=32 done
[2024-02-02T15:14:09.212] _slurm_rpc_submit_batch_job: JobId=33 InitPrio=4294901738 usec=139
[2024-02-02T15:14:09.438] sched/backfill: _start_job: Started JobId=33 in normal on compute[00-03]
[2024-02-02T15:14:19.529] _job_complete: JobId=33 WEXITSTATUS 0
[2024-02-02T15:14:19.529] _job_complete: JobId=33 done
[2024-02-02T15:15:13.346] _pick_best_nodes: JobId=34 never runnable in partition normal
[2024-02-02T15:15:13.346] _slurm_rpc_submit_batch_job: Requested node configuration is not available
[2024-02-02T15:15:27.971] _slurm_rpc_submit_batch_job: JobId=35 InitPrio=4294901736 usec=144
[2024-02-02T15:15:28.078] sched: Allocate JobId=35 NodeList=compute00 #CPUs=4 Partition=normal
[2024-02-02T15:15:38.148] _job_complete: JobId=35 WEXITSTATUS 0
[2024-02-02T15:15:38.148] _job_complete: JobId=35 done
[2024-02-02T15:17:49.197] _slurm_rpc_submit_batch_job: JobId=36 InitPrio=4294901735 usec=139
[2024-02-02T15:17:49.418] sched: Allocate JobId=36 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T15:17:59.507] _job_complete: JobId=36 WEXITSTATUS 0
[2024-02-02T15:17:59.508] _job_complete: JobId=36 done
[2024-02-02T15:18:32.014] _slurm_rpc_submit_batch_job: JobId=37 InitPrio=4294901734 usec=222
[2024-02-02T15:18:32.512] sched: Allocate JobId=37 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T15:18:42.602] _job_complete: JobId=37 WEXITSTATUS 0
[2024-02-02T15:18:42.602] _job_complete: JobId=37 done
[2024-02-02T15:18:56.988] _pick_best_nodes: JobId=38 never runnable in partition normal
[2024-02-02T15:18:56.988] _slurm_rpc_submit_batch_job: Requested node configuration is not available
[2024-02-02T15:19:12.297] _slurm_rpc_submit_batch_job: JobId=39 InitPrio=4294901732 usec=140
[2024-02-02T15:19:12.601] sched: Allocate JobId=39 NodeList=compute[00-01] #CPUs=4 Partition=normal
[2024-02-02T15:19:22.688] _job_complete: JobId=39 WEXITSTATUS 0
[2024-02-02T15:19:22.688] _job_complete: JobId=39 done
[2024-02-02T15:23:40.929] _slurm_rpc_submit_batch_job: JobId=40 InitPrio=4294901731 usec=167
[2024-02-02T15:23:41.266] sched: Allocate JobId=40 NodeList=compute[00-01] #CPUs=8 Partition=normal
[2024-02-02T15:23:51.354] _job_complete: JobId=40 WEXITSTATUS 0
[2024-02-02T15:23:51.354] _job_complete: JobId=40 done
[2024-02-02T15:34:37.974] _slurm_rpc_submit_batch_job: JobId=41 InitPrio=4294901730 usec=140
[2024-02-02T15:34:38.553] sched/backfill: _start_job: Started JobId=41 in normal on compute[00-01]
[2024-02-02T15:34:38.634] _job_complete: JobId=41 WEXITSTATUS 0
[2024-02-02T15:34:38.634] _job_complete: JobId=41 done
[2024-02-02T15:35:14.798] _slurm_rpc_submit_batch_job: JobId=42 InitPrio=4294901729 usec=152
[2024-02-02T15:35:14.941] sched: Allocate JobId=42 NodeList=compute[00-01] #CPUs=8 Partition=normal
[2024-02-02T15:35:15.037] _job_complete: JobId=42 WEXITSTATUS 0
[2024-02-02T15:35:15.037] _job_complete: JobId=42 done
[2024-02-02T15:36:35.187] _slurm_rpc_submit_batch_job: JobId=43 InitPrio=4294901728 usec=133
[2024-02-02T15:36:35.560] sched/backfill: _start_job: Started JobId=43 in normal on compute[00-01]
[2024-02-02T15:36:35.646] _job_complete: JobId=43 WEXITSTATUS 0
[2024-02-02T15:36:35.646] _job_complete: JobId=43 done
[2024-02-02T15:41:12.561] Terminate signal (SIGINT or SIGTERM) received
[2024-02-02T15:41:12.616] Saving all slurm state
[2024-02-02T15:41:12.648] error: chdir(/var/log): Permission denied
[2024-02-02T15:41:12.648] error: Configured MailProg is invalid
[2024-02-02T15:41:12.648] slurmctld version 22.05.11 started on cluster cluster
[2024-02-02T15:41:12.650] No memory enforcing mechanism configured.
[2024-02-02T15:41:12.651] Recovered state of 4 nodes
[2024-02-02T15:41:12.651] Recovered JobId=43 Assoc=0
[2024-02-02T15:41:12.651] Recovered information about 1 jobs
[2024-02-02T15:41:12.651] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T15:41:12.651] Recovered state of 0 reservations
[2024-02-02T15:41:12.651] read_slurm_conf: backup_controller not specified
[2024-02-02T15:41:12.651] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-02T15:41:12.651] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T15:41:12.651] Running as primary controller
[2024-02-02T15:41:12.651] No parameter for mcs plugin, default values set
[2024-02-02T15:41:12.651] mcs: MCSParameters = (null). ondemand set.
[2024-02-02T15:41:17.663] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-02T15:42:25.718] _slurm_rpc_submit_batch_job: JobId=44 InitPrio=4294901727 usec=171
[2024-02-02T15:42:25.812] sched: Allocate JobId=44 NodeList=compute[00-01] #CPUs=8 Partition=normal
[2024-02-02T15:42:25.893] _job_complete: JobId=44 WEXITSTATUS 0
[2024-02-02T15:42:25.893] _job_complete: JobId=44 done
[2024-02-02T15:43:46.496] error: Security violation, UPDATE_NODE RPC from uid=1001
[2024-02-02T15:43:46.496] _slurm_rpc_update_node for compute00: Invalid user id
[2024-02-02T15:44:11.272] error: Security violation, UPDATE_NODE RPC from uid=1001
[2024-02-02T15:44:11.272] _slurm_rpc_update_node for compute00: Invalid user id
[2024-02-02T15:44:50.474] update_node: node compute00 reason set to: testando outros nos
[2024-02-02T15:44:50.474] update_node: node compute00 state set to DOWN
[2024-02-02T15:45:15.281] _slurm_rpc_submit_batch_job: JobId=45 InitPrio=4294901726 usec=163
[2024-02-02T15:45:16.212] sched: Allocate JobId=45 NodeList=compute[01-02] #CPUs=8 Partition=normal
[2024-02-02T15:45:16.293] _job_complete: JobId=45 WEXITSTATUS 0
[2024-02-02T15:45:16.293] _job_complete: JobId=45 done
[2024-02-02T15:51:41.259] _slurm_rpc_submit_batch_job: JobId=46 InitPrio=4294901725 usec=151
[2024-02-02T15:51:42.161] sched: Allocate JobId=46 NodeList=compute[01-03] #CPUs=3 Partition=normal
[2024-02-02T15:51:42.243] _job_complete: JobId=46 WEXITSTATUS 0
[2024-02-02T15:51:42.243] _job_complete: JobId=46 done
[2024-02-02T15:52:52.634] _slurm_rpc_submit_batch_job: JobId=47 InitPrio=4294901724 usec=163
[2024-02-02T15:52:52.736] sched/backfill: _start_job: Started JobId=47 in normal on compute[01-03]
[2024-02-02T15:52:52.804] _job_complete: JobId=47 WEXITSTATUS 0
[2024-02-02T15:52:52.804] _job_complete: JobId=47 done
[2024-02-02T15:56:37.426] sched: _slurm_rpc_allocate_resources JobId=48 NodeList=compute01 usec=153
[2024-02-02T15:57:22.515] _job_complete: JobId=48 WEXITSTATUS 0
[2024-02-02T15:57:22.515] _job_complete: JobId=48 done
[2024-02-02T15:57:38.768] sched: _slurm_rpc_allocate_resources JobId=49 NodeList=compute01 usec=178
[2024-02-02T15:57:53.922] _job_complete: JobId=49 WEXITSTATUS 0
[2024-02-02T15:57:53.922] _job_complete: JobId=49 done
[2024-02-02T16:04:45.525] _slurm_rpc_submit_batch_job: JobId=50 InitPrio=4294901721 usec=146
[2024-02-02T16:04:46.012] sched: Allocate JobId=50 NodeList=compute[01-03] #CPUs=3 Partition=normal
[2024-02-02T16:04:46.096] _job_complete: JobId=50 WEXITSTATUS 0
[2024-02-02T16:04:46.096] _job_complete: JobId=50 done
[2024-02-02T16:07:47.623] update_node: node compute00 state set to IDLE
[2024-02-02T16:07:48.469] Node compute00 now responding
[2024-02-02T16:07:59.645] Terminate signal (SIGINT or SIGTERM) received
[2024-02-02T16:07:59.687] Saving all slurm state
[2024-02-02T16:07:59.718] error: chdir(/var/log): Permission denied
[2024-02-02T16:07:59.718] error: Configured MailProg is invalid
[2024-02-02T16:07:59.718] slurmctld version 22.05.11 started on cluster cluster
[2024-02-02T16:07:59.720] No memory enforcing mechanism configured.
[2024-02-02T16:07:59.720] Recovered state of 4 nodes
[2024-02-02T16:07:59.721] Recovered JobId=50 Assoc=0
[2024-02-02T16:07:59.721] Recovered information about 1 jobs
[2024-02-02T16:07:59.721] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T16:07:59.721] Recovered state of 0 reservations
[2024-02-02T16:07:59.721] read_slurm_conf: backup_controller not specified
[2024-02-02T16:07:59.721] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-02T16:07:59.721] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T16:07:59.721] Running as primary controller
[2024-02-02T16:07:59.721] No parameter for mcs plugin, default values set
[2024-02-02T16:07:59.721] mcs: MCSParameters = (null). ondemand set.
[2024-02-02T16:08:03.737] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:03.737] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:03.738] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:03.739] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:04.732] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-02T16:08:14.187] killing old slurmctld[10125]
[2024-02-02T16:08:14.188] Terminate signal (SIGINT or SIGTERM) received
[2024-02-02T16:08:14.255] Saving all slurm state
[2024-02-02T16:08:14.284] error: chdir(/var/log): Permission denied
[2024-02-02T16:08:14.284] error: Configured MailProg is invalid
[2024-02-02T16:08:14.284] slurmctld version 22.05.11 started on cluster cluster
[2024-02-02T16:08:14.286] No memory enforcing mechanism configured.
[2024-02-02T16:08:14.287] Recovered state of 4 nodes
[2024-02-02T16:08:14.287] Recovered JobId=50 Assoc=0
[2024-02-02T16:08:14.287] Recovered information about 1 jobs
[2024-02-02T16:08:14.287] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T16:08:14.287] Recovered state of 0 reservations
[2024-02-02T16:08:14.287] read_slurm_conf: backup_controller not specified
[2024-02-02T16:08:14.287] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-02T16:08:14.287] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T16:08:14.287] Running as primary controller
[2024-02-02T16:08:14.287] No parameter for mcs plugin, default values set
[2024-02-02T16:08:14.287] mcs: MCSParameters = (null). ondemand set.
[2024-02-02T16:08:18.304] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:18.304] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:18.304] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:18.305] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:19.299] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-02T16:08:19.656] Terminate signal (SIGINT or SIGTERM) received
[2024-02-02T16:08:19.700] Saving all slurm state
[2024-02-02T16:08:31.568] error: chdir(/var/log): Permission denied
[2024-02-02T16:08:31.568] error: Configured MailProg is invalid
[2024-02-02T16:08:31.569] slurmctld version 22.05.11 started on cluster cluster
[2024-02-02T16:08:31.570] No memory enforcing mechanism configured.
[2024-02-02T16:08:31.592] Recovered state of 4 nodes
[2024-02-02T16:08:31.592] Recovered JobId=50 Assoc=0
[2024-02-02T16:08:31.592] Recovered information about 1 jobs
[2024-02-02T16:08:31.592] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T16:08:31.592] Recovered state of 0 reservations
[2024-02-02T16:08:31.592] read_slurm_conf: backup_controller not specified
[2024-02-02T16:08:31.592] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-02T16:08:31.592] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-02T16:08:31.592] Running as primary controller
[2024-02-02T16:08:31.592] No parameter for mcs plugin, default values set
[2024-02-02T16:08:31.592] mcs: MCSParameters = (null). ondemand set.
[2024-02-02T16:08:35.607] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:35.608] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:35.608] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:35.609] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:08:36.602] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-02T16:09:03.916] _slurm_rpc_submit_batch_job: JobId=51 InitPrio=4294901720 usec=190
[2024-02-02T16:09:04.667] sched: Allocate JobId=51 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-02T16:09:04.752] _job_complete: JobId=51 WEXITSTATUS 0
[2024-02-02T16:09:04.752] _job_complete: JobId=51 done
[2024-02-02T16:14:22.964] _slurm_rpc_submit_batch_job: JobId=52 InitPrio=4294901719 usec=141
[2024-02-02T16:14:23.438] sched: Allocate JobId=52 NodeList=compute00 #CPUs=3 Partition=normal
[2024-02-02T16:14:23.523] _job_complete: JobId=52 WEXITSTATUS 0
[2024-02-02T16:14:23.523] _job_complete: JobId=52 done
[2024-02-02T16:14:56.965] _slurm_rpc_submit_batch_job: JobId=53 InitPrio=4294901718 usec=149
[2024-02-02T16:14:57.511] sched: Allocate JobId=53 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-02T16:14:57.596] _job_complete: JobId=53 WEXITSTATUS 0
[2024-02-02T16:14:57.596] _job_complete: JobId=53 done
[2024-02-02T16:24:54.817] _slurm_rpc_submit_batch_job: JobId=54 InitPrio=4294901717 usec=171
[2024-02-02T16:24:54.991] sched: Allocate JobId=54 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:25:39.456] _job_complete: JobId=54 WEXITSTATUS 0
[2024-02-02T16:25:39.456] _job_complete: JobId=54 done
[2024-02-02T16:26:24.611] _slurm_rpc_submit_batch_job: JobId=55 InitPrio=4294901716 usec=171
[2024-02-02T16:26:25.161] sched: Allocate JobId=55 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:27:10.787] _job_complete: JobId=55 WEXITSTATUS 0
[2024-02-02T16:27:10.787] _job_complete: JobId=55 done
[2024-02-02T16:28:25.783] _slurm_rpc_submit_batch_job: JobId=56 InitPrio=4294901715 usec=222
[2024-02-02T16:28:26.349] _slurm_rpc_submit_batch_job: JobId=57 InitPrio=4294901714 usec=179
[2024-02-02T16:28:26.398] sched: Allocate JobId=56 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:26.398] sched: Allocate JobId=57 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:26.941] _slurm_rpc_submit_batch_job: JobId=58 InitPrio=4294901713 usec=139
[2024-02-02T16:28:27.447] _slurm_rpc_submit_batch_job: JobId=59 InitPrio=4294901712 usec=162
[2024-02-02T16:28:27.928] _slurm_rpc_submit_batch_job: JobId=60 InitPrio=4294901711 usec=132
[2024-02-02T16:28:28.392] _slurm_rpc_submit_batch_job: JobId=61 InitPrio=4294901710 usec=154
[2024-02-02T16:28:28.867] _slurm_rpc_submit_batch_job: JobId=62 InitPrio=4294901709 usec=163
[2024-02-02T16:28:29.351] _slurm_rpc_submit_batch_job: JobId=63 InitPrio=4294901708 usec=158
[2024-02-02T16:28:29.404] sched: Allocate JobId=58 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:29.405] sched: Allocate JobId=59 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:29.405] sched: Allocate JobId=60 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:29.405] sched: Allocate JobId=61 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:29.405] sched: Allocate JobId=62 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:29.405] sched: Allocate JobId=63 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:29.831] _slurm_rpc_submit_batch_job: JobId=64 InitPrio=4294901707 usec=133
[2024-02-02T16:28:30.302] _slurm_rpc_submit_batch_job: JobId=65 InitPrio=4294901706 usec=109
[2024-02-02T16:28:30.737] _slurm_rpc_submit_batch_job: JobId=66 InitPrio=4294901705 usec=127
[2024-02-02T16:28:31.187] _slurm_rpc_submit_batch_job: JobId=67 InitPrio=4294901704 usec=102
[2024-02-02T16:28:31.406] sched: Allocate JobId=64 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:31.407] sched: Allocate JobId=65 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:31.407] sched: Allocate JobId=66 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:31.407] sched: Allocate JobId=67 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:31.634] _slurm_rpc_submit_batch_job: JobId=68 InitPrio=4294901703 usec=103
[2024-02-02T16:28:32.089] _slurm_rpc_submit_batch_job: JobId=69 InitPrio=4294901702 usec=104
[2024-02-02T16:28:32.527] _slurm_rpc_submit_batch_job: JobId=70 InitPrio=4294901701 usec=104
[2024-02-02T16:28:32.972] _slurm_rpc_submit_batch_job: JobId=71 InitPrio=4294901700 usec=107
[2024-02-02T16:28:33.429] _slurm_rpc_submit_batch_job: JobId=72 InitPrio=4294901699 usec=104
[2024-02-02T16:28:33.885] _slurm_rpc_submit_batch_job: JobId=73 InitPrio=4294901698 usec=99
[2024-02-02T16:28:34.315] _slurm_rpc_submit_batch_job: JobId=74 InitPrio=4294901697 usec=103
[2024-02-02T16:28:34.412] sched: Allocate JobId=68 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:34.412] sched: Allocate JobId=69 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:34.412] sched: Allocate JobId=70 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:34.412] sched: Allocate JobId=71 NodeList=compute[00-01] #CPUs=2 Partition=normal
[2024-02-02T16:28:34.412] sched: Allocate JobId=72 NodeList=compute[02-03] #CPUs=2 Partition=normal
[2024-02-02T16:28:34.412] sched: Allocate JobId=73 NodeList=compute[02-03] #CPUs=2 Partition=normal
[2024-02-02T16:28:34.412] sched: Allocate JobId=74 NodeList=compute[02-03] #CPUs=2 Partition=normal
[2024-02-02T16:28:34.998] _slurm_rpc_submit_batch_job: JobId=75 InitPrio=4294901696 usec=109
[2024-02-02T16:28:37.415] sched: Allocate JobId=75 NodeList=compute[02-03] #CPUs=2 Partition=normal
[2024-02-02T16:29:23.150] _job_complete: JobId=56 WEXITSTATUS 0
[2024-02-02T16:29:23.150] _job_complete: JobId=56 done
[2024-02-02T16:29:23.151] _job_complete: JobId=57 WEXITSTATUS 0
[2024-02-02T16:29:23.151] _job_complete: JobId=57 done
[2024-02-02T16:30:13.222] _job_complete: JobId=72 WEXITSTATUS 0
[2024-02-02T16:30:13.222] _job_complete: JobId=72 done
[2024-02-02T16:30:13.222] _job_complete: JobId=74 WEXITSTATUS 0
[2024-02-02T16:30:13.222] _job_complete: JobId=74 done
[2024-02-02T16:30:35.834] _job_complete: JobId=73 WEXITSTATUS 0
[2024-02-02T16:30:35.834] _job_complete: JobId=73 done
[2024-02-02T16:30:42.309] _job_complete: JobId=75 WEXITSTATUS 0
[2024-02-02T16:30:42.309] _job_complete: JobId=75 done
[2024-02-02T16:30:57.070] _job_complete: JobId=61 WEXITSTATUS 0
[2024-02-02T16:30:57.070] _job_complete: JobId=61 done
[2024-02-02T16:30:57.071] _job_complete: JobId=62 WEXITSTATUS 0
[2024-02-02T16:30:57.071] _job_complete: JobId=62 done
[2024-02-02T16:30:57.071] _job_complete: JobId=60 WEXITSTATUS 0
[2024-02-02T16:30:57.071] _job_complete: JobId=60 done
[2024-02-02T16:30:57.071] _job_complete: JobId=63 WEXITSTATUS 0
[2024-02-02T16:30:57.071] _job_complete: JobId=63 done
[2024-02-02T16:31:01.577] _job_complete: JobId=59 WEXITSTATUS 0
[2024-02-02T16:31:01.577] _job_complete: JobId=59 done
[2024-02-02T16:31:03.351] _job_complete: JobId=58 WEXITSTATUS 0
[2024-02-02T16:31:03.351] _job_complete: JobId=58 done
[2024-02-02T16:31:10.270] _job_complete: JobId=70 WEXITSTATUS 0
[2024-02-02T16:31:10.270] _job_complete: JobId=70 done
[2024-02-02T16:31:10.270] _job_complete: JobId=66 WEXITSTATUS 0
[2024-02-02T16:31:10.270] _job_complete: JobId=66 done
[2024-02-02T16:31:10.430] _job_complete: JobId=65 WEXITSTATUS 0
[2024-02-02T16:31:10.430] _job_complete: JobId=65 done
[2024-02-02T16:31:10.604] _job_complete: JobId=69 WEXITSTATUS 0
[2024-02-02T16:31:10.604] _job_complete: JobId=69 done
[2024-02-02T16:31:10.747] _job_complete: JobId=71 WEXITSTATUS 0
[2024-02-02T16:31:10.747] _job_complete: JobId=71 done
[2024-02-02T16:31:11.159] _job_complete: JobId=64 WEXITSTATUS 0
[2024-02-02T16:31:11.159] _job_complete: JobId=64 done
[2024-02-02T16:31:11.196] _job_complete: JobId=67 WEXITSTATUS 0
[2024-02-02T16:31:11.196] _job_complete: JobId=67 done
[2024-02-02T16:31:11.296] _job_complete: JobId=68 WEXITSTATUS 0
[2024-02-02T16:31:11.296] _job_complete: JobId=68 done
[2024-02-02T16:33:01.053] _slurm_rpc_submit_batch_job: JobId=76 InitPrio=4294901695 usec=154
[2024-02-02T16:33:01.803] sched: Allocate JobId=76 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:02.035] _slurm_rpc_submit_batch_job: JobId=77 InitPrio=4294901694 usec=152
[2024-02-02T16:33:02.721] sched/backfill: _start_job: Started JobId=77 in normal on compute[00-03]
[2024-02-02T16:33:02.858] _slurm_rpc_submit_batch_job: JobId=78 InitPrio=4294901693 usec=111
[2024-02-02T16:33:03.444] _slurm_rpc_submit_batch_job: JobId=79 InitPrio=4294901692 usec=125
[2024-02-02T16:33:04.023] _slurm_rpc_submit_batch_job: JobId=80 InitPrio=4294901691 usec=103
[2024-02-02T16:33:04.654] _slurm_rpc_submit_batch_job: JobId=81 InitPrio=4294901690 usec=102
[2024-02-02T16:33:04.807] sched: Allocate JobId=78 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:04.807] sched: Allocate JobId=79 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:04.807] sched: Allocate JobId=80 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:04.807] sched: Allocate JobId=81 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:05.153] _slurm_rpc_submit_batch_job: JobId=82 InitPrio=4294901689 usec=102
[2024-02-02T16:33:05.648] _slurm_rpc_submit_batch_job: JobId=83 InitPrio=4294901688 usec=103
[2024-02-02T16:33:06.121] _slurm_rpc_submit_batch_job: JobId=84 InitPrio=4294901687 usec=137
[2024-02-02T16:33:06.603] _slurm_rpc_submit_batch_job: JobId=85 InitPrio=4294901686 usec=101
[2024-02-02T16:33:07.072] _slurm_rpc_submit_batch_job: JobId=86 InitPrio=4294901685 usec=99
[2024-02-02T16:33:07.541] _slurm_rpc_submit_batch_job: JobId=87 InitPrio=4294901684 usec=104
[2024-02-02T16:33:07.810] sched: Allocate JobId=82 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:07.810] sched: Allocate JobId=83 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:07.810] sched: Allocate JobId=84 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:07.811] sched: Allocate JobId=85 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:07.811] sched: Allocate JobId=86 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:07.811] sched: Allocate JobId=87 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:08.001] _slurm_rpc_submit_batch_job: JobId=88 InitPrio=4294901683 usec=107
[2024-02-02T16:33:08.451] _slurm_rpc_submit_batch_job: JobId=89 InitPrio=4294901682 usec=103
[2024-02-02T16:33:08.897] _slurm_rpc_submit_batch_job: JobId=90 InitPrio=4294901681 usec=100
[2024-02-02T16:33:09.348] _slurm_rpc_submit_batch_job: JobId=91 InitPrio=4294901680 usec=121
[2024-02-02T16:33:09.788] _slurm_rpc_submit_batch_job: JobId=92 InitPrio=4294901679 usec=104
[2024-02-02T16:33:10.255] _slurm_rpc_submit_batch_job: JobId=93 InitPrio=4294901678 usec=105
[2024-02-02T16:33:10.719] _slurm_rpc_submit_batch_job: JobId=94 InitPrio=4294901677 usec=106
[2024-02-02T16:33:10.814] sched: Allocate JobId=88 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:10.814] sched: Allocate JobId=89 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:10.814] sched: Allocate JobId=90 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:10.814] sched: Allocate JobId=91 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:33:11.165] _slurm_rpc_submit_batch_job: JobId=95 InitPrio=4294901676 usec=105
[2024-02-02T16:33:11.635] _slurm_rpc_submit_batch_job: JobId=96 InitPrio=4294901675 usec=99
[2024-02-02T16:33:12.077] _slurm_rpc_submit_batch_job: JobId=97 InitPrio=4294901674 usec=100
[2024-02-02T16:33:12.533] _slurm_rpc_submit_batch_job: JobId=98 InitPrio=4294901673 usec=102
[2024-02-02T16:33:12.993] _slurm_rpc_submit_batch_job: JobId=99 InitPrio=4294901672 usec=175
[2024-02-02T16:33:13.432] _slurm_rpc_submit_batch_job: JobId=100 InitPrio=4294901671 usec=148
[2024-02-02T16:33:13.872] _slurm_rpc_submit_batch_job: JobId=101 InitPrio=4294901670 usec=134
[2024-02-02T16:33:14.334] _slurm_rpc_submit_batch_job: JobId=102 InitPrio=4294901669 usec=127
[2024-02-02T16:33:14.771] _slurm_rpc_submit_batch_job: JobId=103 InitPrio=4294901668 usec=103
[2024-02-02T16:33:15.224] _slurm_rpc_submit_batch_job: JobId=104 InitPrio=4294901667 usec=128
[2024-02-02T16:33:15.676] _slurm_rpc_submit_batch_job: JobId=105 InitPrio=4294901666 usec=120
[2024-02-02T16:33:16.147] _slurm_rpc_submit_batch_job: JobId=106 InitPrio=4294901665 usec=101
[2024-02-02T16:33:16.594] _slurm_rpc_submit_batch_job: JobId=107 InitPrio=4294901664 usec=102
[2024-02-02T16:33:17.042] _slurm_rpc_submit_batch_job: JobId=108 InitPrio=4294901663 usec=20705
[2024-02-02T16:33:17.467] _slurm_rpc_submit_batch_job: JobId=109 InitPrio=4294901662 usec=101
[2024-02-02T16:33:17.922] _slurm_rpc_submit_batch_job: JobId=110 InitPrio=4294901661 usec=100
[2024-02-02T16:33:18.391] _slurm_rpc_submit_batch_job: JobId=111 InitPrio=4294901660 usec=97
[2024-02-02T16:33:18.865] _slurm_rpc_submit_batch_job: JobId=112 InitPrio=4294901659 usec=101
[2024-02-02T16:33:19.298] _slurm_rpc_submit_batch_job: JobId=113 InitPrio=4294901658 usec=122
[2024-02-02T16:33:19.805] _slurm_rpc_submit_batch_job: JobId=114 InitPrio=4294901657 usec=120
[2024-02-02T16:33:20.287] _slurm_rpc_submit_batch_job: JobId=115 InitPrio=4294901656 usec=155
[2024-02-02T16:33:20.759] _slurm_rpc_submit_batch_job: JobId=116 InitPrio=4294901655 usec=168
[2024-02-02T16:33:21.201] _slurm_rpc_submit_batch_job: JobId=117 InitPrio=4294901654 usec=155
[2024-02-02T16:33:21.660] _slurm_rpc_submit_batch_job: JobId=118 InitPrio=4294901653 usec=171
[2024-02-02T16:33:22.131] _slurm_rpc_submit_batch_job: JobId=119 InitPrio=4294901652 usec=171
[2024-02-02T16:33:22.593] _slurm_rpc_submit_batch_job: JobId=120 InitPrio=4294901651 usec=149
[2024-02-02T16:33:23.064] _slurm_rpc_submit_batch_job: JobId=121 InitPrio=4294901650 usec=137
[2024-02-02T16:33:23.518] _slurm_rpc_submit_batch_job: JobId=122 InitPrio=4294901649 usec=149
[2024-02-02T16:33:24.006] _slurm_rpc_submit_batch_job: JobId=123 InitPrio=4294901648 usec=1425
[2024-02-02T16:33:24.785] _slurm_rpc_submit_batch_job: JobId=124 InitPrio=4294901647 usec=20892
[2024-02-02T16:33:25.235] _slurm_rpc_submit_batch_job: JobId=125 InitPrio=4294901646 usec=148
[2024-02-02T16:33:25.714] _slurm_rpc_submit_batch_job: JobId=126 InitPrio=4294901645 usec=150
[2024-02-02T16:33:26.183] _slurm_rpc_submit_batch_job: JobId=127 InitPrio=4294901644 usec=141
[2024-02-02T16:33:27.184] _slurm_rpc_submit_batch_job: JobId=128 InitPrio=4294901643 usec=136
[2024-02-02T16:33:27.714] _slurm_rpc_submit_batch_job: JobId=129 InitPrio=4294901642 usec=155
[2024-02-02T16:33:28.253] _slurm_rpc_submit_batch_job: JobId=130 InitPrio=4294901641 usec=141
[2024-02-02T16:33:28.770] _slurm_rpc_submit_batch_job: JobId=131 InitPrio=4294901640 usec=150
[2024-02-02T16:33:29.258] _slurm_rpc_submit_batch_job: JobId=132 InitPrio=4294901639 usec=103
[2024-02-02T16:33:29.752] _slurm_rpc_submit_batch_job: JobId=133 InitPrio=4294901638 usec=20810
[2024-02-02T16:33:30.440] _slurm_rpc_submit_batch_job: JobId=134 InitPrio=4294901637 usec=104
[2024-02-02T16:33:30.939] _slurm_rpc_submit_batch_job: JobId=135 InitPrio=4294901636 usec=105
[2024-02-02T16:33:31.425] _slurm_rpc_submit_batch_job: JobId=136 InitPrio=4294901635 usec=133
[2024-02-02T16:33:31.900] _slurm_rpc_submit_batch_job: JobId=137 InitPrio=4294901634 usec=129
[2024-02-02T16:33:33.169] _slurm_rpc_submit_batch_job: JobId=138 InitPrio=4294901633 usec=158
[2024-02-02T16:33:33.704] _slurm_rpc_submit_batch_job: JobId=139 InitPrio=4294901632 usec=108
[2024-02-02T16:33:34.213] _slurm_rpc_submit_batch_job: JobId=140 InitPrio=4294901631 usec=104
[2024-02-02T16:33:34.759] _slurm_rpc_submit_batch_job: JobId=141 InitPrio=4294901630 usec=131
[2024-02-02T16:33:35.255] _slurm_rpc_submit_batch_job: JobId=142 InitPrio=4294901629 usec=104
[2024-02-02T16:33:35.757] _slurm_rpc_submit_batch_job: JobId=143 InitPrio=4294901628 usec=140
[2024-02-02T16:33:36.269] _slurm_rpc_submit_batch_job: JobId=144 InitPrio=4294901627 usec=106
[2024-02-02T16:33:37.187] _slurm_rpc_submit_batch_job: JobId=145 InitPrio=4294901626 usec=119
[2024-02-02T16:34:03.037] _job_complete: JobId=76 WEXITSTATUS 0
[2024-02-02T16:34:03.037] _job_complete: JobId=76 done
[2024-02-02T16:34:03.122] sched: Allocate JobId=92 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:18.990] _job_complete: JobId=82 WEXITSTATUS 0
[2024-02-02T16:34:18.990] _job_complete: JobId=82 done
[2024-02-02T16:34:19.159] sched: Allocate JobId=93 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:19.659] _job_complete: JobId=83 WEXITSTATUS 0
[2024-02-02T16:34:19.659] _job_complete: JobId=83 done
[2024-02-02T16:34:19.752] sched: Allocate JobId=94 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:20.892] _job_complete: JobId=77 WEXITSTATUS 0
[2024-02-02T16:34:20.892] _job_complete: JobId=77 done
[2024-02-02T16:34:20.893] _job_complete: JobId=80 WEXITSTATUS 0
[2024-02-02T16:34:20.893] _job_complete: JobId=80 done
[2024-02-02T16:34:21.089] sched: Allocate JobId=95 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:21.089] sched: Allocate JobId=96 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:21.206] _job_complete: JobId=78 WEXITSTATUS 0
[2024-02-02T16:34:21.206] _job_complete: JobId=78 done
[2024-02-02T16:34:21.289] sched: Allocate JobId=97 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:21.751] _job_complete: JobId=81 WEXITSTATUS 0
[2024-02-02T16:34:21.751] _job_complete: JobId=81 done
[2024-02-02T16:34:21.805] sched: Allocate JobId=98 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:26.565] _job_complete: JobId=79 WEXITSTATUS 0
[2024-02-02T16:34:26.565] _job_complete: JobId=79 done
[2024-02-02T16:34:26.749] sched: Allocate JobId=99 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:43.479] _job_complete: JobId=87 WEXITSTATUS 0
[2024-02-02T16:34:43.479] _job_complete: JobId=87 done
[2024-02-02T16:34:43.596] _job_complete: JobId=86 WEXITSTATUS 0
[2024-02-02T16:34:43.596] _job_complete: JobId=86 done
[2024-02-02T16:34:43.681] sched: Allocate JobId=100 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:43.681] sched: Allocate JobId=101 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:45.124] _job_complete: JobId=85 WEXITSTATUS 0
[2024-02-02T16:34:45.124] _job_complete: JobId=85 done
[2024-02-02T16:34:45.259] sched: Allocate JobId=102 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:48.252] _job_complete: JobId=84 WEXITSTATUS 0
[2024-02-02T16:34:48.252] _job_complete: JobId=84 done
[2024-02-02T16:34:48.444] sched: Allocate JobId=103 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:57.760] _job_complete: JobId=89 WEXITSTATUS 0
[2024-02-02T16:34:57.761] _job_complete: JobId=89 done
[2024-02-02T16:34:57.892] sched: Allocate JobId=104 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:34:58.754] _job_complete: JobId=91 WEXITSTATUS 0
[2024-02-02T16:34:58.754] _job_complete: JobId=91 done
[2024-02-02T16:34:58.843] sched: Allocate JobId=105 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:35:00.150] _job_complete: JobId=90 WEXITSTATUS 0
[2024-02-02T16:35:00.150] _job_complete: JobId=90 done
[2024-02-02T16:35:00.203] sched: Allocate JobId=106 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:35:01.620] _job_complete: JobId=88 WEXITSTATUS 0
[2024-02-02T16:35:01.620] _job_complete: JobId=88 done
[2024-02-02T16:35:01.673] sched: Allocate JobId=107 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:35:27.214] _slurm_rpc_submit_batch_job: JobId=146 InitPrio=4294901625 usec=164
[2024-02-02T16:35:43.744] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=146 uid 1001
[2024-02-02T16:35:52.609] _job_complete: JobId=94 WEXITSTATUS 0
[2024-02-02T16:35:52.609] _job_complete: JobId=94 done
[2024-02-02T16:35:52.692] sched: Allocate JobId=108 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:35:54.426] _job_complete: JobId=93 WEXITSTATUS 0
[2024-02-02T16:35:54.426] _job_complete: JobId=93 done
[2024-02-02T16:35:54.477] sched: Allocate JobId=109 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:35:56.009] _job_complete: JobId=92 WEXITSTATUS 0
[2024-02-02T16:35:56.009] _job_complete: JobId=92 done
[2024-02-02T16:35:56.103] sched: Allocate JobId=110 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:35:56.546] _job_complete: JobId=95 WEXITSTATUS 0
[2024-02-02T16:35:56.546] _job_complete: JobId=95 done
[2024-02-02T16:35:56.706] sched: Allocate JobId=111 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:35:57.254] _job_complete: JobId=96 WEXITSTATUS 0
[2024-02-02T16:35:57.254] _job_complete: JobId=96 done
[2024-02-02T16:35:57.306] sched: Allocate JobId=112 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:36:05.832] _job_complete: JobId=97 WEXITSTATUS 0
[2024-02-02T16:36:05.832] _job_complete: JobId=97 done
[2024-02-02T16:36:06.023] _job_complete: JobId=98 WEXITSTATUS 0
[2024-02-02T16:36:06.023] _job_complete: JobId=98 done
[2024-02-02T16:36:06.193] sched: Allocate JobId=113 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:36:06.193] sched: Allocate JobId=114 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:36:24.704] _job_complete: JobId=99 WEXITSTATUS 0
[2024-02-02T16:36:24.704] _job_complete: JobId=99 done
[2024-02-02T16:36:24.756] sched: Allocate JobId=115 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:36:59.196] _slurm_rpc_submit_batch_job: JobId=147 InitPrio=4294901624 usec=139
[2024-02-02T16:37:08.838] _job_complete: JobId=101 WEXITSTATUS 0
[2024-02-02T16:37:08.839] _job_complete: JobId=101 done
[2024-02-02T16:37:08.915] sched: Allocate JobId=116 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:37:10.318] _job_complete: JobId=102 WEXITSTATUS 0
[2024-02-02T16:37:10.318] _job_complete: JobId=102 done
[2024-02-02T16:37:10.407] sched: Allocate JobId=117 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:37:11.891] _job_complete: JobId=100 WEXITSTATUS 0
[2024-02-02T16:37:11.891] _job_complete: JobId=100 done
[2024-02-02T16:37:11.938] sched: Allocate JobId=118 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:37:15.032] _job_complete: JobId=103 WEXITSTATUS 0
[2024-02-02T16:37:15.032] _job_complete: JobId=103 done
[2024-02-02T16:37:15.190] sched: Allocate JobId=119 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:37:24.499] _job_complete: JobId=104 WEXITSTATUS 0
[2024-02-02T16:37:24.499] _job_complete: JobId=104 done
[2024-02-02T16:37:24.594] sched: Allocate JobId=120 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:37:25.688] _job_complete: JobId=105 WEXITSTATUS 0
[2024-02-02T16:37:25.688] _job_complete: JobId=105 done
[2024-02-02T16:37:25.737] sched: Allocate JobId=121 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:37:41.341] _slurm_rpc_submit_batch_job: JobId=148 InitPrio=4294901623 usec=137
[2024-02-02T16:37:41.954] _job_complete: JobId=106 WEXITSTATUS 0
[2024-02-02T16:37:41.954] _job_complete: JobId=106 done
[2024-02-02T16:37:42.044] sched: Allocate JobId=122 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:37:43.457] _job_complete: JobId=107 WEXITSTATUS 0
[2024-02-02T16:37:43.457] _job_complete: JobId=107 done
[2024-02-02T16:37:43.506] sched: Allocate JobId=123 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:38:49.689] _job_complete: JobId=109 WEXITSTATUS 0
[2024-02-02T16:38:49.689] _job_complete: JobId=109 done
[2024-02-02T16:38:49.781] sched: Allocate JobId=124 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:39:06.765] _job_complete: JobId=110 WEXITSTATUS 0
[2024-02-02T16:39:06.766] _job_complete: JobId=110 done
[2024-02-02T16:39:06.933] sched: Allocate JobId=125 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:39:11.732] _job_complete: JobId=108 WEXITSTATUS 0
[2024-02-02T16:39:11.732] _job_complete: JobId=108 done
[2024-02-02T16:39:11.780] sched: Allocate JobId=126 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:39:33.158] _job_complete: JobId=113 WEXITSTATUS 0
[2024-02-02T16:39:33.158] _job_complete: JobId=113 done
[2024-02-02T16:39:33.261] sched: Allocate JobId=127 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:39:39.999] _job_complete: JobId=112 WEXITSTATUS 0
[2024-02-02T16:39:39.999] _job_complete: JobId=112 done
[2024-02-02T16:39:40.180] sched: Allocate JobId=128 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:39:48.777] _job_complete: JobId=111 WEXITSTATUS 0
[2024-02-02T16:39:48.777] _job_complete: JobId=111 done
[2024-02-02T16:39:48.824] sched: Allocate JobId=129 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:39:54.266] _job_complete: JobId=114 WEXITSTATUS 0
[2024-02-02T16:39:54.266] _job_complete: JobId=114 done
[2024-02-02T16:39:54.429] sched: Allocate JobId=130 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:40:11.386] _job_complete: JobId=115 WEXITSTATUS 0
[2024-02-02T16:40:11.386] _job_complete: JobId=115 done
[2024-02-02T16:40:11.432] sched: Allocate JobId=131 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:40:15.379] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:40:15.379] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:40:15.379] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:40:15.414] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T16:41:01.288] _job_complete: JobId=118 WEXITSTATUS 0
[2024-02-02T16:41:01.288] _job_complete: JobId=118 done
[2024-02-02T16:41:01.426] sched: Allocate JobId=132 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:41:01.436] _job_complete: JobId=117 WEXITSTATUS 0
[2024-02-02T16:41:01.436] _job_complete: JobId=117 done
[2024-02-02T16:41:01.604] sched: Allocate JobId=133 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:41:01.720] _job_complete: JobId=119 WEXITSTATUS 0
[2024-02-02T16:41:01.720] _job_complete: JobId=119 done
[2024-02-02T16:41:01.904] sched: Allocate JobId=134 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:41:02.007] _job_complete: JobId=116 WEXITSTATUS 0
[2024-02-02T16:41:02.007] _job_complete: JobId=116 done
[2024-02-02T16:41:02.053] sched: Allocate JobId=135 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:41:21.267] _job_complete: JobId=120 WEXITSTATUS 0
[2024-02-02T16:41:21.268] _job_complete: JobId=120 done
[2024-02-02T16:41:21.338] sched: Allocate JobId=136 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:41:25.386] _job_complete: JobId=121 WEXITSTATUS 0
[2024-02-02T16:41:25.386] _job_complete: JobId=121 done
[2024-02-02T16:41:25.432] sched: Allocate JobId=137 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:41:58.307] _job_complete: JobId=123 WEXITSTATUS 0
[2024-02-02T16:41:58.307] _job_complete: JobId=123 done
[2024-02-02T16:41:58.381] sched: Allocate JobId=138 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:42:03.913] _job_complete: JobId=122 WEXITSTATUS 0
[2024-02-02T16:42:03.913] _job_complete: JobId=122 done
[2024-02-02T16:42:04.063] sched: Allocate JobId=139 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:42:32.876] _job_complete: JobId=124 WEXITSTATUS 0
[2024-02-02T16:42:32.876] _job_complete: JobId=124 done
[2024-02-02T16:42:32.944] sched: Allocate JobId=140 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:42:38.280] _job_complete: JobId=125 WEXITSTATUS 0
[2024-02-02T16:42:38.280] _job_complete: JobId=125 done
[2024-02-02T16:42:38.446] sched: Allocate JobId=141 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:42:41.460] _job_complete: JobId=126 WEXITSTATUS 0
[2024-02-02T16:42:41.460] _job_complete: JobId=126 done
[2024-02-02T16:42:41.500] sched: Allocate JobId=142 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:43:09.352] _job_complete: JobId=128 WEXITSTATUS 0
[2024-02-02T16:43:09.352] _job_complete: JobId=128 done
[2024-02-02T16:43:09.475] sched: Allocate JobId=143 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:43:16.096] _job_complete: JobId=127 WEXITSTATUS 0
[2024-02-02T16:43:16.096] _job_complete: JobId=127 done
[2024-02-02T16:43:16.136] sched: Allocate JobId=144 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:43:31.928] _job_complete: JobId=131 WEXITSTATUS 0
[2024-02-02T16:43:31.928] _job_complete: JobId=131 done
[2024-02-02T16:43:31.986] sched: Allocate JobId=145 NodeList=compute[00-03] #CPUs=4 Partition=normal
[2024-02-02T16:43:38.741] _job_complete: JobId=129 WEXITSTATUS 0
[2024-02-02T16:43:38.741] _job_complete: JobId=129 done
[2024-02-02T16:43:38.915] sched: Allocate JobId=147 NodeList=compute00 #CPUs=1 Partition=normal
[2024-02-02T16:43:38.915] sched: Allocate JobId=148 NodeList=compute01 #CPUs=1 Partition=normal
[2024-02-02T16:43:41.839] _job_complete: JobId=130 WEXITSTATUS 0
[2024-02-02T16:43:41.839] _job_complete: JobId=130 done
[2024-02-02T16:44:50.512] _job_complete: JobId=148 WEXITSTATUS 0
[2024-02-02T16:44:50.512] _job_complete: JobId=148 done
[2024-02-02T16:45:03.926] _job_complete: JobId=133 WEXITSTATUS 0
[2024-02-02T16:45:03.926] _job_complete: JobId=133 done
[2024-02-02T16:45:03.926] _job_complete: JobId=135 WEXITSTATUS 0
[2024-02-02T16:45:03.926] _job_complete: JobId=135 done
[2024-02-02T16:45:04.391] _job_complete: JobId=134 WEXITSTATUS 0
[2024-02-02T16:45:04.391] _job_complete: JobId=134 done
[2024-02-02T16:45:12.180] _job_complete: JobId=132 WEXITSTATUS 0
[2024-02-02T16:45:12.180] _job_complete: JobId=132 done
[2024-02-02T16:45:16.569] _job_complete: JobId=140 WEXITSTATUS 0
[2024-02-02T16:45:16.569] _job_complete: JobId=140 done
[2024-02-02T16:45:16.573] _job_complete: JobId=136 WEXITSTATUS 0
[2024-02-02T16:45:16.573] _job_complete: JobId=136 done
[2024-02-02T16:45:17.529] _job_complete: JobId=141 WEXITSTATUS 0
[2024-02-02T16:45:17.529] _job_complete: JobId=141 done
[2024-02-02T16:45:25.217] _job_complete: JobId=137 WEXITSTATUS 0
[2024-02-02T16:45:25.217] _job_complete: JobId=137 done
[2024-02-02T16:45:34.209] _job_complete: JobId=143 WEXITSTATUS 0
[2024-02-02T16:45:34.209] _job_complete: JobId=143 done
[2024-02-02T16:45:37.945] _job_complete: JobId=139 WEXITSTATUS 0
[2024-02-02T16:45:37.945] _job_complete: JobId=139 done
[2024-02-02T16:45:40.630] _job_complete: JobId=138 WEXITSTATUS 0
[2024-02-02T16:45:40.630] _job_complete: JobId=138 done
[2024-02-02T16:45:43.461] _job_complete: JobId=142 WEXITSTATUS 0
[2024-02-02T16:45:43.461] _job_complete: JobId=142 done
[2024-02-02T16:45:45.378] _job_complete: JobId=147 WEXITSTATUS 0
[2024-02-02T16:45:45.378] _job_complete: JobId=147 done
[2024-02-02T16:45:49.165] _job_complete: JobId=145 WEXITSTATUS 0
[2024-02-02T16:45:49.165] _job_complete: JobId=145 done
[2024-02-02T16:45:57.346] _job_complete: JobId=144 WEXITSTATUS 0
[2024-02-02T16:45:57.346] _job_complete: JobId=144 done
[2024-02-02T17:13:35.803] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T17:13:35.803] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T17:13:35.803] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T17:13:35.804] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T17:46:55.670] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T17:46:55.670] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T17:46:55.670] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T17:46:55.670] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T18:20:15.619] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T18:20:15.620] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T18:20:15.620] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T18:20:15.621] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T18:53:35.546] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T18:53:35.546] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T18:53:35.546] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T18:53:35.546] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T19:26:55.512] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T19:26:55.512] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T19:26:55.512] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T19:26:55.512] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T20:00:15.428] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T20:00:15.428] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T20:00:15.428] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T20:00:15.428] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T20:33:35.639] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T20:33:35.639] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T20:33:35.639] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T20:33:35.641] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T21:06:55.842] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T21:06:55.842] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T21:06:55.842] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T21:06:55.843] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T21:40:15.868] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T21:40:15.868] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T21:40:15.868] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T21:40:15.869] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T22:13:35.851] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T22:13:35.851] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T22:13:35.851] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T22:13:35.852] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T22:46:55.883] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T22:46:55.883] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T22:46:55.883] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T22:46:55.883] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T23:20:15.848] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T23:20:15.848] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T23:20:15.848] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T23:20:15.849] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T23:53:35.121] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T23:53:35.121] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T23:53:35.121] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-02T23:53:35.122] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T00:26:55.152] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T00:26:55.153] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T00:26:55.153] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T00:26:55.153] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T01:00:15.132] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T01:00:15.132] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T01:00:15.132] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T01:00:15.133] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T01:33:35.182] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T01:33:35.182] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T01:33:35.182] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T01:33:35.183] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T02:06:55.155] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T02:06:55.155] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T02:06:55.155] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T02:06:55.157] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T02:40:15.195] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T02:40:15.195] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T02:40:15.195] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T02:40:15.195] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T03:13:35.205] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T03:13:35.205] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T03:13:35.205] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T03:13:35.206] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T03:46:55.147] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T03:46:55.147] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T03:46:55.147] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T03:46:55.147] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T04:20:15.139] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T04:20:15.139] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T04:20:15.139] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T04:20:15.139] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T04:53:35.165] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T04:53:35.165] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T04:53:35.165] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T04:53:35.166] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T05:26:55.183] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T05:26:55.183] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T05:26:55.183] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T05:26:55.184] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T06:00:15.187] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T06:00:15.187] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T06:00:15.187] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T06:00:15.189] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T06:33:35.179] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T06:33:35.180] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T06:33:35.180] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T06:33:35.180] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T07:06:55.176] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T07:06:55.176] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T07:06:55.176] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T07:06:55.177] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T07:40:15.352] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T07:40:15.352] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T07:40:15.353] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T07:40:15.353] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T08:13:35.393] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T08:13:35.393] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T08:13:35.393] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T08:13:35.393] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T08:46:55.410] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T08:46:55.410] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T08:46:55.410] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T08:46:55.412] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T09:20:15.460] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T09:20:15.460] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T09:20:15.460] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T09:20:15.461] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T09:53:35.590] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T09:53:35.590] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T09:53:35.590] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T09:53:35.591] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T10:26:55.558] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T10:26:55.558] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T10:26:55.558] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T10:26:55.559] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T11:00:15.599] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T11:00:15.599] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T11:00:15.599] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T11:00:15.601] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T11:33:35.696] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T11:33:35.696] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T11:33:35.696] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T11:33:35.696] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T12:06:55.715] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T12:06:55.715] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T12:06:55.715] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T12:06:55.717] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T12:40:15.822] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T12:40:15.822] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T12:40:15.822] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T12:40:15.824] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T13:13:35.841] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T13:13:35.841] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T13:13:35.841] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T13:13:35.842] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T13:46:55.849] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T13:46:55.849] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T13:46:55.849] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T13:46:55.850] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T14:20:15.869] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T14:20:15.869] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T14:20:15.869] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T14:20:15.870] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T14:53:35.845] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T14:53:35.845] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T14:53:35.845] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T14:53:35.845] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T15:26:55.876] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T15:26:55.876] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T15:26:55.876] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T15:26:55.877] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T16:00:15.835] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T16:00:15.835] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T16:00:15.836] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T16:00:15.836] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T16:33:35.869] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T16:33:35.869] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T16:33:35.869] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T16:33:35.869] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T17:06:55.759] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T17:06:55.760] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T17:06:55.760] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T17:06:55.761] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T17:40:16.707] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T17:40:16.707] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T17:40:16.707] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T17:40:16.708] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T18:13:36.661] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T18:13:36.661] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T18:13:36.662] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T18:13:36.664] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T18:46:56.634] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T18:46:56.634] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T18:46:56.634] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T18:46:56.634] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T19:20:16.679] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T19:20:16.679] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T19:20:16.679] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T19:20:16.679] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T19:53:36.794] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T19:53:36.794] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T19:53:36.794] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T19:53:36.795] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T20:26:56.821] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T20:26:56.821] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T20:26:56.821] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T20:26:56.821] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T21:00:16.028] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T21:00:16.028] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T21:00:16.028] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T21:00:16.030] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T21:33:36.034] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T21:33:36.034] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T21:33:36.034] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T21:33:36.034] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T22:06:57.001] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T22:06:57.001] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T22:06:57.001] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T22:06:57.002] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T22:40:17.002] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T22:40:17.002] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T22:40:17.002] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T22:40:17.002] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T23:13:36.016] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T23:13:36.016] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T23:13:36.016] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T23:13:36.016] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T23:46:56.980] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T23:46:56.980] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T23:46:56.980] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-03T23:46:56.980] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T00:20:16.045] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T00:20:16.045] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T00:20:16.045] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T00:20:16.046] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T00:53:36.080] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T00:53:36.080] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T00:53:36.080] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T00:53:36.080] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T01:26:56.186] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T01:26:56.186] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T01:26:56.186] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T01:26:56.187] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T02:00:16.252] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T02:00:16.252] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T02:00:16.252] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T02:00:16.253] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T02:33:36.401] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T02:33:36.401] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T02:33:36.401] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T02:33:36.402] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T03:06:56.584] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T03:06:56.584] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T03:06:56.584] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T03:06:56.585] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T03:40:16.645] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T03:40:16.645] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T03:40:16.645] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T03:40:16.646] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T04:13:36.835] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T04:13:36.835] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T04:13:36.835] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T04:13:36.836] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T04:46:56.793] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T04:46:56.794] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T04:46:56.794] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T04:46:56.795] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T05:20:16.826] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T05:20:16.826] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T05:20:16.826] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T05:20:16.827] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T05:53:36.843] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T05:53:36.843] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T05:53:36.843] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T05:53:36.844] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T06:26:56.829] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T06:26:56.829] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T06:26:56.829] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T06:26:56.830] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T07:00:16.879] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T07:00:16.879] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T07:00:16.879] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T07:00:16.880] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T07:33:36.894] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T07:33:36.894] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T07:33:36.894] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T07:33:36.895] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T08:06:56.901] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T08:06:56.901] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T08:06:56.901] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T08:06:56.902] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T08:40:16.872] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T08:40:16.872] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T08:40:16.872] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T08:40:16.873] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T09:13:36.050] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T09:13:36.050] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T09:13:36.050] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T09:13:36.050] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T09:46:56.254] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T09:46:56.254] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T09:46:56.255] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T09:46:56.255] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T10:20:16.438] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T10:20:16.439] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T10:20:16.439] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T10:20:16.440] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T10:53:36.620] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T10:53:36.620] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T10:53:36.620] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T10:53:36.622] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T11:26:56.689] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T11:26:56.689] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T11:26:56.689] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-04T11:26:56.690] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-02-05T08:07:44.016] error: chdir(/var/log): Permission denied
[2024-02-05T08:07:44.018] error: Configured MailProg is invalid
[2024-02-05T08:07:44.019] slurmctld version 22.05.11 started on cluster cluster
[2024-02-05T08:07:44.035] No memory enforcing mechanism configured.
[2024-02-05T08:07:44.038] Recovered state of 4 nodes
[2024-02-05T08:07:44.038] Recovered information about 0 jobs
[2024-02-05T08:07:44.038] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-05T08:07:44.038] Recovered state of 0 reservations
[2024-02-05T08:07:44.038] read_slurm_conf: backup_controller not specified
[2024-02-05T08:07:44.038] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-05T08:07:44.038] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-05T08:07:44.038] Running as primary controller
[2024-02-05T08:07:44.038] No parameter for mcs plugin, default values set
[2024-02-05T08:07:44.038] mcs: MCSParameters = (null). ondemand set.
[2024-02-05T08:08:44.790] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-05T08:09:28.895] error: Nodes compute[00-03] not responding, setting DOWN
[2024-02-09T06:59:29.284] Node compute02 now responding
[2024-02-09T06:59:29.284] validate_node_specs: Node compute02 unexpectedly rebooted boot_time=1707138594 last response=1707065216
[2024-02-09T06:59:29.289] Node compute03 now responding
[2024-02-09T06:59:29.289] validate_node_specs: Node compute03 unexpectedly rebooted boot_time=1707150309 last response=1707065216
[2024-02-09T06:59:29.289] Node compute01 now responding
[2024-02-09T06:59:29.289] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1707138597 last response=1707065216
[2024-02-09T06:59:29.291] Node compute00 now responding
[2024-02-09T06:59:29.292] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1707138581 last response=1707065216
[2024-02-09T07:01:52.176] Terminate signal (SIGINT or SIGTERM) received
[2024-02-09T07:01:52.179] Saving all slurm state
[2024-02-09T07:01:52.206] error: chdir(/var/log): Permission denied
[2024-02-09T07:01:52.206] error: Configured MailProg is invalid
[2024-02-09T07:01:52.206] slurmctld version 22.05.11 started on cluster cluster
[2024-02-09T07:01:52.207] No memory enforcing mechanism configured.
[2024-02-09T07:01:52.208] Recovered state of 4 nodes
[2024-02-09T07:01:52.208] Down nodes: compute[00-03]
[2024-02-09T07:01:52.208] Recovered information about 0 jobs
[2024-02-09T07:01:52.208] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-09T07:01:52.208] Recovered state of 0 reservations
[2024-02-09T07:01:52.208] read_slurm_conf: backup_controller not specified
[2024-02-09T07:01:52.208] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-09T07:01:52.208] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-09T07:01:52.208] Running as primary controller
[2024-02-09T07:01:52.208] No parameter for mcs plugin, default values set
[2024-02-09T07:01:52.208] mcs: MCSParameters = (null). ondemand set.
[2024-02-09T07:02:52.348] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-09T07:03:58.031] Terminate signal (SIGINT or SIGTERM) received
[2024-02-09T07:03:58.054] Saving all slurm state
[2024-02-09T07:03:58.084] error: chdir(/var/log): Permission denied
[2024-02-09T07:03:58.084] error: Configured MailProg is invalid
[2024-02-09T07:03:58.084] slurmctld version 22.05.11 started on cluster cluster
[2024-02-09T07:03:58.086] No memory enforcing mechanism configured.
[2024-02-09T07:03:58.087] Recovered state of 4 nodes
[2024-02-09T07:03:58.087] Down nodes: compute[00-03]
[2024-02-09T07:03:58.087] Recovered information about 0 jobs
[2024-02-09T07:03:58.087] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-09T07:03:58.087] Recovered state of 0 reservations
[2024-02-09T07:03:58.087] read_slurm_conf: backup_controller not specified
[2024-02-09T07:03:58.087] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-09T07:03:58.087] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-09T07:03:58.087] Running as primary controller
[2024-02-09T07:03:58.087] No parameter for mcs plugin, default values set
[2024-02-09T07:03:58.087] mcs: MCSParameters = (null). ondemand set.
[2024-02-09T07:04:58.165] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-09T07:09:47.163] Node compute01 now responding
[2024-02-09T07:09:47.163] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1707480206 last response=1707480116
[2024-02-09T07:09:47.167] Node compute02 now responding
[2024-02-09T07:09:47.167] validate_node_specs: Node compute02 unexpectedly rebooted boot_time=1707480204 last response=1707480116
[2024-02-09T07:09:47.171] Node compute00 now responding
[2024-02-09T07:09:47.171] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1707480200 last response=1707480116
[2024-02-09T07:10:59.520] Node compute03 now responding
[2024-02-09T07:10:59.520] validate_node_specs: Node compute03 unexpectedly rebooted boot_time=1707480560 last response=1707480116
[2024-02-09T07:12:24.647] killing old slurmctld[29124]
[2024-02-09T07:12:24.647] Terminate signal (SIGINT or SIGTERM) received
[2024-02-09T07:12:24.683] Saving all slurm state
[2024-02-09T07:12:24.710] error: chdir(/var/log): Permission denied
[2024-02-09T07:12:24.711] error: Configured MailProg is invalid
[2024-02-09T07:12:24.711] slurmctld version 22.05.11 started on cluster cluster
[2024-02-09T07:12:24.712] No memory enforcing mechanism configured.
[2024-02-09T07:12:24.713] Recovered state of 4 nodes
[2024-02-09T07:12:24.713] Down nodes: compute[00-03]
[2024-02-09T07:12:24.713] Recovered information about 0 jobs
[2024-02-09T07:12:24.713] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-09T07:12:24.713] Recovered state of 0 reservations
[2024-02-09T07:12:24.713] read_slurm_conf: backup_controller not specified
[2024-02-09T07:12:24.713] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-09T07:12:24.713] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-09T07:12:24.713] Running as primary controller
[2024-02-09T07:12:24.713] No parameter for mcs plugin, default values set
[2024-02-09T07:12:24.713] mcs: MCSParameters = (null). ondemand set.
[2024-02-09T07:12:31.046] Terminate signal (SIGINT or SIGTERM) received
[2024-02-09T07:12:31.129] Saving all slurm state
[2024-02-09T07:12:37.304] error: chdir(/var/log): Permission denied
[2024-02-09T07:12:37.304] error: Configured MailProg is invalid
[2024-02-09T07:12:37.304] slurmctld version 22.05.11 started on cluster cluster
[2024-02-09T07:12:37.306] No memory enforcing mechanism configured.
[2024-02-09T07:12:37.328] Recovered state of 4 nodes
[2024-02-09T07:12:37.328] Down nodes: compute[00-03]
[2024-02-09T07:12:37.328] Recovered information about 0 jobs
[2024-02-09T07:12:37.328] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-09T07:12:37.328] Recovered state of 0 reservations
[2024-02-09T07:12:37.328] read_slurm_conf: backup_controller not specified
[2024-02-09T07:12:37.328] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-09T07:12:37.328] select/cons_res: part_data_create_array: select/cons_res: preparing for 2 partitions
[2024-02-09T07:12:37.328] Running as primary controller
[2024-02-09T07:12:37.328] No parameter for mcs plugin, default values set
[2024-02-09T07:12:37.328] mcs: MCSParameters = (null). ondemand set.
[2024-02-09T07:13:37.468] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-09T07:13:47.937] update_node: node compute00 state set to IDLE
[2024-02-09T07:13:48.502] Node compute00 now responding
[2024-02-09T07:14:11.219] update_node: node compute01 state set to IDLE
[2024-02-09T07:14:11.219] update_node: node compute02 state set to IDLE
[2024-02-09T07:14:11.219] update_node: node compute03 state set to IDLE
[2024-02-09T07:14:11.551] Node compute02 now responding
[2024-02-09T07:14:11.551] Node compute03 now responding
[2024-02-09T07:14:11.552] Node compute01 now responding
[2024-02-09T07:22:37.108] _slurm_rpc_submit_batch_job: JobId=149 InitPrio=4294901759 usec=168
[2024-02-09T07:22:37.738] sched: Allocate JobId=149 NodeList=compute00 #CPUs=1 Partition=normal
[2024-02-09T07:22:37.768] _job_complete: JobId=149 WTERMSIG 53
[2024-02-09T07:22:37.769] _job_complete: JobId=149 done
[2024-02-09T07:22:50.717] _slurm_rpc_submit_batch_job: JobId=150 InitPrio=4294901758 usec=148
[2024-02-09T07:22:50.793] sched: Allocate JobId=150 NodeList=compute00 #CPUs=1 Partition=normal
[2024-02-09T07:22:50.823] _job_complete: JobId=150 WTERMSIG 53
[2024-02-09T07:22:50.823] _job_complete: JobId=150 done
[2024-02-09T07:28:26.362] _slurm_rpc_submit_batch_job: JobId=151 InitPrio=4294901757 usec=20694
[2024-02-09T07:28:26.600] sched: Allocate JobId=151 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:28:36.684] _job_complete: JobId=151 WEXITSTATUS 0
[2024-02-09T07:28:36.684] _job_complete: JobId=151 done
[2024-02-09T07:32:28.524] _slurm_rpc_submit_batch_job: JobId=152 InitPrio=4294901756 usec=150
[2024-02-09T07:32:29.118] _slurm_rpc_submit_batch_job: JobId=153 InitPrio=4294901755 usec=138
[2024-02-09T07:32:29.171] sched: Allocate JobId=152 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:32:29.653] _slurm_rpc_submit_batch_job: JobId=154 InitPrio=4294901754 usec=153
[2024-02-09T07:32:30.168] _slurm_rpc_submit_batch_job: JobId=155 InitPrio=4294901753 usec=146
[2024-02-09T07:32:30.680] _slurm_rpc_submit_batch_job: JobId=156 InitPrio=4294901752 usec=154
[2024-02-09T07:32:31.149] _slurm_rpc_submit_batch_job: JobId=157 InitPrio=4294901751 usec=139
[2024-02-09T07:32:31.637] _slurm_rpc_submit_batch_job: JobId=158 InitPrio=4294901750 usec=143
[2024-02-09T07:32:32.185] _slurm_rpc_submit_batch_job: JobId=159 InitPrio=4294901749 usec=149
[2024-02-09T07:32:32.700] _slurm_rpc_submit_batch_job: JobId=160 InitPrio=4294901748 usec=145
[2024-02-09T07:32:33.313] _slurm_rpc_submit_batch_job: JobId=161 InitPrio=4294901747 usec=143
[2024-02-09T07:32:33.996] _slurm_rpc_submit_batch_job: JobId=162 InitPrio=4294901746 usec=132
[2024-02-09T07:32:34.407] _slurm_rpc_submit_batch_job: JobId=163 InitPrio=4294901745 usec=142
[2024-02-09T07:32:35.023] _slurm_rpc_submit_batch_job: JobId=164 InitPrio=4294901744 usec=142
[2024-02-09T07:32:35.635] _slurm_rpc_submit_batch_job: JobId=165 InitPrio=4294901743 usec=153
[2024-02-09T07:32:36.090] _slurm_rpc_submit_batch_job: JobId=166 InitPrio=4294901742 usec=139
[2024-02-09T07:32:36.662] _slurm_rpc_submit_batch_job: JobId=167 InitPrio=4294901741 usec=134
[2024-02-09T07:32:37.278] _slurm_rpc_submit_batch_job: JobId=168 InitPrio=4294901740 usec=141
[2024-02-09T07:32:37.701] _slurm_rpc_submit_batch_job: JobId=169 InitPrio=4294901739 usec=142
[2024-02-09T07:32:38.305] _slurm_rpc_submit_batch_job: JobId=170 InitPrio=4294901738 usec=161
[2024-02-09T07:32:39.252] _job_complete: JobId=152 WEXITSTATUS 0
[2024-02-09T07:32:39.253] _job_complete: JobId=152 done
[2024-02-09T07:32:39.343] sched: Allocate JobId=153 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:32:49.419] _job_complete: JobId=153 WEXITSTATUS 0
[2024-02-09T07:32:49.419] _job_complete: JobId=153 done
[2024-02-09T07:32:49.508] sched: Allocate JobId=154 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:32:59.564] _job_complete: JobId=154 WEXITSTATUS 0
[2024-02-09T07:32:59.565] _job_complete: JobId=154 done
[2024-02-09T07:32:59.639] sched: Allocate JobId=155 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:33:09.690] _job_complete: JobId=155 WEXITSTATUS 0
[2024-02-09T07:33:09.691] _job_complete: JobId=155 done
[2024-02-09T07:33:09.763] sched: Allocate JobId=156 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:33:19.817] _job_complete: JobId=156 WEXITSTATUS 0
[2024-02-09T07:33:19.817] _job_complete: JobId=156 done
[2024-02-09T07:33:19.889] sched: Allocate JobId=157 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:33:29.943] _job_complete: JobId=157 WEXITSTATUS 0
[2024-02-09T07:33:29.943] _job_complete: JobId=157 done
[2024-02-09T07:33:30.018] sched: Allocate JobId=158 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:33:40.072] _job_complete: JobId=158 WEXITSTATUS 0
[2024-02-09T07:33:40.072] _job_complete: JobId=158 done
[2024-02-09T07:33:40.145] sched: Allocate JobId=159 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:33:50.197] _job_complete: JobId=159 WEXITSTATUS 0
[2024-02-09T07:33:50.197] _job_complete: JobId=159 done
[2024-02-09T07:33:50.289] sched: Allocate JobId=160 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:34:00.342] _job_complete: JobId=160 WEXITSTATUS 0
[2024-02-09T07:34:00.342] _job_complete: JobId=160 done
[2024-02-09T07:34:00.431] sched: Allocate JobId=161 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:34:10.485] _job_complete: JobId=161 WEXITSTATUS 0
[2024-02-09T07:34:10.485] _job_complete: JobId=161 done
[2024-02-09T07:34:10.559] sched: Allocate JobId=162 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:34:20.614] _job_complete: JobId=162 WEXITSTATUS 0
[2024-02-09T07:34:20.614] _job_complete: JobId=162 done
[2024-02-09T07:34:20.686] sched: Allocate JobId=163 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:34:30.738] _job_complete: JobId=163 WEXITSTATUS 0
[2024-02-09T07:34:30.738] _job_complete: JobId=163 done
[2024-02-09T07:34:30.811] sched: Allocate JobId=164 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:34:40.865] _job_complete: JobId=164 WEXITSTATUS 0
[2024-02-09T07:34:40.865] _job_complete: JobId=164 done
[2024-02-09T07:34:40.937] sched: Allocate JobId=165 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:34:50.992] _job_complete: JobId=165 WEXITSTATUS 0
[2024-02-09T07:34:50.992] _job_complete: JobId=165 done
[2024-02-09T07:34:51.066] sched: Allocate JobId=166 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:35:01.121] _job_complete: JobId=166 WEXITSTATUS 0
[2024-02-09T07:35:01.121] _job_complete: JobId=166 done
[2024-02-09T07:35:01.194] sched: Allocate JobId=167 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:35:11.248] _job_complete: JobId=167 WEXITSTATUS 0
[2024-02-09T07:35:11.248] _job_complete: JobId=167 done
[2024-02-09T07:35:11.340] sched: Allocate JobId=168 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:35:21.394] _job_complete: JobId=168 WEXITSTATUS 0
[2024-02-09T07:35:21.394] _job_complete: JobId=168 done
[2024-02-09T07:35:21.480] sched: Allocate JobId=169 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:35:31.537] _job_complete: JobId=169 WEXITSTATUS 0
[2024-02-09T07:35:31.537] _job_complete: JobId=169 done
[2024-02-09T07:35:31.611] sched: Allocate JobId=170 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T07:35:41.664] _job_complete: JobId=170 WEXITSTATUS 0
[2024-02-09T07:35:41.664] _job_complete: JobId=170 done
[2024-02-09T13:21:55.099] _slurm_rpc_submit_batch_job: JobId=171 InitPrio=4294901737 usec=142
[2024-02-09T13:21:55.361] sched: Allocate JobId=171 NodeList=compute00 #CPUs=1 Partition=normal
[2024-02-09T13:21:55.392] _job_complete: JobId=171 WTERMSIG 53
[2024-02-09T13:21:55.392] _job_complete: JobId=171 done
[2024-02-09T13:22:39.490] _slurm_rpc_submit_batch_job: JobId=172 InitPrio=4294901736 usec=144
[2024-02-09T13:22:39.490] sched: Allocate JobId=172 NodeList=compute00 #CPUs=1 Partition=normal
[2024-02-09T13:22:39.520] _job_complete: JobId=172 WTERMSIG 53
[2024-02-09T13:22:39.520] _job_complete: JobId=172 done
[2024-02-09T13:22:40.172] _slurm_rpc_submit_batch_job: JobId=173 InitPrio=4294901735 usec=171
[2024-02-09T13:22:40.745] _slurm_rpc_submit_batch_job: JobId=174 InitPrio=4294901734 usec=143
[2024-02-09T13:22:41.246] _slurm_rpc_submit_batch_job: JobId=175 InitPrio=4294901733 usec=135
[2024-02-09T13:22:41.737] _slurm_rpc_submit_batch_job: JobId=176 InitPrio=4294901732 usec=146
[2024-02-09T13:22:42.498] sched: Allocate JobId=173 NodeList=compute00 #CPUs=1 Partition=normal
[2024-02-09T13:22:42.498] sched: Allocate JobId=174 NodeList=compute00 #CPUs=1 Partition=normal
[2024-02-09T13:22:42.498] sched: Allocate JobId=175 NodeList=compute00 #CPUs=1 Partition=normal
[2024-02-09T13:22:42.498] sched: Allocate JobId=176 NodeList=compute00 #CPUs=1 Partition=normal
[2024-02-09T13:22:42.520] _job_complete: JobId=175 WTERMSIG 53
[2024-02-09T13:22:42.520] _job_complete: JobId=175 done
[2024-02-09T13:22:42.520] _job_complete: JobId=174 WTERMSIG 53
[2024-02-09T13:22:42.520] _job_complete: JobId=174 done
[2024-02-09T13:22:42.520] _job_complete: JobId=176 WTERMSIG 53
[2024-02-09T13:22:42.520] _job_complete: JobId=176 done
[2024-02-09T13:22:42.531] _job_complete: JobId=173 WTERMSIG 53
[2024-02-09T13:22:42.531] _job_complete: JobId=173 done
[2024-02-09T13:24:57.096] _slurm_rpc_submit_batch_job: JobId=177 InitPrio=4294901731 usec=149
[2024-02-09T13:24:57.396] sched/backfill: _start_job: Started JobId=177 in normal on compute[00-02]
[2024-02-09T13:25:07.480] _job_complete: JobId=177 WEXITSTATUS 0
[2024-02-09T13:25:07.480] _job_complete: JobId=177 done
[2024-02-09T13:26:12.782] _slurm_rpc_submit_batch_job: JobId=178 InitPrio=4294901730 usec=160
[2024-02-09T13:26:12.994] sched: Allocate JobId=178 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:26:13.366] _slurm_rpc_submit_batch_job: JobId=179 InitPrio=4294901729 usec=169
[2024-02-09T13:26:23.076] _job_complete: JobId=178 WEXITSTATUS 0
[2024-02-09T13:26:23.077] _job_complete: JobId=178 done
[2024-02-09T13:26:23.165] sched: Allocate JobId=179 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:26:33.220] _job_complete: JobId=179 WEXITSTATUS 0
[2024-02-09T13:26:33.220] _job_complete: JobId=179 done
[2024-02-09T13:26:43.467] _slurm_rpc_submit_batch_job: JobId=180 InitPrio=4294901728 usec=148
[2024-02-09T13:26:44.063] sched: Allocate JobId=180 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:26:44.069] _slurm_rpc_submit_batch_job: JobId=181 InitPrio=4294901727 usec=136
[2024-02-09T13:26:44.525] _slurm_rpc_submit_batch_job: JobId=182 InitPrio=4294901726 usec=149
[2024-02-09T13:26:44.909] _slurm_rpc_submit_batch_job: JobId=183 InitPrio=4294901725 usec=149
[2024-02-09T13:26:45.267] _slurm_rpc_submit_batch_job: JobId=184 InitPrio=4294901724 usec=154
[2024-02-09T13:26:45.614] _slurm_rpc_submit_batch_job: JobId=185 InitPrio=4294901723 usec=150
[2024-02-09T13:26:45.983] _slurm_rpc_submit_batch_job: JobId=186 InitPrio=4294901722 usec=148
[2024-02-09T13:26:46.469] _slurm_rpc_submit_batch_job: JobId=187 InitPrio=4294901721 usec=136
[2024-02-09T13:26:46.858] _slurm_rpc_submit_batch_job: JobId=188 InitPrio=4294901720 usec=164
[2024-02-09T13:26:47.178] _slurm_rpc_submit_batch_job: JobId=189 InitPrio=4294901719 usec=145
[2024-02-09T13:26:47.800] _slurm_rpc_submit_batch_job: JobId=190 InitPrio=4294901718 usec=144
[2024-02-09T13:26:54.147] _job_complete: JobId=180 WEXITSTATUS 0
[2024-02-09T13:26:54.147] _job_complete: JobId=180 done
[2024-02-09T13:26:54.240] sched: Allocate JobId=181 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:27:04.293] _job_complete: JobId=181 WEXITSTATUS 0
[2024-02-09T13:27:04.293] _job_complete: JobId=181 done
[2024-02-09T13:27:04.370] sched: Allocate JobId=182 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:27:14.423] _job_complete: JobId=182 WEXITSTATUS 0
[2024-02-09T13:27:14.424] _job_complete: JobId=182 done
[2024-02-09T13:27:14.508] sched: Allocate JobId=183 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:27:24.559] _job_complete: JobId=183 WEXITSTATUS 0
[2024-02-09T13:27:24.559] _job_complete: JobId=183 done
[2024-02-09T13:27:24.632] sched: Allocate JobId=184 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:27:34.687] _job_complete: JobId=184 WEXITSTATUS 0
[2024-02-09T13:27:34.687] _job_complete: JobId=184 done
[2024-02-09T13:27:34.762] sched: Allocate JobId=185 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:27:44.813] _job_complete: JobId=185 WEXITSTATUS 0
[2024-02-09T13:27:44.813] _job_complete: JobId=185 done
[2024-02-09T13:27:44.887] sched: Allocate JobId=186 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:27:54.940] _job_complete: JobId=186 WEXITSTATUS 0
[2024-02-09T13:27:54.941] _job_complete: JobId=186 done
[2024-02-09T13:27:55.012] sched: Allocate JobId=187 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:28:05.067] _job_complete: JobId=187 WEXITSTATUS 0
[2024-02-09T13:28:05.068] _job_complete: JobId=187 done
[2024-02-09T13:28:05.141] sched: Allocate JobId=188 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:28:15.196] _job_complete: JobId=188 WEXITSTATUS 0
[2024-02-09T13:28:15.197] _job_complete: JobId=188 done
[2024-02-09T13:28:15.288] sched: Allocate JobId=189 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:28:25.344] _job_complete: JobId=189 WEXITSTATUS 0
[2024-02-09T13:28:25.344] _job_complete: JobId=189 done
[2024-02-09T13:28:25.439] sched: Allocate JobId=190 NodeList=compute[00-02] #CPUs=3 Partition=normal
[2024-02-09T13:28:35.495] _job_complete: JobId=190 WEXITSTATUS 0
[2024-02-09T13:28:35.495] _job_complete: JobId=190 done
[2024-02-15T12:32:44.263] error: Nodes compute[00-03] not responding
[2024-02-15T12:32:44.263] error: Nodes compute[00-03] not responding, setting DOWN
[2024-02-16T11:38:32.598] killing old slurmctld[29332]
[2024-02-16T11:38:32.598] Terminate signal (SIGINT or SIGTERM) received
[2024-02-16T11:38:32.657] Saving all slurm state
[2024-02-16T11:38:32.687] error: chdir(/var/log): Permission denied
[2024-02-16T11:38:32.687] error: Configured MailProg is invalid
[2024-02-16T11:38:32.687] slurmctld version 22.05.11 started on cluster cluster
[2024-02-16T11:38:32.689] No memory enforcing mechanism configured.
[2024-02-16T11:38:32.689] Recovered state of 4 nodes
[2024-02-16T11:38:32.689] Down nodes: compute[00-03]
[2024-02-16T11:38:32.689] Recovered information about 0 jobs
[2024-02-16T11:38:32.689] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-02-16T11:38:32.689] Recovered state of 0 reservations
[2024-02-16T11:38:32.689] read_slurm_conf: backup_controller not specified
[2024-02-16T11:38:32.689] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-16T11:38:32.689] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-02-16T11:38:32.690] Running as primary controller
[2024-02-16T11:38:32.690] No parameter for mcs plugin, default values set
[2024-02-16T11:38:32.690] mcs: MCSParameters = (null). ondemand set.
[2024-02-16T11:38:37.098] Terminate signal (SIGINT or SIGTERM) received
[2024-02-16T11:38:37.198] Saving all slurm state
[2024-02-16T11:38:40.230] Dropped 1 hung communications to shutdown
[2024-02-16T11:38:40.230] Slurmctld shutdown completing with 1 active agent thread
[2024-02-16T11:38:41.697] error: chdir(/var/log): Permission denied
[2024-02-16T11:38:41.697] error: Configured MailProg is invalid
[2024-02-16T11:38:41.698] slurmctld version 22.05.11 started on cluster cluster
[2024-02-16T11:38:41.699] No memory enforcing mechanism configured.
[2024-02-16T11:38:41.721] Recovered state of 4 nodes
[2024-02-16T11:38:41.721] Down nodes: compute[00-03]
[2024-02-16T11:38:41.721] Recovered information about 0 jobs
[2024-02-16T11:38:41.721] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-02-16T11:38:41.721] Recovered state of 0 reservations
[2024-02-16T11:38:41.721] read_slurm_conf: backup_controller not specified
[2024-02-16T11:38:41.721] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-02-16T11:38:41.721] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-02-16T11:38:41.721] Running as primary controller
[2024-02-16T11:38:41.721] No parameter for mcs plugin, default values set
[2024-02-16T11:38:41.721] mcs: MCSParameters = (null). ondemand set.
[2024-02-16T11:38:52.272] validate_node_specs: Node compute02 unexpectedly rebooted boot_time=1708021597 last response=1708017964
[2024-02-16T11:38:52.275] validate_node_specs: Node compute03 unexpectedly rebooted boot_time=1708088083 last response=1708017964
[2024-02-16T11:38:52.276] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1708021626 last response=1708017964
[2024-02-16T11:38:52.278] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1708021596 last response=1708017964
[2024-02-16T11:39:41.841] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-02-16T11:39:52.169] update_node: node compute01 state set to IDLE
[2024-02-16T11:39:52.169] update_node: node compute02 state set to IDLE
[2024-02-16T11:39:52.169] update_node: node compute03 state set to IDLE
[2024-02-16T11:39:52.869] Node compute01 now responding
[2024-02-16T11:39:52.869] Node compute02 now responding
[2024-02-16T11:39:52.869] Node compute03 now responding
[2024-02-16T11:40:14.313] update_node: node compute00 state set to IDLE
[2024-02-16T11:40:14.912] Node compute00 now responding
[2024-02-20T06:49:56.161] _get_job_parts: invalid partition specified: ty
[2024-02-20T06:49:56.161] _slurm_rpc_allocate_resources: Invalid partition name specified 
[2024-02-20T06:51:17.954] sched: _slurm_rpc_allocate_resources JobId=191 NodeList=compute00 usec=22414
[2024-02-20T06:51:37.579] _job_complete: JobId=191 WEXITSTATUS 0
[2024-02-20T06:51:37.601] _job_complete: JobId=191 done
[2024-02-20T06:51:45.023] sched: _slurm_rpc_allocate_resources JobId=192 NodeList=compute00 usec=156
[2024-02-20T06:52:26.525] _job_complete: JobId=192 WEXITSTATUS 0
[2024-02-20T06:52:26.525] _job_complete: JobId=192 done
[2024-02-20T06:52:35.691] sched: _slurm_rpc_allocate_resources JobId=193 NodeList=compute00 usec=157
[2024-02-20T06:52:47.164] _job_complete: JobId=193 WEXITSTATUS 127
[2024-02-20T06:52:47.164] _job_complete: JobId=193 done
[2024-03-01T06:34:23.219] sched: _slurm_rpc_allocate_resources JobId=194 NodeList=compute00 usec=177
[2024-03-01T06:34:23.269] _job_complete: JobId=194 WEXITSTATUS 2
[2024-03-01T06:34:23.269] _job_complete: JobId=194 done
[2024-03-01T06:34:30.041] sched: _slurm_rpc_allocate_resources JobId=195 NodeList=compute00 usec=176
[2024-03-01T06:34:30.088] _job_complete: JobId=195 WEXITSTATUS 2
[2024-03-01T06:34:30.088] _job_complete: JobId=195 done
[2024-03-01T06:35:14.143] _pick_best_nodes: JobId=196 never runnable in partition normal
[2024-03-01T06:35:14.143] _slurm_rpc_submit_batch_job: Requested node configuration is not available
[2024-03-01T06:36:10.473] sched: _slurm_rpc_allocate_resources JobId=197 NodeList=compute00 usec=156
[2024-03-01T06:36:10.522] _job_complete: JobId=197 WEXITSTATUS 2
[2024-03-01T06:36:10.522] _job_complete: JobId=197 done
[2024-03-01T06:36:20.069] sched: _slurm_rpc_allocate_resources JobId=198 NodeList=compute00 usec=159
[2024-03-01T06:36:20.115] _job_complete: JobId=198 WEXITSTATUS 2
[2024-03-01T06:36:20.115] _job_complete: JobId=198 done
[2024-03-01T06:37:22.306] sched: _slurm_rpc_allocate_resources JobId=199 NodeList=compute00 usec=162
[2024-03-01T06:37:38.404] _job_complete: JobId=199 WEXITSTATUS 0
[2024-03-01T06:37:38.405] _job_complete: JobId=199 done
[2024-03-01T06:37:49.007] sched: _slurm_rpc_allocate_resources JobId=200 NodeList=compute00 usec=171
[2024-03-01T06:38:00.076] sched: _slurm_rpc_allocate_resources JobId=201 NodeList=compute01 usec=159
[2024-03-01T06:38:00.123] _job_complete: JobId=201 WEXITSTATUS 2
[2024-03-01T06:38:00.123] _job_complete: JobId=201 done
[2024-03-01T06:38:14.068] _slurm_rpc_submit_batch_job: JobId=202 InitPrio=4294901748 usec=145
[2024-03-01T06:38:14.090] sched: Allocate JobId=202 NodeList=compute01 #CPUs=1 Partition=express
[2024-03-01T06:38:14.117] _job_complete: JobId=202 WTERMSIG 53
[2024-03-01T06:38:14.117] _job_complete: JobId=202 done
[2024-03-01T06:38:24.045] _get_job_parts: invalid partition specified: week-long-cpu
[2024-03-01T06:38:24.045] _slurm_rpc_submit_batch_job: Invalid partition name specified
[2024-03-01T06:38:40.555] _slurm_rpc_submit_batch_job: JobId=203 InitPrio=4294901747 usec=133
[2024-03-01T06:39:32.678] _job_complete: JobId=200 WEXITSTATUS 130
[2024-03-01T06:39:32.678] _job_complete: JobId=200 done
[2024-03-01T06:39:50.046] sched: _slurm_rpc_allocate_resources JobId=204 NodeList=compute00 usec=150
[2024-03-01T06:39:55.752] _slurm_rpc_submit_batch_job: JobId=205 InitPrio=4294901745 usec=154
[2024-03-01T06:39:56.279] sched: Allocate JobId=205 NodeList=compute01 #CPUs=1 Partition=express
[2024-03-01T06:39:56.308] _job_complete: JobId=205 WTERMSIG 53
[2024-03-01T06:39:56.308] _job_complete: JobId=205 done
[2024-03-01T06:40:17.241] sched: _slurm_rpc_allocate_resources JobId=206 NodeList=compute01 usec=145
[2024-03-01T06:40:28.259] sched: _slurm_rpc_allocate_resources JobId=207 NodeList=compute02 usec=163
[2024-03-01T06:40:35.445] _job_complete: JobId=204 WEXITSTATUS 130
[2024-03-01T06:40:35.445] _job_complete: JobId=204 done
[2024-03-01T06:40:44.307] sched: _slurm_rpc_allocate_resources JobId=208 NodeList=compute00 usec=155
[2024-03-01T06:40:51.350] _job_complete: JobId=207 WEXITSTATUS 130
[2024-03-01T06:40:51.350] _job_complete: JobId=207 done
[2024-03-01T06:41:17.617] _job_complete: JobId=208 WEXITSTATUS 0
[2024-03-01T06:41:17.617] _job_complete: JobId=208 done
[2024-03-01T06:42:19.257] _job_complete: JobId=206 WEXITSTATUS 0
[2024-03-01T06:42:19.257] _job_complete: JobId=206 done
[2024-03-01T06:42:52.853] sched: _slurm_rpc_allocate_resources JobId=210 NodeList=compute00 usec=148
[2024-03-01T06:42:52.900] _job_complete: JobId=210 WEXITSTATUS 2
[2024-03-01T06:42:52.900] _job_complete: JobId=210 done
[2024-03-01T06:43:21.016] _pick_best_nodes: JobId=211 never runnable in partition normal
[2024-03-01T06:43:21.017] _slurm_rpc_submit_batch_job: Requested node configuration is not available
[2024-03-01T06:44:37.239] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=203 uid 1005
[2024-03-01T06:44:56.703] _slurm_rpc_submit_batch_job: JobId=212 InitPrio=4294901738 usec=120
[2024-03-01T06:44:56.987] _slurm_rpc_submit_batch_job: JobId=213 InitPrio=4294901737 usec=121
[2024-03-01T06:45:56.706] _slurm_rpc_submit_batch_job: JobId=214 InitPrio=4294901736 usec=132
[2024-03-01T06:46:13.142] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=212 uid 1005
[2024-03-01T06:46:15.485] _slurm_rpc_submit_batch_job: JobId=215 InitPrio=4294901735 usec=143
[2024-03-01T06:46:15.866] sched: Allocate JobId=215 NodeList=compute00 #CPUs=4 Partition=normal
[2024-03-01T06:46:15.896] _job_complete: JobId=215 WTERMSIG 53
[2024-03-01T06:46:15.896] _job_complete: JobId=215 done
[2024-03-01T06:46:17.390] _slurm_rpc_submit_batch_job: JobId=216 InitPrio=4294901734 usec=140
[2024-03-01T06:46:18.871] sched: Allocate JobId=216 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T06:46:18.900] _job_complete: JobId=216 WTERMSIG 53
[2024-03-01T06:46:18.901] _job_complete: JobId=216 done
[2024-03-01T06:46:30.906] _get_job_parts: invalid partition specified: nome_da_partição
[2024-03-01T06:46:30.906] _slurm_rpc_submit_batch_job: Invalid partition name specified
[2024-03-01T06:46:43.475] _get_job_parts: invalid partition specified: particao_1
[2024-03-01T06:46:43.475] _slurm_rpc_submit_batch_job: Invalid partition name specified
[2024-03-01T06:46:59.696] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=214 uid 1005
[2024-03-01T06:47:03.108] _slurm_rpc_submit_batch_job: JobId=217 InitPrio=4294901733 usec=134
[2024-03-01T06:47:16.789] _slurm_rpc_submit_batch_job: JobId=218 InitPrio=4294901732 usec=133
[2024-03-01T06:47:16.939] sched: Allocate JobId=218 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T06:47:16.968] _job_complete: JobId=218 WTERMSIG 53
[2024-03-01T06:47:16.968] _job_complete: JobId=218 done
[2024-03-01T06:49:00.757] _slurm_rpc_submit_batch_job: JobId=219 InitPrio=4294901731 usec=158
[2024-03-01T06:49:01.100] sched: Allocate JobId=219 NodeList=compute00 #CPUs=4 Partition=normal
[2024-03-01T06:49:01.129] _job_complete: JobId=219 WTERMSIG 53
[2024-03-01T06:49:01.129] _job_complete: JobId=219 done
[2024-03-01T06:49:45.729] sched: _slurm_rpc_allocate_resources JobId=220 NodeList=compute00 usec=166
[2024-03-01T06:50:11.385] _job_complete: JobId=220 WEXITSTATUS 130
[2024-03-01T06:50:11.385] _job_complete: JobId=220 done
[2024-03-01T06:50:21.368] _slurm_rpc_submit_batch_job: JobId=221 InitPrio=4294901729 usec=136
[2024-03-01T06:50:22.213] sched: Allocate JobId=221 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T06:50:22.242] _job_complete: JobId=221 WTERMSIG 53
[2024-03-01T06:50:22.242] _job_complete: JobId=221 done
[2024-03-01T06:50:56.315] sched: _slurm_rpc_allocate_resources JobId=222 NodeList=compute00 usec=168
[2024-03-01T06:51:28.276] _slurm_rpc_submit_batch_job: JobId=223 InitPrio=4294901727 usec=136
[2024-03-01T06:51:28.303] sched: Allocate JobId=223 NodeList=compute01 #CPUs=1 Partition=express
[2024-03-01T06:51:28.331] _job_complete: JobId=223 WTERMSIG 53
[2024-03-01T06:51:28.331] _job_complete: JobId=223 done
[2024-03-01T06:51:43.141] _job_complete: JobId=222 WEXITSTATUS 0
[2024-03-01T06:51:43.141] _job_complete: JobId=222 done
[2024-03-01T06:52:06.344] _slurm_rpc_submit_batch_job: JobId=224 InitPrio=4294901726 usec=132
[2024-03-01T06:53:39.967] _slurm_rpc_submit_batch_job: JobId=225 InitPrio=4294901725 usec=141
[2024-03-01T06:53:40.471] sched: Allocate JobId=225 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T06:53:40.501] _job_complete: JobId=225 WTERMSIG 53
[2024-03-01T06:53:40.501] _job_complete: JobId=225 done
[2024-03-01T06:54:44.700] _slurm_rpc_submit_batch_job: JobId=226 InitPrio=4294901724 usec=170
[2024-03-01T06:54:45.544] sched: Allocate JobId=226 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T06:54:45.573] _job_complete: JobId=226 WTERMSIG 53
[2024-03-01T06:54:45.573] _job_complete: JobId=226 done
[2024-03-01T06:55:28.132] sched: _slurm_rpc_allocate_resources JobId=227 NodeList=(null) usec=132
[2024-03-01T06:55:28.593] sched: Allocate JobId=227 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T06:55:57.917] _job_complete: JobId=227 WEXITSTATUS 130
[2024-03-01T06:55:57.917] _job_complete: JobId=227 done
[2024-03-01T06:56:02.433] _slurm_rpc_submit_batch_job: JobId=228 InitPrio=4294901722 usec=138
[2024-03-01T06:56:02.642] sched: Allocate JobId=228 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T06:56:28.991] _slurm_rpc_submit_batch_job: JobId=229 InitPrio=4294901721 usec=134
[2024-03-01T06:56:29.686] sched: Allocate JobId=229 NodeList=compute01 #CPUs=4 Partition=normal
[2024-03-01T06:56:30.006] _slurm_rpc_submit_batch_job: JobId=230 InitPrio=4294901720 usec=137
[2024-03-01T06:56:32.691] sched: Allocate JobId=230 NodeList=compute01 #CPUs=1 Partition=express
[2024-03-01T06:56:34.908] _job_complete: JobId=229 WEXITSTATUS 0
[2024-03-01T06:56:34.908] _job_complete: JobId=229 done
[2024-03-01T06:57:02.750] _job_complete: JobId=228 WEXITSTATUS 0
[2024-03-01T06:57:02.750] _job_complete: JobId=228 done
[2024-03-01T06:57:03.682] _slurm_rpc_submit_batch_job: JobId=231 InitPrio=4294901719 usec=128
[2024-03-01T06:57:21.113] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=213 uid 0
[2024-03-01T06:57:23.106] _slurm_rpc_submit_batch_job: JobId=232 InitPrio=4294901718 usec=137
[2024-03-01T06:57:23.739] sched: Allocate JobId=232 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T06:57:24.654] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=217 uid 0
[2024-03-01T06:57:31.172] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=224 uid 0
[2024-03-01T06:57:36.521] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=231 uid 0
[2024-03-01T06:57:41.752] Time limit exhausted for JobId=230
[2024-03-01T06:57:57.366] _slurm_rpc_submit_batch_job: JobId=233 InitPrio=4294901717 usec=143
[2024-03-01T06:58:15.245] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=233 uid 0
[2024-03-01T06:58:23.847] _job_complete: JobId=232 WEXITSTATUS 0
[2024-03-01T06:58:23.847] _job_complete: JobId=232 done
[2024-03-01T06:58:30.451] sched: _slurm_rpc_allocate_resources JobId=234 NodeList=compute00 usec=176
[2024-03-01T06:58:44.789] _slurm_rpc_submit_batch_job: JobId=235 InitPrio=4294901715 usec=145
[2024-03-01T06:58:44.821] sched: Allocate JobId=235 NodeList=compute02 #CPUs=1 Partition=normal
[2024-03-01T06:58:47.262] _slurm_rpc_submit_batch_job: JobId=236 InitPrio=4294901714 usec=145
[2024-03-01T06:58:47.825] sched: Allocate JobId=236 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-01T06:58:49.876] _job_complete: JobId=235 WEXITSTATUS 0
[2024-03-01T06:58:49.876] _job_complete: JobId=235 done
[2024-03-01T06:59:00.692] cleanup_completing: JobId=230 completion process took 79 seconds
[2024-03-01T06:59:23.217] _slurm_rpc_submit_batch_job: JobId=237 InitPrio=4294901713 usec=133
[2024-03-01T06:59:23.905] sched: Allocate JobId=237 NodeList=compute01 #CPUs=1 Partition=normal
[2024-03-01T06:59:29.957] _slurm_rpc_submit_batch_job: JobId=238 InitPrio=4294901712 usec=137
[2024-03-01T06:59:30.916] sched: Allocate JobId=238 NodeList=compute02 #CPUs=4 Partition=normal
[2024-03-01T06:59:36.116] _job_complete: JobId=238 WEXITSTATUS 0
[2024-03-01T06:59:36.116] _job_complete: JobId=238 done
[2024-03-01T06:59:36.240] _job_complete: JobId=234 WEXITSTATUS 0
[2024-03-01T06:59:36.240] _job_complete: JobId=234 done
[2024-03-01T06:59:47.925] _job_complete: JobId=236 WEXITSTATUS 0
[2024-03-01T06:59:47.925] _job_complete: JobId=236 done
[2024-03-01T06:59:48.020] _slurm_rpc_submit_batch_job: JobId=239 InitPrio=4294901711 usec=130
[2024-03-01T06:59:48.574] _slurm_rpc_submit_batch_job: JobId=240 InitPrio=4294901710 usec=146
[2024-03-01T06:59:48.948] sched: Allocate JobId=239 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T06:59:48.948] sched: Allocate JobId=240 NodeList=compute02 #CPUs=1 Partition=normal
[2024-03-01T06:59:54.032] _job_complete: JobId=239 WEXITSTATUS 0
[2024-03-01T06:59:54.032] _job_complete: JobId=239 done
[2024-03-01T07:00:09.098] _slurm_rpc_submit_batch_job: JobId=241 InitPrio=4294901709 usec=136
[2024-03-01T07:00:09.982] sched: Allocate JobId=241 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:00:15.063] _job_complete: JobId=241 WEXITSTATUS 0
[2024-03-01T07:00:15.063] _job_complete: JobId=241 done
[2024-03-01T07:00:24.007] _job_complete: JobId=237 WEXITSTATUS 0
[2024-03-01T07:00:24.007] _job_complete: JobId=237 done
[2024-03-01T07:00:28.711] _slurm_rpc_submit_batch_job: JobId=242 InitPrio=4294901708 usec=141
[2024-03-01T07:00:29.010] sched: Allocate JobId=242 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-01T07:00:34.068] _job_complete: JobId=242 WEXITSTATUS 0
[2024-03-01T07:00:34.069] _job_complete: JobId=242 done
[2024-03-01T07:00:34.666] _slurm_rpc_submit_batch_job: JobId=243 InitPrio=4294901707 usec=135
[2024-03-01T07:00:35.019] sched: Allocate JobId=243 NodeList=compute03 #CPUs=1 Partition=express
[2024-03-01T07:00:35.556] _slurm_rpc_submit_batch_job: JobId=244 InitPrio=4294901706 usec=132
[2024-03-01T07:00:38.024] sched: Allocate JobId=244 NodeList=compute03 #CPUs=4 Partition=normal
[2024-03-01T07:00:43.243] _job_complete: JobId=244 WEXITSTATUS 0
[2024-03-01T07:00:43.243] _job_complete: JobId=244 done
[2024-03-01T07:00:46.775] _slurm_rpc_submit_batch_job: JobId=245 InitPrio=4294901705 usec=153
[2024-03-01T07:00:47.040] sched: Allocate JobId=245 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-01T07:00:48.198] _slurm_rpc_submit_batch_job: JobId=246 InitPrio=4294901704 usec=144
[2024-03-01T07:00:49.028] _job_complete: JobId=240 WEXITSTATUS 0
[2024-03-01T07:00:49.028] _job_complete: JobId=240 done
[2024-03-01T07:00:49.110] sched: Allocate JobId=246 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:00:50.419] _slurm_rpc_submit_batch_job: JobId=247 InitPrio=4294901703 usec=140
[2024-03-01T07:00:53.050] sched: Allocate JobId=247 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-01T07:00:58.126] _job_complete: JobId=247 WEXITSTATUS 0
[2024-03-01T07:00:58.126] _job_complete: JobId=247 done
[2024-03-01T07:01:25.458] _slurm_rpc_submit_batch_job: JobId=248 InitPrio=4294901702 usec=145
[2024-03-01T07:01:28.108] sched: Allocate JobId=248 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-01T07:01:32.711] _slurm_rpc_submit_batch_job: JobId=249 InitPrio=4294901701 usec=146
[2024-03-01T07:01:33.116] sched: Allocate JobId=249 NodeList=compute01 #CPUs=1 Partition=normal
[2024-03-01T07:01:41.130] Time limit exhausted for JobId=243
[2024-03-01T07:02:11.180] Time limit exhausted for JobId=245
[2024-03-01T07:02:11.180] Time limit exhausted for JobId=246
[2024-03-01T07:02:31.556] Terminate signal (SIGINT or SIGTERM) received
[2024-03-01T07:02:31.611] Saving all slurm state
[2024-03-01T07:02:31.628] error: chdir(/var/log): Permission denied
[2024-03-01T07:02:31.628] error: Configured MailProg is invalid
[2024-03-01T07:02:31.628] slurmctld version 22.05.11 started on cluster cluster
[2024-03-01T07:02:31.631] No memory enforcing mechanism configured.
[2024-03-01T07:02:31.633] Recovered state of 4 nodes
[2024-03-01T07:02:31.633] Recovered JobId=224 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=230 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=231 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=232 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=233 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=234 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=235 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=236 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=237 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=238 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=239 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=240 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=241 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=242 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=243 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=244 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=245 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=246 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=247 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=248 StepId=batch
[2024-03-01T07:02:31.633] Recovered JobId=248 Assoc=0
[2024-03-01T07:02:31.633] Recovered JobId=249 StepId=batch
[2024-03-01T07:02:31.633] Recovered JobId=249 Assoc=0
[2024-03-01T07:02:31.633] Recovered information about 21 jobs
[2024-03-01T07:02:31.633] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-03-01T07:02:31.633] Recovered state of 0 reservations
[2024-03-01T07:02:31.633] read_slurm_conf: backup_controller not specified
[2024-03-01T07:02:31.633] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-03-01T07:02:31.633] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-03-01T07:02:31.633] Running as primary controller
[2024-03-01T07:02:31.634] No parameter for mcs plugin, default values set
[2024-03-01T07:02:31.634] mcs: MCSParameters = (null). ondemand set.
[2024-03-01T07:02:33.214] _job_complete: JobId=249 WEXITSTATUS 0
[2024-03-01T07:02:33.215] _job_complete: JobId=249 done
[2024-03-01T07:02:33.300] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-03-01T07:02:35.646] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T07:02:35.646] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T07:02:35.646] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T07:02:35.646] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T07:02:52.090] _slurm_rpc_submit_batch_job: JobId=250 InitPrio=4294901700 usec=174
[2024-03-01T07:02:52.659] sched: Allocate JobId=250 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:03:02.344] _slurm_rpc_submit_batch_job: JobId=251 InitPrio=4294901699 usec=151
[2024-03-01T07:03:02.669] sched: Allocate JobId=251 NodeList=compute03 #CPUs=1 Partition=express
[2024-03-01T07:03:10.927] _slurm_rpc_submit_batch_job: JobId=252 InitPrio=4294901698 usec=140
[2024-03-01T07:03:11.681] sched: Allocate JobId=252 NodeList=compute03 #CPUs=4 Partition=normal
[2024-03-01T07:03:16.899] _job_complete: JobId=252 WEXITSTATUS 0
[2024-03-01T07:03:16.899] _job_complete: JobId=252 done
[2024-03-01T07:03:31.704] Time limit exhausted for JobId=248
[2024-03-01T07:03:45.283] sched: _slurm_rpc_allocate_resources JobId=253 NodeList=compute01 usec=172
[2024-03-01T07:03:52.767] _job_complete: JobId=250 WEXITSTATUS 0
[2024-03-01T07:03:52.767] _job_complete: JobId=250 done
[2024-03-01T07:04:31.772] Time limit exhausted for JobId=251
[2024-03-01T07:04:41.155] _job_complete: JobId=253 WEXITSTATUS 127
[2024-03-01T07:04:41.155] _job_complete: JobId=253 done
[2024-03-01T07:04:45.905] _slurm_rpc_submit_batch_job: JobId=254 InitPrio=4294901696 usec=138
[2024-03-01T07:04:46.787] sched: Allocate JobId=254 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:05:06.938] _slurm_rpc_submit_batch_job: JobId=255 InitPrio=4294901695 usec=145
[2024-03-01T07:05:07.817] sched: Allocate JobId=255 NodeList=compute01 #CPUs=1 Partition=normal
[2024-03-01T07:05:20.379] sched: _slurm_rpc_allocate_resources JobId=256 NodeList=compute02 usec=168
[2024-03-01T07:05:29.203] _job_complete: JobId=256 WEXITSTATUS 0
[2024-03-01T07:05:29.204] _job_complete: JobId=256 done
[2024-03-01T07:05:32.191] sched: _slurm_rpc_allocate_resources JobId=257 NodeList=compute02 usec=167
[2024-03-01T07:05:36.566] _job_complete: JobId=257 WEXITSTATUS 0
[2024-03-01T07:05:36.566] _job_complete: JobId=257 done
[2024-03-01T07:05:37.200] sched: _slurm_rpc_allocate_resources JobId=258 NodeList=compute02 usec=151
[2024-03-01T07:05:38.127] _job_complete: JobId=258 WEXITSTATUS 0
[2024-03-01T07:05:38.127] _job_complete: JobId=258 done
[2024-03-01T07:06:01.884] Time limit exhausted for JobId=254
[2024-03-01T07:06:12.444] _slurm_rpc_submit_batch_job: JobId=259 InitPrio=4294901691 usec=144
[2024-03-01T07:06:12.895] sched: Allocate JobId=259 NodeList=compute01 #CPUs=1 Partition=express
[2024-03-01T07:06:31.925] Time limit exhausted for JobId=255
[2024-03-01T07:06:52.710] sched: _slurm_rpc_allocate_resources JobId=260 NodeList=compute00 usec=162
[2024-03-01T07:06:59.715] _job_complete: JobId=260 WEXITSTATUS 0
[2024-03-01T07:06:59.716] _job_complete: JobId=260 done
[2024-03-01T07:07:00.344] _slurm_rpc_submit_batch_job: JobId=261 InitPrio=4294901689 usec=131
[2024-03-01T07:07:00.977] sched: Allocate JobId=261 NodeList=compute01 #CPUs=4 Partition=normal
[2024-03-01T07:07:06.077] _job_complete: JobId=261 WEXITSTATUS 0
[2024-03-01T07:07:06.078] _job_complete: JobId=261 done
[2024-03-01T07:07:12.964] sched: _slurm_rpc_allocate_resources JobId=262 NodeList=compute00 usec=154
[2024-03-01T07:07:13.021] _job_complete: JobId=262 WEXITSTATUS 13
[2024-03-01T07:07:13.021] _job_complete: JobId=262 done
[2024-03-01T07:07:17.652] sched: _slurm_rpc_allocate_resources JobId=263 NodeList=compute00 usec=148
[2024-03-01T07:07:39.784] _get_job_parts: invalid partition specified: ty
[2024-03-01T07:07:39.784] _slurm_rpc_allocate_resources: Invalid partition name specified 
[2024-03-01T07:07:46.033] _slurm_rpc_submit_batch_job: JobId=264 InitPrio=4294901686 usec=138
[2024-03-01T07:07:46.071] sched: Allocate JobId=264 NodeList=compute02 #CPUs=1 Partition=normal
[2024-03-01T07:07:59.232] _slurm_rpc_submit_batch_job: JobId=265 InitPrio=4294901685 usec=137
[2024-03-01T07:08:00.091] sched: Allocate JobId=265 NodeList=compute01 #CPUs=1 Partition=express
[2024-03-01T07:08:22.365] _job_complete: JobId=263 WEXITSTATUS 0
[2024-03-01T07:08:22.365] _job_complete: JobId=263 done
[2024-03-01T07:08:31.129] Time limit exhausted for JobId=259
[2024-03-01T07:08:46.131] _job_complete: JobId=264 WEXITSTATUS 0
[2024-03-01T07:08:46.131] _job_complete: JobId=264 done
[2024-03-01T07:09:01.165] Time limit exhausted for JobId=265
[2024-03-01T07:09:38.034] _slurm_rpc_submit_batch_job: JobId=266 InitPrio=4294901684 usec=143
[2024-03-01T07:09:38.210] sched: Allocate JobId=266 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:10:01.500] _slurm_rpc_submit_batch_job: JobId=267 InitPrio=4294901683 usec=133
[2024-03-01T07:10:02.241] sched: Allocate JobId=267 NodeList=compute01 #CPUs=1 Partition=normal
[2024-03-01T07:10:07.908] _slurm_rpc_submit_batch_job: JobId=268 InitPrio=4294901682 usec=150
[2024-03-01T07:10:08.248] sched: Allocate JobId=268 NodeList=compute02 #CPUs=1 Partition=normal
[2024-03-01T07:10:29.882] sched: _slurm_rpc_allocate_resources JobId=269 NodeList=compute03 usec=154
[2024-03-01T07:10:29.930] _job_complete: JobId=269 WEXITSTATUS 13
[2024-03-01T07:10:29.930] _job_complete: JobId=269 done
[2024-03-01T07:10:31.787] _slurm_rpc_submit_batch_job: JobId=270 InitPrio=4294901680 usec=142
[2024-03-01T07:10:32.636] sched/backfill: _start_job: Started JobId=270 in express on compute02
[2024-03-01T07:10:33.470] _slurm_rpc_submit_batch_job: JobId=271 InitPrio=4294901679 usec=154
[2024-03-01T07:10:34.281] sched: Allocate JobId=271 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-01T07:10:38.343] _job_complete: JobId=266 WEXITSTATUS 0
[2024-03-01T07:10:38.343] _job_complete: JobId=266 done
[2024-03-01T07:11:02.081] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=270 uid 1035
[2024-03-01T07:11:02.343] _job_complete: JobId=267 WEXITSTATUS 0
[2024-03-01T07:11:02.343] _job_complete: JobId=267 done
[2024-03-01T07:11:54.378] _job_complete: JobId=271 WEXITSTATUS 0
[2024-03-01T07:11:54.378] _job_complete: JobId=271 done
[2024-03-01T07:11:55.751] _slurm_rpc_submit_batch_job: JobId=272 InitPrio=4294901678 usec=155
[2024-03-01T07:11:56.383] sched: Allocate JobId=272 NodeList=compute02 #CPUs=1 Partition=normal
[2024-03-01T07:12:30.873] sched: _slurm_rpc_allocate_resources JobId=273 NodeList=compute03 usec=157
[2024-03-01T07:12:31.445] Time limit exhausted for JobId=268
[2024-03-01T07:12:40.944] sched: _slurm_rpc_allocate_resources JobId=274 NodeList=compute02 usec=152
[2024-03-01T07:12:51.129] _slurm_rpc_submit_batch_job: JobId=275 InitPrio=4294901675 usec=151
[2024-03-01T07:12:51.505] sched: Allocate JobId=275 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:13:02.821] sched: _slurm_rpc_allocate_resources JobId=276 NodeList=compute01 usec=151
[2024-03-01T07:13:37.662] _slurm_rpc_submit_batch_job: JobId=277 InitPrio=4294901673 usec=151
[2024-03-01T07:13:46.966] _slurm_rpc_submit_batch_job: JobId=278 InitPrio=4294901672 usec=147
[2024-03-01T07:13:51.703] _slurm_rpc_submit_batch_job: JobId=279 InitPrio=4294901671 usec=144
[2024-03-01T07:14:01.601] Time limit exhausted for JobId=272
[2024-03-01T07:14:01.602] Time limit exhausted for JobId=274
[2024-03-01T07:14:01.608] _slurm_rpc_complete_job_allocation: JobId=274 error Job/step already completing or completed
[2024-03-01T07:14:03.344] sched: Allocate JobId=277 NodeList=compute02 #CPUs=1 Partition=normal
[2024-03-01T07:14:11.615] _job_complete: JobId=275 WEXITSTATUS 0
[2024-03-01T07:14:11.615] _job_complete: JobId=275 done
[2024-03-01T07:14:11.706] sched: Allocate JobId=278 NodeList=compute00 #CPUs=4 Partition=normal
[2024-03-01T07:14:16.790] _job_complete: JobId=278 WEXITSTATUS 0
[2024-03-01T07:14:16.790] _job_complete: JobId=278 done
[2024-03-01T07:14:16.864] sched: Allocate JobId=279 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:14:18.835] _job_complete: JobId=276 WEXITSTATUS 1
[2024-03-01T07:14:18.835] _job_complete: JobId=276 done
[2024-03-01T07:14:24.324] _job_complete: JobId=273 WTERMSIG 126
[2024-03-01T07:14:24.324] _job_complete: JobId=273 cancelled by interactive user
[2024-03-01T07:14:24.324] _job_complete: JobId=273 done
[2024-03-01T07:14:44.781] sched: _slurm_rpc_allocate_resources JobId=280 NodeList=compute01 usec=164
[2024-03-01T07:15:05.470] _job_complete: JobId=280 WTERMSIG 126
[2024-03-01T07:15:05.470] _job_complete: JobId=280 cancelled by interactive user
[2024-03-01T07:15:05.471] _job_complete: JobId=280 done
[2024-03-01T07:15:31.724] Time limit exhausted for JobId=279
[2024-03-01T07:15:38.829] error: _find_node_record: lookup failure for node "node01"
[2024-03-01T07:15:38.829] error: node_name2bitmap: invalid node specified: "node01"
[2024-03-01T07:15:38.829] _slurm_rpc_allocate_resources: Invalid node name specified 
[2024-03-01T07:15:41.486] _slurm_rpc_submit_batch_job: JobId=281 InitPrio=4294901669 usec=144
[2024-03-01T07:15:41.738] sched: Allocate JobId=281 NodeList=compute03 #CPUs=4 Partition=normal
[2024-03-01T07:16:03.440] _job_complete: JobId=277 WEXITSTATUS 0
[2024-03-01T07:16:03.440] _job_complete: JobId=277 done
[2024-03-01T07:16:45.250] _slurm_rpc_submit_batch_job: JobId=282 InitPrio=4294901668 usec=150
[2024-03-01T07:16:45.831] sched: Allocate JobId=282 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-01T07:17:01.846] Time limit exhausted for JobId=281
[2024-03-01T07:17:03.651] sched: _slurm_rpc_allocate_resources JobId=283 NodeList=(null) usec=116
[2024-03-01T07:17:26.495] _job_complete: JobId=283 WTERMSIG 126
[2024-03-01T07:17:26.495] _job_complete: JobId=283 cancelled by interactive user
[2024-03-01T07:17:26.495] _job_complete: JobId=283 done
[2024-03-01T07:17:26.495] _slurm_rpc_complete_job_allocation: JobId=283 error Job/step already completing or completed
[2024-03-01T07:17:35.987] sched: _slurm_rpc_allocate_resources JobId=284 NodeList=(null) usec=123
[2024-03-01T07:17:39.407] _slurm_rpc_submit_batch_job: JobId=285 InitPrio=4294901665 usec=166
[2024-03-01T07:17:39.924] sched: Allocate JobId=285 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:17:56.953] _slurm_rpc_submit_batch_job: JobId=286 InitPrio=4294901664 usec=144
[2024-03-01T07:17:57.948] sched: Allocate JobId=286 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-01T07:18:01.953] Time limit exhausted for JobId=282
[2024-03-01T07:18:04.690] _job_complete: JobId=286 WTERMSIG 9
[2024-03-01T07:18:04.690] _job_complete: JobId=286 done
[2024-03-01T07:18:13.241] _slurm_rpc_submit_batch_job: JobId=287 InitPrio=4294901663 usec=138
[2024-03-01T07:18:13.971] sched: Allocate JobId=287 NodeList=compute01 #CPUs=1 Partition=normal
[2024-03-01T07:18:59.355] _slurm_rpc_submit_batch_job: JobId=288 InitPrio=4294901662 usec=136
[2024-03-01T07:19:00.029] sched: Allocate JobId=288 NodeList=compute02 #CPUs=1 Partition=normal
[2024-03-01T07:19:01.031] Time limit exhausted for JobId=285
[2024-03-01T07:19:31.076] Time limit exhausted for JobId=287
[2024-03-01T07:20:01.117] Time limit exhausted for JobId=288
[2024-03-01T07:20:14.760] _slurm_rpc_submit_batch_job: JobId=289 InitPrio=4294901661 usec=149
[2024-03-01T07:20:15.134] sched: Allocate JobId=289 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:20:15.717] sched: _slurm_rpc_allocate_resources JobId=290 NodeList=(null) usec=127
[2024-03-01T07:21:20.383] _job_complete: JobId=284 WTERMSIG 126
[2024-03-01T07:21:20.383] _job_complete: JobId=284 cancelled by interactive user
[2024-03-01T07:21:20.383] _job_complete: JobId=284 done
[2024-03-01T07:21:20.384] _slurm_rpc_complete_job_allocation: JobId=284 error Job/step already completing or completed
[2024-03-01T07:21:22.208] _slurm_rpc_submit_batch_job: JobId=291 InitPrio=4294901659 usec=138
[2024-03-01T07:21:22.232] sched: Allocate JobId=291 NodeList=compute01 #CPUs=1 Partition=normal
[2024-03-01T07:21:31.245] Time limit exhausted for JobId=289
[2024-03-01T07:21:47.989] sched: _slurm_rpc_allocate_resources JobId=292 NodeList=(null) usec=110
[2024-03-01T07:22:19.039] _job_complete: JobId=292 WTERMSIG 126
[2024-03-01T07:22:19.039] _job_complete: JobId=292 cancelled by interactive user
[2024-03-01T07:22:19.039] _job_complete: JobId=292 done
[2024-03-01T07:22:23.877] sched: _slurm_rpc_allocate_resources JobId=293 NodeList=(null) usec=147
[2024-03-01T07:22:24.308] sched: Allocate JobId=293 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T07:22:30.300] _job_complete: JobId=290 WTERMSIG 126
[2024-03-01T07:22:30.301] _job_complete: JobId=290 cancelled by interactive user
[2024-03-01T07:22:30.301] _job_complete: JobId=290 done
[2024-03-01T07:22:30.301] _slurm_rpc_complete_job_allocation: JobId=290 error Job/step already completing or completed
[2024-03-01T07:22:38.944] _job_complete: JobId=293 WTERMSIG 2
[2024-03-01T07:22:38.944] _job_complete: JobId=293 done
[2024-03-01T07:23:00.086] sched: _slurm_rpc_allocate_resources JobId=294 NodeList=compute00 usec=150
[2024-03-01T07:23:31.402] Time limit exhausted for JobId=291
[2024-03-01T07:23:52.270] sched: _slurm_rpc_allocate_resources JobId=295 NodeList=compute01 usec=152
[2024-03-01T07:25:01.496] Time limit exhausted for JobId=295
[2024-03-01T07:25:01.504] _slurm_rpc_complete_job_allocation: JobId=295 error Job/step already completing or completed
[2024-03-01T07:25:58.242] _job_complete: JobId=294 WTERMSIG 2
[2024-03-01T07:25:58.242] _job_complete: JobId=294 done
[2024-03-01T07:26:55.534] sched: _slurm_rpc_allocate_resources JobId=296 NodeList=compute00 usec=149
[2024-03-01T07:27:09.336] sched: _slurm_rpc_allocate_resources JobId=297 NodeList=compute01 usec=170
[2024-03-01T07:27:10.715] _job_complete: JobId=297 WTERMSIG 2
[2024-03-01T07:27:10.715] _job_complete: JobId=297 done
[2024-03-01T07:27:43.233] _job_complete: JobId=296 WTERMSIG 2
[2024-03-01T07:27:43.233] _job_complete: JobId=296 done
[2024-03-01T07:32:46.920] sched: _slurm_rpc_allocate_resources JobId=298 NodeList=compute00 usec=159
[2024-03-01T07:32:46.971] _job_complete: JobId=298 WEXITSTATUS 0
[2024-03-01T07:32:46.972] _job_complete: JobId=298 done
[2024-03-01T07:34:06.474] sched: _slurm_rpc_allocate_resources JobId=299 NodeList=compute00 usec=164
[2024-03-01T07:34:13.024] sched: _slurm_rpc_allocate_resources JobId=300 NodeList=compute01 usec=161
[2024-03-01T07:34:13.077] _job_complete: JobId=300 WEXITSTATUS 0
[2024-03-01T07:34:13.077] _job_complete: JobId=300 done
[2024-03-01T07:34:15.371] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T07:34:15.371] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T07:34:15.372] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T07:34:15.373] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T07:34:15.986] sched: _slurm_rpc_allocate_resources JobId=301 NodeList=compute01 usec=161
[2024-03-01T07:34:16.038] _job_complete: JobId=301 WEXITSTATUS 0
[2024-03-01T07:34:16.038] _job_complete: JobId=301 done
[2024-03-01T07:54:31.492] Time limit exhausted for JobId=299
[2024-03-01T07:54:31.527] _slurm_rpc_complete_job_allocation: JobId=299 error Job/step already completing or completed
[2024-03-01T08:07:36.004] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T08:07:36.004] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T08:07:36.005] error: Node compute03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T08:07:36.006] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-03-01T08:27:04.081] Terminate signal (SIGINT or SIGTERM) received
[2024-03-01T08:27:04.168] Saving all slurm state
[2024-03-01T08:27:04.186] error: chdir(/var/log): Permission denied
[2024-03-01T08:27:04.186] error: Configured MailProg is invalid
[2024-03-01T08:27:04.187] slurmctld version 22.05.11 started on cluster cluster
[2024-03-01T08:27:04.189] No memory enforcing mechanism configured.
[2024-03-01T08:27:04.190] Recovered state of 4 nodes
[2024-03-01T08:27:04.190] Recovered information about 0 jobs
[2024-03-01T08:27:04.190] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-03-01T08:27:04.190] Recovered state of 0 reservations
[2024-03-01T08:27:04.190] read_slurm_conf: backup_controller not specified
[2024-03-01T08:27:04.190] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-03-01T08:27:04.190] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-03-01T08:27:04.190] Running as primary controller
[2024-03-01T08:27:04.190] No parameter for mcs plugin, default values set
[2024-03-01T08:27:04.190] mcs: MCSParameters = (null). ondemand set.
[2024-03-01T08:28:04.265] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-03-01T08:32:04.608] error: Nodes compute[00-03] not responding
[2024-03-01T08:32:08.644] error: Nodes compute[00-03] not responding, setting DOWN
[2024-03-01T09:07:09.290] Node compute02 now responding
[2024-03-01T09:07:09.290] validate_node_specs: Node compute02 unexpectedly rebooted boot_time=1709299620 last response=1709299455
[2024-03-01T09:07:09.293] Node compute03 now responding
[2024-03-01T09:07:09.293] validate_node_specs: Node compute03 unexpectedly rebooted boot_time=1709300661 last response=1709299455
[2024-03-01T09:07:09.295] Node compute01 now responding
[2024-03-01T09:07:09.295] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1709299628 last response=1709299455
[2024-03-01T09:07:09.296] Node compute00 now responding
[2024-03-01T09:07:09.296] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1709299606 last response=1709299455
[2024-03-01T09:09:01.597] update_node: node compute00 state set to IDLE
[2024-03-01T09:09:01.597] update_node: node compute01 state set to IDLE
[2024-03-01T09:09:01.597] update_node: node compute02 state set to IDLE
[2024-03-01T09:09:01.597] update_node: node compute03 state set to IDLE
[2024-03-01T09:09:01.745] Node compute03 now responding
[2024-03-01T09:09:01.745] Node compute02 now responding
[2024-03-01T09:09:01.745] Node compute01 now responding
[2024-03-01T09:09:01.745] Node compute00 now responding
[2024-03-01T09:11:24.479] sched: _slurm_rpc_allocate_resources JobId=302 NodeList=compute00 usec=191
[2024-03-01T09:11:47.692] _job_complete: JobId=302 WEXITSTATUS 0
[2024-03-01T09:11:47.692] _job_complete: JobId=302 done
[2024-03-01T09:36:32.523] sched: _slurm_rpc_allocate_resources JobId=303 NodeList=compute00 usec=160
[2024-03-01T09:36:41.524] _job_complete: JobId=303 WEXITSTATUS 0
[2024-03-01T09:36:41.524] _job_complete: JobId=303 done
[2024-03-01T09:36:45.776] sched: _slurm_rpc_allocate_resources JobId=304 NodeList=compute00 usec=172
[2024-03-01T09:37:57.426] _job_complete: JobId=304 WEXITSTATUS 0
[2024-03-01T09:37:57.426] _job_complete: JobId=304 done
[2024-03-01T11:13:19.943] sched: _slurm_rpc_allocate_resources JobId=305 NodeList=compute00 usec=155
[2024-03-01T11:13:34.539] _job_complete: JobId=305 WEXITSTATUS 1
[2024-03-01T11:13:34.539] _job_complete: JobId=305 done
[2024-03-01T12:09:36.128] _slurm_rpc_submit_batch_job: JobId=306 InitPrio=4294901755 usec=113
[2024-03-01T12:11:11.461] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=306 uid 1008
[2024-03-01T12:11:40.578] _slurm_rpc_submit_batch_job: JobId=307 InitPrio=4294901754 usec=103
[2024-03-01T12:14:38.896] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=307 uid 1008
[2024-03-01T12:14:48.432] _slurm_rpc_submit_batch_job: JobId=308 InitPrio=4294901753 usec=125
[2024-03-01T12:14:48.608] sched: Allocate JobId=308 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:15:01.682] _job_complete: JobId=308 WEXITSTATUS 0
[2024-03-01T12:15:01.683] _job_complete: JobId=308 done
[2024-03-01T12:28:04.715] _slurm_rpc_submit_batch_job: JobId=309 InitPrio=4294901752 usec=108
[2024-03-01T12:28:38.728] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=309 uid 1008
[2024-03-01T12:28:52.592] _slurm_rpc_submit_batch_job: JobId=310 InitPrio=4294901751 usec=115
[2024-03-01T12:30:31.344] sched: _slurm_rpc_allocate_resources JobId=311 NodeList=(null) usec=135
[2024-03-01T12:30:31.421] sched: Allocate JobId=311 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:30:33.135] _job_complete: JobId=311 WTERMSIG 126
[2024-03-01T12:30:33.135] _job_complete: JobId=311 cancelled by interactive user
[2024-03-01T12:30:33.135] _job_complete: JobId=311 done
[2024-03-01T12:30:45.294] _get_job_parts: invalid partition specified: ty
[2024-03-01T12:30:45.294] _slurm_rpc_allocate_resources: Invalid partition name specified 
[2024-03-01T12:30:45.838] _get_job_parts: invalid partition specified: ty
[2024-03-01T12:30:45.838] _slurm_rpc_allocate_resources: Invalid partition name specified 
[2024-03-01T12:30:53.581] sched: _slurm_rpc_allocate_resources JobId=312 NodeList=(null) usec=125
[2024-03-01T12:30:57.386] sched: _slurm_rpc_allocate_resources JobId=313 NodeList=(null) usec=129
[2024-03-01T12:30:57.468] sched: Allocate JobId=313 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:32:21.096] _slurm_rpc_submit_batch_job: JobId=314 InitPrio=4294901747 usec=135
[2024-03-01T12:32:21.320] sched/backfill: _start_job: Started JobId=314 in express on compute01
[2024-03-01T12:32:21.374] _job_complete: JobId=314 WEXITSTATUS 0
[2024-03-01T12:32:21.374] _job_complete: JobId=314 done
[2024-03-01T12:34:20.411] _job_complete: JobId=313 WEXITSTATUS 130
[2024-03-01T12:34:20.411] _job_complete: JobId=313 done
[2024-03-01T12:34:38.892] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=310 uid 1008
[2024-03-01T12:34:46.418] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=312 uid 1018
[2024-03-01T12:35:54.880] _slurm_rpc_submit_batch_job: JobId=315 InitPrio=4294901746 usec=117
[2024-03-01T12:35:55.389] sched: _slurm_rpc_allocate_resources JobId=316 NodeList=(null) usec=134
[2024-03-01T12:35:58.006] sched: Allocate JobId=316 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:35:58.056] _job_complete: JobId=316 WEXITSTATUS 13
[2024-03-01T12:35:58.056] _job_complete: JobId=316 done
[2024-03-01T12:35:59.452] _slurm_rpc_submit_batch_job: JobId=317 InitPrio=4294901744 usec=153
[2024-03-01T12:36:01.011] sched: Allocate JobId=317 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T12:36:01.085] _job_complete: JobId=317 WEXITSTATUS 2
[2024-03-01T12:36:01.085] _job_complete: JobId=317 done
[2024-03-01T12:36:27.104] _slurm_rpc_submit_batch_job: JobId=318 InitPrio=4294901743 usec=151
[2024-03-01T12:36:28.054] sched: Allocate JobId=318 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T12:36:28.131] _job_complete: JobId=318 WEXITSTATUS 0
[2024-03-01T12:36:28.131] _job_complete: JobId=318 done
[2024-03-01T12:36:40.309] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=315 uid 1008
[2024-03-01T12:37:13.775] _slurm_rpc_submit_batch_job: JobId=319 InitPrio=4294901742 usec=103
[2024-03-01T12:37:14.902] _slurm_rpc_submit_batch_job: JobId=320 InitPrio=4294901741 usec=121
[2024-03-01T12:37:34.637] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=319 uid 1008
[2024-03-01T12:37:37.451] _slurm_rpc_submit_batch_job: JobId=321 InitPrio=4294901740 usec=103
[2024-03-01T12:37:48.177] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=321 uid 1008
[2024-03-01T12:37:50.611] _slurm_rpc_submit_batch_job: JobId=322 InitPrio=4294901739 usec=146
[2024-03-01T12:37:51.198] sched: Allocate JobId=322 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T12:37:51.268] _job_complete: JobId=322 WEXITSTATUS 2
[2024-03-01T12:37:51.268] _job_complete: JobId=322 done
[2024-03-01T12:37:57.701] _slurm_rpc_submit_batch_job: JobId=323 InitPrio=4294901738 usec=113
[2024-03-01T12:38:12.545] sched: _slurm_rpc_allocate_resources JobId=324 NodeList=(null) usec=138
[2024-03-01T12:38:13.238] sched: Allocate JobId=324 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:38:13.287] _job_complete: JobId=324 WEXITSTATUS 13
[2024-03-01T12:38:13.287] _job_complete: JobId=324 done
[2024-03-01T12:38:41.984] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=320 uid 1008
[2024-03-01T12:38:41.984] error: Security violation, REQUEST_KILL_JOB RPC for JobId=320 from uid 1008
[2024-03-01T12:38:41.984] _slurm_rpc_kill_job: job_str_signal() uid=1008 JobId=320 sig=9 returned: Access/permission denied
[2024-03-01T12:38:44.299] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=323 uid 1008
[2024-03-01T12:39:07.708] _slurm_rpc_submit_batch_job: JobId=325 InitPrio=4294901736 usec=133
[2024-03-01T12:39:08.344] sched: Allocate JobId=325 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:39:21.422] _job_complete: JobId=325 WEXITSTATUS 0
[2024-03-01T12:39:21.422] _job_complete: JobId=325 done
[2024-03-01T12:39:47.051] _slurm_rpc_submit_batch_job: JobId=326 InitPrio=4294901735 usec=115
[2024-03-01T12:40:16.574] _slurm_rpc_submit_batch_job: JobId=327 InitPrio=4294901734 usec=153
[2024-03-01T12:40:17.456] sched: Allocate JobId=327 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T12:40:17.533] _job_complete: JobId=327 WEXITSTATUS 0
[2024-03-01T12:40:17.534] _job_complete: JobId=327 done
[2024-03-01T12:40:55.739] _slurm_rpc_submit_batch_job: JobId=328 InitPrio=4294901733 usec=128
[2024-03-01T12:41:06.086] sched: _slurm_rpc_allocate_resources JobId=329 NodeList=(null) usec=130
[2024-03-01T12:41:07.531] sched: Allocate JobId=329 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:41:07.583] _job_complete: JobId=329 WEXITSTATUS 13
[2024-03-01T12:41:07.583] _job_complete: JobId=329 done
[2024-03-01T12:41:22.693] _slurm_rpc_submit_batch_job: JobId=330 InitPrio=4294901731 usec=145
[2024-03-01T12:41:23.554] sched: Allocate JobId=330 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T12:41:23.618] _job_complete: JobId=330 WEXITSTATUS 0
[2024-03-01T12:41:23.618] _job_complete: JobId=330 done
[2024-03-01T12:42:18.310] _slurm_rpc_submit_batch_job: JobId=331 InitPrio=4294901730 usec=147
[2024-03-01T12:42:18.671] sched: Allocate JobId=331 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:42:21.176] _job_complete: JobId=331 WEXITSTATUS 0
[2024-03-01T12:42:21.176] _job_complete: JobId=331 done
[2024-03-01T12:43:04.791] sched: _slurm_rpc_allocate_resources JobId=332 NodeList=(null) usec=133
[2024-03-01T12:43:07.749] sched: Allocate JobId=332 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:43:07.799] _job_complete: JobId=332 WEXITSTATUS 13
[2024-03-01T12:43:07.799] _job_complete: JobId=332 done
[2024-03-01T12:43:26.314] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=326 uid 1008
[2024-03-01T12:43:30.964] _slurm_rpc_submit_batch_job: JobId=333 InitPrio=4294901728 usec=115
[2024-03-01T12:43:31.786] sched: Allocate JobId=333 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:43:44.003] _slurm_rpc_submit_batch_job: JobId=334 InitPrio=4294901727 usec=143
[2024-03-01T12:43:44.337] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=320 uid 1008
[2024-03-01T12:43:44.337] error: Security violation, REQUEST_KILL_JOB RPC for JobId=320 from uid 1008
[2024-03-01T12:43:44.337] _slurm_rpc_kill_job: job_str_signal() uid=1008 JobId=320 sig=9 returned: Access/permission denied
[2024-03-01T12:43:44.337] sched/backfill: _start_job: Started JobId=334 in express on compute00
[2024-03-01T12:43:44.754] _job_complete: JobId=334 WEXITSTATUS 139
[2024-03-01T12:43:44.754] _job_complete: JobId=334 done
[2024-03-01T12:43:44.845] _job_complete: JobId=333 WEXITSTATUS 0
[2024-03-01T12:43:44.845] _job_complete: JobId=333 done
[2024-03-01T12:43:48.142] _slurm_rpc_submit_batch_job: JobId=335 InitPrio=4294901726 usec=186
[2024-03-01T12:43:48.809] sched: Allocate JobId=335 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T12:43:48.891] _job_complete: JobId=335 WEXITSTATUS 0
[2024-03-01T12:43:48.891] _job_complete: JobId=335 done
[2024-03-01T12:44:10.681] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=320 uid 1030
[2024-03-01T12:44:19.216] _get_job_parts: invalid partition specified: spec
[2024-03-01T12:44:19.216] _slurm_rpc_submit_batch_job: Invalid partition name specified
[2024-03-01T12:44:32.303] _slurm_rpc_submit_batch_job: JobId=336 InitPrio=4294901725 usec=122
[2024-03-01T12:44:32.873] sched: Allocate JobId=336 NodeList=compute00 #CPUs=1 Partition=espec
[2024-03-01T12:44:45.948] _job_complete: JobId=336 WEXITSTATUS 0
[2024-03-01T12:44:45.948] _job_complete: JobId=336 done
[2024-03-01T12:45:25.384] _slurm_rpc_submit_batch_job: JobId=337 InitPrio=4294901724 usec=135
[2024-03-01T12:45:25.955] sched: Allocate JobId=337 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T12:45:26.031] _job_complete: JobId=337 WEXITSTATUS 0
[2024-03-01T12:45:26.031] _job_complete: JobId=337 done
[2024-03-01T12:46:17.350] sched: _slurm_rpc_allocate_resources JobId=338 NodeList=compute00 usec=155
[2024-03-01T12:46:17.400] _job_complete: JobId=338 WEXITSTATUS 13
[2024-03-01T12:46:17.400] _job_complete: JobId=338 done
[2024-03-01T12:46:29.233] sched: _slurm_rpc_allocate_resources JobId=339 NodeList=compute00 usec=153
[2024-03-01T12:46:29.283] _job_complete: JobId=339 WEXITSTATUS 13
[2024-03-01T12:46:29.284] _job_complete: JobId=339 done
[2024-03-01T12:46:40.679] _slurm_rpc_submit_batch_job: JobId=340 InitPrio=4294901721 usec=129
[2024-03-01T12:46:41.068] sched: Allocate JobId=340 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T12:46:41.147] _job_complete: JobId=340 WEXITSTATUS 0
[2024-03-01T12:46:41.147] _job_complete: JobId=340 done
[2024-03-01T12:46:46.303] sched: _slurm_rpc_allocate_resources JobId=341 NodeList=compute00 usec=169
[2024-03-01T12:46:46.350] _job_complete: JobId=341 WEXITSTATUS 13
[2024-03-01T12:46:46.350] _job_complete: JobId=341 done
[2024-03-01T12:46:59.293] sched: _slurm_rpc_allocate_resources JobId=342 NodeList=compute00 usec=148
[2024-03-01T12:46:59.340] _job_complete: JobId=342 WEXITSTATUS 13
[2024-03-01T12:46:59.340] _job_complete: JobId=342 done
[2024-03-01T12:47:04.131] sched: _slurm_rpc_allocate_resources JobId=343 NodeList=compute00 usec=151
[2024-03-01T12:47:04.180] _job_complete: JobId=343 WEXITSTATUS 13
[2024-03-01T12:47:04.181] _job_complete: JobId=343 done
[2024-03-01T12:47:09.510] sched: _slurm_rpc_allocate_resources JobId=344 NodeList=compute00 usec=155
[2024-03-01T12:47:09.558] _job_complete: JobId=344 WEXITSTATUS 13
[2024-03-01T12:47:09.558] _job_complete: JobId=344 done
[2024-03-01T12:47:27.163] sched: _slurm_rpc_allocate_resources JobId=345 NodeList=compute00 usec=159
[2024-03-01T12:48:27.516] _job_complete: JobId=345 WEXITSTATUS 0
[2024-03-01T12:48:27.516] _job_complete: JobId=345 done
[2024-03-01T12:49:19.674] sched: _slurm_rpc_allocate_resources JobId=346 NodeList=compute00 usec=152
[2024-03-01T12:53:29.282] sched: _slurm_rpc_allocate_resources JobId=347 NodeList=compute01 usec=156
[2024-03-01T12:54:13.686] _job_complete: JobId=347 WEXITSTATUS 0
[2024-03-01T12:54:13.686] _job_complete: JobId=347 done
[2024-03-01T12:58:39.595] _slurm_rpc_submit_batch_job: JobId=348 InitPrio=4294901713 usec=134
[2024-03-01T12:58:40.376] sched: Allocate JobId=348 NodeList=compute01 #CPUs=1 Partition=espec
[2024-03-01T12:59:06.492] _job_complete: JobId=348 WEXITSTATUS 0
[2024-03-01T12:59:06.492] _job_complete: JobId=348 done
[2024-03-01T13:01:03.322] _slurm_rpc_submit_batch_job: JobId=349 InitPrio=4294901712 usec=132
[2024-03-01T13:01:03.598] sched: Allocate JobId=349 NodeList=compute01 #CPUs=1 Partition=express
[2024-03-01T13:01:20.527] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=328 uid 1030
[2024-03-01T13:01:34.854] _job_complete: JobId=346 WEXITSTATUS 0
[2024-03-01T13:01:34.854] _job_complete: JobId=346 done
[2024-03-01T13:02:07.003] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=349 uid 1022
[2024-03-01T13:02:15.781] sched: _slurm_rpc_allocate_resources JobId=350 NodeList=compute00 usec=150
[2024-03-01T13:07:18.361] _job_complete: JobId=350 WEXITSTATUS 0
[2024-03-01T13:07:18.361] _job_complete: JobId=350 done
[2024-03-01T13:07:45.530] _slurm_rpc_submit_batch_job: JobId=351 InitPrio=4294901710 usec=132
[2024-03-01T13:07:46.359] sched: Allocate JobId=351 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T13:07:46.438] _job_complete: JobId=351 WEXITSTATUS 0
[2024-03-01T13:07:46.438] _job_complete: JobId=351 done
[2024-03-01T13:07:57.379] sched: _slurm_rpc_allocate_resources JobId=352 NodeList=compute00 usec=161
[2024-03-01T13:08:30.848] _job_complete: JobId=352 WEXITSTATUS 0
[2024-03-01T13:08:30.848] _job_complete: JobId=352 done
[2024-03-01T13:08:59.744] sched: _slurm_rpc_allocate_resources JobId=353 NodeList=compute00 usec=152
[2024-03-01T13:10:54.637] _job_complete: JobId=353 WEXITSTATUS 127
[2024-03-01T13:10:54.637] _job_complete: JobId=353 done
[2024-03-01T13:11:19.520] _slurm_rpc_submit_batch_job: JobId=354 InitPrio=4294901707 usec=138
[2024-03-01T13:11:19.702] sched: Allocate JobId=354 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-01T13:11:20.811] _job_complete: JobId=354 WEXITSTATUS 0
[2024-03-01T13:11:20.811] _job_complete: JobId=354 done
[2024-03-01T13:11:32.062] _slurm_rpc_submit_batch_job: JobId=355 InitPrio=4294901706 usec=142
[2024-03-01T13:11:32.722] sched: Allocate JobId=355 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T13:12:25.855] _slurm_rpc_submit_batch_job: JobId=356 InitPrio=4294901705 usec=146
[2024-03-01T13:12:26.419] sched/backfill: _start_job: Started JobId=356 in express on compute00
[2024-03-01T13:12:26.498] _job_complete: JobId=356 WEXITSTATUS 0
[2024-03-01T13:12:26.498] _job_complete: JobId=356 done
[2024-03-01T13:14:26.189] _slurm_rpc_submit_batch_job: JobId=357 InitPrio=4294901704 usec=149
[2024-03-01T13:14:27.041] sched: Allocate JobId=357 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T13:16:35.253] Time limit exhausted for JobId=355
[2024-03-01T13:17:14.406] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=357 uid 1030
[2024-03-01T13:21:43.243] sched: _slurm_rpc_allocate_resources JobId=358 NodeList=compute00 usec=155
[2024-03-01T13:24:21.491] _job_complete: JobId=358 WEXITSTATUS 0
[2024-03-01T13:24:21.491] _job_complete: JobId=358 done
[2024-03-01T13:28:32.405] _get_job_parts: invalid partition specified: EXPRESS
[2024-03-01T13:28:32.405] _slurm_rpc_submit_batch_job: Invalid partition name specified
[2024-03-01T13:28:43.548] _slurm_rpc_submit_batch_job: JobId=359 InitPrio=4294901702 usec=136
[2024-03-01T13:28:43.707] sched: Allocate JobId=359 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T13:28:43.778] _job_complete: JobId=359 WEXITSTATUS 2
[2024-03-01T13:28:43.778] _job_complete: JobId=359 done
[2024-03-01T13:30:02.527] sched: _slurm_rpc_allocate_resources JobId=360 NodeList=compute00 usec=162
[2024-03-01T13:30:29.656] _job_complete: JobId=360 WEXITSTATUS 0
[2024-03-01T13:30:29.656] _job_complete: JobId=360 done
[2024-03-01T13:31:04.652] _slurm_rpc_submit_batch_job: JobId=361 InitPrio=4294901700 usec=147
[2024-03-01T13:31:04.960] sched: Allocate JobId=361 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-01T13:31:05.039] _job_complete: JobId=361 WEXITSTATUS 0
[2024-03-01T13:31:05.039] _job_complete: JobId=361 done
[2024-03-05T09:32:26.464] _slurm_rpc_submit_batch_job: JobId=362 InitPrio=4294901699 usec=1975
[2024-03-05T09:32:26.533] sched/backfill: _start_job: Started JobId=362_1(363) in express on compute00
[2024-03-05T09:32:26.534] sched/backfill: _start_job: Started JobId=362_2(362) in express on compute00
[2024-03-05T09:32:26.573] _job_complete: JobId=362_1(363) WEXITSTATUS 127
[2024-03-05T09:32:26.574] _job_complete: JobId=362_1(363) done
[2024-03-05T09:32:26.579] _job_complete: JobId=362_2(362) WEXITSTATUS 127
[2024-03-05T09:32:26.579] _job_complete: JobId=362_2(362) done
[2024-03-05T09:32:53.999] _slurm_rpc_submit_batch_job: JobId=364 InitPrio=4294901698 usec=145
[2024-03-05T09:32:54.819] sched: Allocate JobId=364_1(365) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:32:54.819] sched: Allocate JobId=364_2(364) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:32:54.861] _job_complete: JobId=364_2(364) WEXITSTATUS 255
[2024-03-05T09:32:54.861] _job_complete: JobId=364_2(364) done
[2024-03-05T09:32:54.862] _job_complete: JobId=364_1(365) WEXITSTATUS 255
[2024-03-05T09:32:54.862] _job_complete: JobId=364_1(365) done
[2024-03-05T09:33:12.960] _slurm_rpc_submit_batch_job: JobId=366 InitPrio=4294901697 usec=132
[2024-03-05T09:33:13.842] sched: Allocate JobId=366_1(367) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:33:13.842] sched: Allocate JobId=366_2(366) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:33:13.907] _job_complete: JobId=366_1(367) WEXITSTATUS 0
[2024-03-05T09:33:13.907] _job_complete: JobId=366_1(367) done
[2024-03-05T09:33:13.907] _job_complete: JobId=366_2(366) WEXITSTATUS 0
[2024-03-05T09:33:13.907] _job_complete: JobId=366_2(366) done
[2024-03-05T09:34:06.245] _slurm_rpc_submit_batch_job: JobId=368 InitPrio=4294901696 usec=134
[2024-03-05T09:34:06.536] sched/backfill: _start_job: Started JobId=368_1(369) in express on compute00
[2024-03-05T09:34:06.536] sched/backfill: _start_job: Started JobId=368_2(368) in express on compute00
[2024-03-05T09:34:06.603] _job_complete: JobId=368_2(368) WEXITSTATUS 0
[2024-03-05T09:34:06.603] _job_complete: JobId=368_2(368) done
[2024-03-05T09:34:06.611] _job_complete: JobId=368_1(369) WEXITSTATUS 0
[2024-03-05T09:34:06.611] _job_complete: JobId=368_1(369) done
[2024-03-05T09:34:16.172] _slurm_rpc_submit_batch_job: JobId=370 InitPrio=4294901695 usec=133
[2024-03-05T09:34:16.927] sched: Allocate JobId=370 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:34:17.016] _job_complete: JobId=370 WEXITSTATUS 0
[2024-03-05T09:34:17.017] _job_complete: JobId=370 done
[2024-03-05T09:34:24.312] _slurm_rpc_submit_batch_job: JobId=371 InitPrio=4294901694 usec=131
[2024-03-05T09:34:24.938] sched: Allocate JobId=371_1(372) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:34:24.938] sched: Allocate JobId=371_2(371) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:34:25.012] _job_complete: JobId=371_1(372) WEXITSTATUS 0
[2024-03-05T09:34:25.013] _job_complete: JobId=371_1(372) done
[2024-03-05T09:34:25.013] _job_complete: JobId=371_2(371) WEXITSTATUS 0
[2024-03-05T09:34:25.013] _job_complete: JobId=371_2(371) done
[2024-03-05T09:35:18.621] _slurm_rpc_submit_batch_job: JobId=373 InitPrio=4294901693 usec=144
[2024-03-05T09:35:19.015] sched: Allocate JobId=373_1(374) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:35:19.015] sched: Allocate JobId=373_2(375) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:35:19.015] sched: Allocate JobId=373_3(376) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:35:19.015] sched: Allocate JobId=373_4(373) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:35:19.078] _job_complete: JobId=373_1(374) WEXITSTATUS 0
[2024-03-05T09:35:19.079] _job_complete: JobId=373_1(374) done
[2024-03-05T09:35:19.087] _job_complete: JobId=373_2(375) WEXITSTATUS 0
[2024-03-05T09:35:19.087] _job_complete: JobId=373_2(375) done
[2024-03-05T09:35:19.090] _job_complete: JobId=373_4(373) WEXITSTATUS 0
[2024-03-05T09:35:19.090] _job_complete: JobId=373_4(373) done
[2024-03-05T09:35:19.092] _job_complete: JobId=373_3(376) WEXITSTATUS 0
[2024-03-05T09:35:19.092] _job_complete: JobId=373_3(376) done
[2024-03-05T09:35:40.185] _slurm_rpc_submit_batch_job: JobId=377 InitPrio=4294901692 usec=128
[2024-03-05T09:35:41.050] sched: Allocate JobId=377_1(378) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:35:41.050] sched: Allocate JobId=377_2(379) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:35:41.050] sched: Allocate JobId=377_3(380) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:35:41.050] sched: Allocate JobId=377_4(377) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:35:41.108] _job_complete: JobId=377_1(378) WEXITSTATUS 0
[2024-03-05T09:35:41.108] _job_complete: JobId=377_1(378) done
[2024-03-05T09:35:41.117] _job_complete: JobId=377_4(377) WEXITSTATUS 0
[2024-03-05T09:35:41.117] _job_complete: JobId=377_4(377) done
[2024-03-05T09:35:41.118] _job_complete: JobId=377_2(379) WEXITSTATUS 0
[2024-03-05T09:35:41.118] _job_complete: JobId=377_2(379) done
[2024-03-05T09:35:41.120] _job_complete: JobId=377_3(380) WEXITSTATUS 0
[2024-03-05T09:35:41.120] _job_complete: JobId=377_3(380) done
[2024-03-05T09:36:21.245] _slurm_rpc_submit_batch_job: JobId=381 InitPrio=4294901691 usec=161
[2024-03-05T09:36:21.539] sched/backfill: _start_job: Started JobId=381_1(382) in express on compute00
[2024-03-05T09:36:21.539] sched/backfill: _start_job: Started JobId=381_2(383) in express on compute00
[2024-03-05T09:36:21.539] sched/backfill: _start_job: Started JobId=381_3(384) in express on compute00
[2024-03-05T09:36:21.539] sched/backfill: _start_job: Started JobId=381_4(381) in express on compute00
[2024-03-05T09:36:21.585] _job_complete: JobId=381_1(382) WEXITSTATUS 0
[2024-03-05T09:36:21.585] _job_complete: JobId=381_1(382) done
[2024-03-05T09:36:21.586] _job_complete: JobId=381_2(383) WEXITSTATUS 0
[2024-03-05T09:36:21.586] _job_complete: JobId=381_2(383) done
[2024-03-05T09:36:21.598] _job_complete: JobId=381_3(384) WEXITSTATUS 0
[2024-03-05T09:36:21.598] _job_complete: JobId=381_3(384) done
[2024-03-05T09:36:21.599] _job_complete: JobId=381_4(381) WEXITSTATUS 0
[2024-03-05T09:36:21.599] _job_complete: JobId=381_4(381) done
[2024-03-05T09:36:45.640] _slurm_rpc_submit_batch_job: JobId=385 InitPrio=4294901690 usec=139
[2024-03-05T09:36:46.136] sched: Allocate JobId=385 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:36:46.223] _job_complete: JobId=385 WEXITSTATUS 0
[2024-03-05T09:36:46.223] _job_complete: JobId=385 done
[2024-03-05T09:36:48.933] _slurm_rpc_submit_batch_job: JobId=386 InitPrio=4294901689 usec=269
[2024-03-05T09:36:49.140] sched: Allocate JobId=386_1(387) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:36:49.140] sched: Allocate JobId=386_2(388) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:36:49.140] sched: Allocate JobId=386_3(389) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:36:49.140] sched: Allocate JobId=386_4(386) NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:36:49.257] _job_complete: JobId=386_1(387) WEXITSTATUS 0
[2024-03-05T09:36:49.257] _job_complete: JobId=386_1(387) done
[2024-03-05T09:36:49.258] _job_complete: JobId=386_2(388) WEXITSTATUS 0
[2024-03-05T09:36:49.258] _job_complete: JobId=386_2(388) done
[2024-03-05T09:36:49.259] _job_complete: JobId=386_4(386) WEXITSTATUS 0
[2024-03-05T09:36:49.259] _job_complete: JobId=386_4(386) done
[2024-03-05T09:36:49.261] _job_complete: JobId=386_3(389) WEXITSTATUS 0
[2024-03-05T09:36:49.261] _job_complete: JobId=386_3(389) done
[2024-03-05T09:37:54.746] _slurm_rpc_submit_batch_job: JobId=390 InitPrio=4294901688 usec=150
[2024-03-05T09:37:55.241] sched: Allocate JobId=390 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:37:55.306] _job_complete: JobId=390 WEXITSTATUS 0
[2024-03-05T09:37:55.306] _job_complete: JobId=390 done
[2024-03-05T09:39:41.706] _slurm_rpc_submit_batch_job: JobId=391 InitPrio=4294901687 usec=148
[2024-03-05T09:39:42.386] sched: Allocate JobId=391 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:39:42.492] _job_complete: JobId=391 WEXITSTATUS 1
[2024-03-05T09:39:42.492] _job_complete: JobId=391 done
[2024-03-05T09:40:00.336] _slurm_rpc_submit_batch_job: JobId=392 InitPrio=4294901686 usec=202
[2024-03-05T09:40:00.409] sched: Allocate JobId=392 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:40:00.473] _job_complete: JobId=392 WEXITSTATUS 0
[2024-03-05T09:40:00.473] _job_complete: JobId=392 done
[2024-03-05T09:48:12.550] sched: _slurm_rpc_allocate_resources JobId=393 NodeList=compute00 usec=156
[2024-03-05T09:49:40.316] _job_complete: JobId=393 WEXITSTATUS 0
[2024-03-05T09:49:40.317] _job_complete: JobId=393 done
[2024-03-05T09:50:23.615] sched: _slurm_rpc_allocate_resources JobId=394 NodeList=compute00 usec=158
[2024-03-05T09:53:19.585] _job_complete: JobId=394 WEXITSTATUS 0
[2024-03-05T09:53:19.585] _job_complete: JobId=394 done
[2024-03-05T09:57:11.547] _slurm_rpc_submit_batch_job: JobId=395 InitPrio=4294901683 usec=136
[2024-03-05T09:57:11.600] sched/backfill: _start_job: Started JobId=395 in express on compute00
[2024-03-05T09:57:11.687] _job_complete: JobId=395 WEXITSTATUS 0
[2024-03-05T09:57:11.687] _job_complete: JobId=395 done
[2024-03-05T09:57:48.786] _slurm_rpc_submit_batch_job: JobId=396 InitPrio=4294901682 usec=136
[2024-03-05T09:57:49.448] sched: Allocate JobId=396 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-05T09:57:49.537] _job_complete: JobId=396 WEXITSTATUS 0
[2024-03-05T09:57:49.537] _job_complete: JobId=396 done
[2024-03-05T11:30:57.045] error: Nodes compute03 not responding, setting DOWN
[2024-03-05T11:32:09.171] error: Nodes compute02 not responding
[2024-03-05T11:35:57.615] error: Nodes compute02 not responding, setting DOWN
[2024-03-05T11:37:09.731] error: Nodes compute01 not responding
[2024-03-05T11:38:43.740] Terminate signal (SIGINT or SIGTERM) received
[2024-03-05T11:38:43.832] Saving all slurm state
[2024-03-05T11:47:59.203] error: chdir(/var/log): Permission denied
[2024-03-05T11:47:59.207] error: Configured MailProg is invalid
[2024-03-05T11:47:59.207] slurmctld version 22.05.11 started on cluster cluster
[2024-03-05T11:47:59.407] No memory enforcing mechanism configured.
[2024-03-05T11:47:59.435] Recovered state of 4 nodes
[2024-03-05T11:47:59.435] Down nodes: compute[02-03]
[2024-03-05T11:47:59.440] Recovered information about 0 jobs
[2024-03-05T11:47:59.440] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-03-05T11:47:59.484] Recovered state of 0 reservations
[2024-03-05T11:47:59.484] read_slurm_conf: backup_controller not specified
[2024-03-05T11:47:59.484] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-03-05T11:47:59.484] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-03-05T11:47:59.484] Running as primary controller
[2024-03-05T11:47:59.490] No parameter for mcs plugin, default values set
[2024-03-05T11:47:59.490] mcs: MCSParameters = (null). ondemand set.
[2024-03-05T11:48:59.949] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-03-05T11:49:43.044] error: Nodes compute[00-01] not responding, setting DOWN
[2024-03-05T11:50:18.926] Node compute03 now responding
[2024-03-05T11:50:18.926] validate_node_specs: Node compute03 unexpectedly rebooted boot_time=1709656510 last response=1709655857
[2024-03-05T11:58:08.698] Node compute01 now responding
[2024-03-05T11:58:08.698] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1709657650 last response=1709656269
[2024-03-05T11:58:08.699] Node compute00 now responding
[2024-03-05T11:58:08.699] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1709657572 last response=1709656469
[2024-03-05T11:58:08.700] Node compute02 now responding
[2024-03-05T11:58:08.700] validate_node_specs: Node compute02 unexpectedly rebooted boot_time=1709657643 last response=1709656069
[2024-03-05T11:59:06.488] update_node: node compute00 state set to IDLE
[2024-03-05T11:59:07.144] Node compute00 now responding
[2024-03-05T11:59:11.241] update_node: node compute01 state set to IDLE
[2024-03-05T11:59:12.155] Node compute01 now responding
[2024-03-05T11:59:14.462] update_node: node compute02 state set to IDLE
[2024-03-05T11:59:15.161] Node compute02 now responding
[2024-03-05T11:59:18.285] update_node: node compute03 state set to IDLE
[2024-03-05T11:59:19.169] Node compute03 now responding
[2024-03-07T14:34:52.341] error: chdir(/var/log): Permission denied
[2024-03-07T14:34:52.347] error: Configured MailProg is invalid
[2024-03-07T14:34:52.347] slurmctld version 22.05.11 started on cluster cluster
[2024-03-07T14:34:52.530] No memory enforcing mechanism configured.
[2024-03-07T14:34:52.581] Recovered state of 4 nodes
[2024-03-07T14:34:52.582] Recovered information about 0 jobs
[2024-03-07T14:34:52.582] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-03-07T14:34:52.630] Recovered state of 0 reservations
[2024-03-07T14:34:52.631] read_slurm_conf: backup_controller not specified
[2024-03-07T14:34:52.631] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-03-07T14:34:52.631] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-03-07T14:34:52.631] Running as primary controller
[2024-03-07T14:34:52.632] No parameter for mcs plugin, default values set
[2024-03-07T14:34:52.632] mcs: MCSParameters = (null). ondemand set.
[2024-03-07T14:34:57.670] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-03-08T05:38:39.016] Invalid node state transition requested for node compute00 from=IDLE to=RESUME
[2024-03-08T05:38:39.016] Invalid node state transition requested for node compute01 from=IDLE to=RESUME
[2024-03-08T05:38:39.016] Invalid node state transition requested for node compute02 from=IDLE to=RESUME
[2024-03-08T05:38:39.016] Invalid node state transition requested for node compute03 from=IDLE to=RESUME
[2024-03-08T05:38:39.016] _slurm_rpc_update_node for compute0[0-3]: Invalid node state specified
[2024-03-08T05:39:09.390] Invalid node state transition requested for node compute00 from=IDLE to=RESUME
[2024-03-08T05:39:09.390] Invalid node state transition requested for node compute01 from=IDLE to=RESUME
[2024-03-08T05:39:09.390] Invalid node state transition requested for node compute02 from=IDLE to=RESUME
[2024-03-08T05:39:09.390] Invalid node state transition requested for node compute03 from=IDLE to=RESUME
[2024-03-08T05:39:09.390] _slurm_rpc_update_node for compute0[0-3]: Invalid node state specified
[2024-03-08T05:39:49.601] sched: _slurm_rpc_allocate_resources JobId=397 NodeList=compute00 usec=197
[2024-03-08T05:39:58.365] _job_complete: JobId=397 WEXITSTATUS 0
[2024-03-08T05:39:58.386] _job_complete: JobId=397 done
[2024-03-12T08:13:19.354] error: Nodes compute02 not responding, setting DOWN
[2024-03-12T09:57:25.142] _slurm_rpc_submit_batch_job: JobId=398 InitPrio=4294901758 usec=134
[2024-03-12T09:57:25.612] sched: Allocate JobId=398 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T09:57:25.683] _job_complete: JobId=398 WEXITSTATUS 127
[2024-03-12T09:57:25.683] _job_complete: JobId=398 done
[2024-03-12T09:58:19.562] _slurm_rpc_submit_batch_job: JobId=399 InitPrio=4294901757 usec=141
[2024-03-12T09:58:19.694] sched: Allocate JobId=399 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T09:58:19.789] _job_complete: JobId=399 WEXITSTATUS 127
[2024-03-12T09:58:19.789] _job_complete: JobId=399 done
[2024-03-12T09:58:53.103] _slurm_rpc_submit_batch_job: JobId=400 InitPrio=4294901756 usec=136
[2024-03-12T09:58:53.132] sched/backfill: _start_job: Started JobId=400 in normal on compute03
[2024-03-12T09:58:53.189] _job_complete: JobId=400 WEXITSTATUS 1
[2024-03-12T09:58:53.189] _job_complete: JobId=400 done
[2024-03-12T09:59:33.810] _slurm_rpc_submit_batch_job: JobId=401 InitPrio=4294901755 usec=136
[2024-03-12T09:59:34.807] sched: Allocate JobId=401 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T09:59:34.883] _job_complete: JobId=401 WEXITSTATUS 0
[2024-03-12T09:59:34.883] _job_complete: JobId=401 done
[2024-03-12T09:59:57.945] _slurm_rpc_submit_batch_job: JobId=402 InitPrio=4294901754 usec=145
[2024-03-12T09:59:58.866] sched: Allocate JobId=402 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T09:59:58.947] _job_complete: JobId=402 WEXITSTATUS 1
[2024-03-12T09:59:58.947] _job_complete: JobId=402 done
[2024-03-12T10:00:51.797] _slurm_rpc_submit_batch_job: JobId=403 InitPrio=4294901753 usec=152
[2024-03-12T10:00:51.938] sched: Allocate JobId=403 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:00:52.059] _job_complete: JobId=403 WEXITSTATUS 1
[2024-03-12T10:00:52.059] _job_complete: JobId=403 done
[2024-03-12T10:02:17.253] _slurm_rpc_submit_batch_job: JobId=404 InitPrio=4294901752 usec=146
[2024-03-12T10:02:18.139] sched/backfill: _start_job: Started JobId=404 in normal on compute03
[2024-03-12T10:02:18.221] _job_complete: JobId=404 WEXITSTATUS 0
[2024-03-12T10:02:18.221] _job_complete: JobId=404 done
[2024-03-12T10:03:35.412] _slurm_rpc_submit_batch_job: JobId=405 InitPrio=4294901751 usec=142
[2024-03-12T10:03:36.162] sched: Allocate JobId=405 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:03:36.232] _job_complete: JobId=405 WEXITSTATUS 1
[2024-03-12T10:03:36.232] _job_complete: JobId=405 done
[2024-03-12T10:03:58.076] _slurm_rpc_submit_batch_job: JobId=406 InitPrio=4294901750 usec=147
[2024-03-12T10:03:58.204] sched: Allocate JobId=406 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:03:58.322] _job_complete: JobId=406 WEXITSTATUS 0
[2024-03-12T10:03:58.322] _job_complete: JobId=406 done
[2024-03-12T10:04:46.720] _slurm_rpc_submit_batch_job: JobId=407 InitPrio=4294901749 usec=154
[2024-03-12T10:04:47.293] sched: Allocate JobId=407 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:04:47.378] _job_complete: JobId=407 WEXITSTATUS 127
[2024-03-12T10:04:47.378] _job_complete: JobId=407 done
[2024-03-12T10:05:02.089] _slurm_rpc_submit_batch_job: JobId=408 InitPrio=4294901748 usec=147
[2024-03-12T10:05:02.344] sched: Allocate JobId=408 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:05:02.425] _job_complete: JobId=408 WEXITSTATUS 0
[2024-03-12T10:05:02.425] _job_complete: JobId=408 done
[2024-03-12T10:05:20.561] _slurm_rpc_submit_batch_job: JobId=409 InitPrio=4294901747 usec=144
[2024-03-12T10:05:21.374] sched: Allocate JobId=409 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:05:21.453] _job_complete: JobId=409 WEXITSTATUS 0
[2024-03-12T10:05:21.453] _job_complete: JobId=409 done
[2024-03-12T10:05:50.559] _slurm_rpc_submit_batch_job: JobId=410 InitPrio=4294901746 usec=152
[2024-03-12T10:05:51.426] sched: Allocate JobId=410 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:05:51.506] _job_complete: JobId=410 WEXITSTATUS 0
[2024-03-12T10:05:51.507] _job_complete: JobId=410 done
[2024-03-12T10:11:25.375] _slurm_rpc_submit_batch_job: JobId=411 InitPrio=4294901745 usec=139
[2024-03-12T10:11:25.998] sched: Allocate JobId=411 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:11:26.057] _job_complete: JobId=411 WEXITSTATUS 0
[2024-03-12T10:11:26.057] _job_complete: JobId=411 done
[2024-03-12T10:11:58.919] _slurm_rpc_submit_batch_job: JobId=412 InitPrio=4294901744 usec=164
[2024-03-12T10:11:59.051] sched: Allocate JobId=412 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:11:59.164] _job_complete: JobId=412 WEXITSTATUS 0
[2024-03-12T10:11:59.164] _job_complete: JobId=412 done
[2024-03-12T10:12:25.286] _slurm_rpc_submit_batch_job: JobId=413 InitPrio=4294901743 usec=133
[2024-03-12T10:12:26.095] sched: Allocate JobId=413 NodeList=compute03 #CPUs=1 Partition=normal
[2024-03-12T10:12:26.178] _job_complete: JobId=413 WEXITSTATUS 0
[2024-03-12T10:12:26.178] _job_complete: JobId=413 done
[2024-03-12T10:23:27.015] sched: _slurm_rpc_allocate_resources JobId=414 NodeList=compute03 usec=148
[2024-03-12T10:24:01.139] _slurm_rpc_submit_batch_job: JobId=415 InitPrio=4294901741 usec=143
[2024-03-12T10:24:01.186] sched: Allocate JobId=415 NodeList=compute00 #CPUs=1 Partition=normal
[2024-03-12T10:24:01.251] _job_complete: JobId=415 WEXITSTATUS 0
[2024-03-12T10:24:01.251] _job_complete: JobId=415 done
[2024-03-12T10:26:48.776] _job_complete: JobId=414 WEXITSTATUS 127
[2024-03-12T10:26:48.776] _job_complete: JobId=414 done
[2024-03-12T10:29:34.929] _slurm_rpc_submit_batch_job: JobId=416 InitPrio=4294901740 usec=141
[2024-03-12T10:29:35.744] sched: Allocate JobId=416 NodeList=compute03 #CPUs=1 Partition=express
[2024-03-12T10:29:35.823] _job_complete: JobId=416 WEXITSTATUS 0
[2024-03-12T10:29:35.823] _job_complete: JobId=416 done
[2024-03-12T10:29:50.561] _slurm_rpc_submit_batch_job: JobId=417 InitPrio=4294901739 usec=133
[2024-03-12T10:29:50.763] sched: Allocate JobId=417 NodeList=compute03 #CPUs=1 Partition=express
[2024-03-12T10:29:50.845] _job_complete: JobId=417 WEXITSTATUS 0
[2024-03-12T10:29:50.845] _job_complete: JobId=417 done
[2024-03-12T10:29:59.692] _slurm_rpc_submit_batch_job: JobId=418 InitPrio=4294901738 usec=148
[2024-03-12T10:29:59.801] sched: Allocate JobId=418 NodeList=compute03 #CPUs=1 Partition=express
[2024-03-12T10:29:59.881] _job_complete: JobId=418 WEXITSTATUS 0
[2024-03-12T10:29:59.881] _job_complete: JobId=418 done
[2024-03-12T10:30:21.065] _slurm_rpc_submit_batch_job: JobId=419 InitPrio=4294901737 usec=138
[2024-03-12T10:30:21.833] sched: Allocate JobId=419 NodeList=compute03 #CPUs=1 Partition=express
[2024-03-12T10:30:21.893] _job_complete: JobId=419 WEXITSTATUS 0
[2024-03-12T10:30:21.893] _job_complete: JobId=419 done
[2024-03-12T10:30:33.703] _slurm_rpc_submit_batch_job: JobId=420 InitPrio=4294901736 usec=129
[2024-03-12T10:30:33.851] sched: Allocate JobId=420 NodeList=compute03 #CPUs=1 Partition=express
[2024-03-12T10:30:33.948] _job_complete: JobId=420 WEXITSTATUS 0
[2024-03-12T10:30:33.948] _job_complete: JobId=420 done
[2024-03-12T10:30:41.216] _slurm_rpc_submit_batch_job: JobId=421 InitPrio=4294901735 usec=20679
[2024-03-12T10:30:41.863] sched: Allocate JobId=421 NodeList=compute03 #CPUs=1 Partition=express
[2024-03-12T10:30:41.923] _job_complete: JobId=421 WEXITSTATUS 0
[2024-03-12T10:30:41.923] _job_complete: JobId=421 done
[2024-03-13T14:16:21.232] _slurm_rpc_submit_batch_job: JobId=422 InitPrio=4294901734 usec=137
[2024-03-13T14:16:22.018] sched: Allocate JobId=422 NodeList=compute03 #CPUs=4 Partition=normal
[2024-03-13T14:16:22.093] _job_complete: JobId=422 WEXITSTATUS 127
[2024-03-13T14:16:22.093] _job_complete: JobId=422 done
[2024-03-13T14:20:16.315] sched: _slurm_rpc_allocate_resources JobId=423 NodeList=compute03 usec=176
[2024-03-13T14:20:39.725] _job_complete: JobId=423 WEXITSTATUS 0
[2024-03-13T14:20:39.725] _job_complete: JobId=423 done
[2024-03-13T14:21:37.806] _slurm_rpc_submit_batch_job: JobId=424 InitPrio=4294901732 usec=152
[2024-03-13T14:21:38.534] sched: Allocate JobId=424 NodeList=compute03 #CPUs=4 Partition=normal
[2024-03-13T14:21:38.632] _job_complete: JobId=424 WEXITSTATUS 0
[2024-03-13T14:21:38.632] _job_complete: JobId=424 done
[2024-03-13T14:24:00.664] _slurm_rpc_submit_batch_job: JobId=425 InitPrio=4294901731 usec=138
[2024-03-13T14:24:29.382] update_node: node compute02 state set to IDLE
[2024-03-13T14:24:29.769] Node compute02 now responding
[2024-03-15T06:58:30.029] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=425 uid 1014
[2024-03-15T06:59:43.325] _slurm_rpc_submit_batch_job: JobId=426 InitPrio=4294901730 usec=135
[2024-03-15T06:59:51.287] _slurm_rpc_submit_batch_job: JobId=427 InitPrio=4294901729 usec=118
[2024-03-15T06:59:59.837] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=426 uid 1014
[2024-03-15T07:00:01.496] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=427 uid 1014
[2024-03-15T07:13:20.909] _slurm_rpc_submit_batch_job: JobId=428 InitPrio=4294901728 usec=133
[2024-03-15T07:13:21.791] sched: Allocate JobId=428 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-15T07:13:21.875] _job_complete: JobId=428 WEXITSTATUS 0
[2024-03-15T07:13:21.875] _job_complete: JobId=428 done
[2024-03-15T07:20:13.801] _slurm_rpc_submit_batch_job: JobId=429 InitPrio=4294901727 usec=140
[2024-03-15T07:20:14.515] sched: Allocate JobId=429 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-15T07:20:14.615] _job_complete: JobId=429 WEXITSTATUS 0
[2024-03-15T07:20:14.615] _job_complete: JobId=429 done
[2024-03-15T07:56:35.574] _slurm_rpc_submit_batch_job: JobId=430 InitPrio=4294901726 usec=139
[2024-03-15T07:56:36.378] sched: Allocate JobId=430 NodeList=compute00 #CPUs=1 Partition=express
[2024-03-15T07:56:36.479] _job_complete: JobId=430 WEXITSTATUS 0
[2024-03-15T07:56:36.479] _job_complete: JobId=430 done
[2024-04-02T09:55:21.752] Terminate signal (SIGINT or SIGTERM) received
[2024-04-02T09:55:21.790] Saving all slurm state
[2024-04-02T11:38:05.449] error: chdir(/var/log): Permission denied
[2024-04-02T11:38:05.468] error: Configured MailProg is invalid
[2024-04-02T11:38:05.469] slurmctld version 22.05.11 started on cluster cluster
[2024-04-02T11:38:05.736] No memory enforcing mechanism configured.
[2024-04-02T11:38:05.788] Recovered state of 4 nodes
[2024-04-02T11:38:05.793] Recovered information about 0 jobs
[2024-04-02T11:38:05.793] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-02T11:38:05.804] Recovered state of 0 reservations
[2024-04-02T11:38:05.810] read_slurm_conf: backup_controller not specified
[2024-04-02T11:38:05.810] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-04-02T11:38:05.810] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-02T11:38:05.810] Running as primary controller
[2024-04-02T11:38:05.811] No parameter for mcs plugin, default values set
[2024-04-02T11:38:05.811] mcs: MCSParameters = (null). ondemand set.
[2024-04-02T11:38:09.857] error: slurm_receive_msgs: [[compute03.localdomain]:6818] failed: Zero Bytes were transmitted or received
[2024-04-02T11:38:09.858] error: slurm_receive_msgs: [[compute02.localdomain]:6818] failed: Zero Bytes were transmitted or received
[2024-04-02T11:38:09.858] error: slurm_receive_msgs: [[compute01.localdomain]:6818] failed: Zero Bytes were transmitted or received
[2024-04-02T11:38:09.858] error: slurm_receive_msgs: [[compute00.localdomain]:6818] failed: Zero Bytes were transmitted or received
[2024-04-02T12:38:10.842] error: Nodes compute[00-03] not responding
[2024-04-02T12:38:10.842] error: Nodes compute[00-03] not responding, setting DOWN
[2024-04-02T12:38:10.842] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-04-02T12:39:50.098] Node compute01 now responding
[2024-04-02T12:39:50.098] node compute01 returned to service
[2024-04-02T12:39:50.098] Node compute03 now responding
[2024-04-02T12:39:50.098] node compute03 returned to service
[2024-04-02T12:39:50.098] Node compute02 now responding
[2024-04-02T12:39:50.098] node compute02 returned to service
[2024-04-02T12:39:50.099] Node compute00 now responding
[2024-04-02T12:39:50.099] node compute00 returned to service
[2024-04-02T12:40:09.259] Invalid node state transition requested for node compute03 from=IDLE to=RESUME
[2024-04-02T12:40:09.259] _slurm_rpc_update_node for compute0[3]: Invalid node state specified
[2024-04-02T12:40:19.587] Invalid node state transition requested for node compute00 from=IDLE to=RESUME
[2024-04-02T12:40:19.587] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-04-02T12:46:30.822] error: Nodes compute[00-02] not responding, setting DOWN
[2024-04-02T12:51:14.288] Node compute02 now responding
[2024-04-02T12:51:14.288] validate_node_specs: Node compute02 unexpectedly rebooted boot_time=1712076147 last response=1712075990
[2024-04-02T12:51:14.289] Node compute01 now responding
[2024-04-02T12:51:14.289] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1712076152 last response=1712075990
[2024-04-02T12:51:14.295] Node compute00 now responding
[2024-04-02T12:51:14.295] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1712076097 last response=1712075990
[2024-04-02T12:51:39.225] update_node: node compute00 state set to IDLE
[2024-04-02T12:51:39.534] Node compute00 now responding
[2024-04-02T12:51:42.684] update_node: node compute01 state set to IDLE
[2024-04-02T12:51:43.541] Node compute01 now responding
[2024-04-02T12:51:46.750] update_node: node compute02 state set to IDLE
[2024-04-02T12:51:47.549] Node compute02 now responding
[2024-04-02T12:51:50.209] Invalid node state transition requested for node compute03 from=IDLE to=RESUME
[2024-04-02T12:51:50.210] _slurm_rpc_update_node for compute03: Invalid node state specified
[2024-04-02T12:58:10.322] error: Nodes compute03 not responding
[2024-04-02T12:58:27.384] error: Nodes compute03 not responding, setting DOWN
[2024-04-02T13:01:58.904] error: chdir(/var/log): Permission denied
[2024-04-02T13:01:58.911] error: Configured MailProg is invalid
[2024-04-02T13:01:58.911] slurmctld version 22.05.11 started on cluster cluster
[2024-04-02T13:01:59.083] No memory enforcing mechanism configured.
[2024-04-02T13:01:59.125] Recovered state of 4 nodes
[2024-04-02T13:01:59.126] Recovered information about 0 jobs
[2024-04-02T13:01:59.126] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-02T13:01:59.144] Recovered state of 0 reservations
[2024-04-02T13:01:59.144] read_slurm_conf: backup_controller not specified
[2024-04-02T13:01:59.144] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-04-02T13:01:59.144] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-02T13:01:59.144] Running as primary controller
[2024-04-02T13:01:59.148] No parameter for mcs plugin, default values set
[2024-04-02T13:01:59.148] mcs: MCSParameters = (null). ondemand set.
[2024-04-02T13:02:04.181] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-04-02T13:03:26.955] Node compute03 now responding
[2024-04-02T13:03:43.398] error: Nodes compute02 not responding, setting DOWN
[2024-04-02T13:03:46.104] Invalid node state transition requested for node compute03 from=IDLE to=RESUME
[2024-04-02T13:03:46.104] _slurm_rpc_update_node for compute0[3]: Invalid node state specified
[2024-04-02T13:05:02.990] update_node: node compute02 state set to IDLE
[2024-04-02T13:05:03.582] Node compute02 now responding
[2024-04-02T13:06:08.797] sched: _slurm_rpc_allocate_resources JobId=431 NodeList=compute00 usec=205
[2024-04-02T13:06:08.845] _job_complete: JobId=431 WEXITSTATUS 13
[2024-04-02T13:06:08.845] _job_complete: JobId=431 done
[2024-04-02T13:06:18.526] _slurm_rpc_submit_batch_job: JobId=432 InitPrio=4294901758 usec=150
[2024-04-02T13:06:18.746] sched: Allocate JobId=432 NodeList=compute00 #CPUs=1 Partition=normal
[2024-04-02T13:06:59.809] error: Nodes compute03 not responding
[2024-04-02T13:07:04.191] _job_complete: JobId=432 WEXITSTATUS 0
[2024-04-02T13:07:04.191] _job_complete: JobId=432 done
[2024-04-05T12:26:46.049] error: Nodes compute03 not responding, setting DOWN
[2024-04-05T12:27:55.535] Terminate signal (SIGINT or SIGTERM) received
[2024-04-05T12:27:55.633] Saving all slurm state
[2024-04-05T12:28:20.124] error: chdir(/var/log): Permission denied
[2024-04-05T12:28:20.131] error: Configured MailProg is invalid
[2024-04-05T12:28:20.131] slurmctld version 22.05.11 started on cluster cluster
[2024-04-05T12:28:20.333] No memory enforcing mechanism configured.
[2024-04-05T12:28:20.361] Recovered state of 4 nodes
[2024-04-05T12:28:20.361] Down nodes: compute03
[2024-04-05T12:28:20.364] Recovered information about 0 jobs
[2024-04-05T12:28:20.364] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-05T12:28:20.388] Recovered state of 0 reservations
[2024-04-05T12:28:20.389] read_slurm_conf: backup_controller not specified
[2024-04-05T12:28:20.389] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-04-05T12:28:20.389] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-05T12:28:20.389] Running as primary controller
[2024-04-05T12:28:20.389] No parameter for mcs plugin, default values set
[2024-04-05T12:28:20.389] mcs: MCSParameters = (null). ondemand set.
[2024-04-05T12:28:25.427] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-04-05T13:16:05.763] Terminate signal (SIGINT or SIGTERM) received
[2024-04-05T13:16:05.768] Saving all slurm state
[2024-04-05T13:16:30.218] error: chdir(/var/log): Permission denied
[2024-04-05T13:16:30.223] error: Configured MailProg is invalid
[2024-04-05T13:16:30.223] slurmctld version 22.05.11 started on cluster cluster
[2024-04-05T13:16:30.382] No memory enforcing mechanism configured.
[2024-04-05T13:16:30.434] Recovered state of 4 nodes
[2024-04-05T13:16:30.434] Down nodes: compute03
[2024-04-05T13:16:30.438] Recovered information about 0 jobs
[2024-04-05T13:16:30.438] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-05T13:16:30.457] Recovered state of 0 reservations
[2024-04-05T13:16:30.458] read_slurm_conf: backup_controller not specified
[2024-04-05T13:16:30.458] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-04-05T13:16:30.458] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-05T13:16:30.458] Running as primary controller
[2024-04-05T13:16:30.464] No parameter for mcs plugin, default values set
[2024-04-05T13:16:30.464] mcs: MCSParameters = (null). ondemand set.
[2024-04-05T13:16:35.475] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-04-05T15:03:48.104] Terminate signal (SIGINT or SIGTERM) received
[2024-04-05T15:03:48.109] Saving all slurm state
[2024-04-05T15:03:48.139] error: chdir(/var/log): Permission denied
[2024-04-05T15:03:48.139] error: Configured MailProg is invalid
[2024-04-05T15:03:48.139] slurmctld version 22.05.11 started on cluster cluster
[2024-04-05T15:03:48.141] No memory enforcing mechanism configured.
[2024-04-05T15:03:48.142] Recovered state of 4 nodes
[2024-04-05T15:03:48.142] Down nodes: compute03
[2024-04-05T15:03:48.142] Recovered information about 0 jobs
[2024-04-05T15:03:48.142] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-05T15:03:48.142] Recovered state of 0 reservations
[2024-04-05T15:03:48.142] read_slurm_conf: backup_controller not specified
[2024-04-05T15:03:48.142] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-04-05T15:03:48.142] select/cons_res: part_data_create_array: select/cons_res: preparing for 3 partitions
[2024-04-05T15:03:48.142] Running as primary controller
[2024-04-05T15:03:48.142] No parameter for mcs plugin, default values set
[2024-04-05T15:03:48.142] mcs: MCSParameters = (null). ondemand set.
[2024-04-05T15:03:53.152] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-04-05T15:04:08.298] error: _find_node_record: lookup failure for node "compute04"
[2024-04-05T15:04:08.298] error: update_node: node compute04 does not exist
[2024-04-05T15:04:08.298] _slurm_rpc_update_node for compute04: Invalid node name specified
[2024-04-05T15:07:22.916] Terminate signal (SIGINT or SIGTERM) received
[2024-04-05T15:07:22.922] Saving all slurm state
[2024-04-05T15:07:22.952] error: chdir(/var/log): Permission denied
[2024-04-05T15:07:22.952] error: Configured MailProg is invalid
[2024-04-05T15:07:22.952] slurmctld version 22.05.11 started on cluster cluster
[2024-04-05T15:07:22.954] No memory enforcing mechanism configured.
[2024-04-05T15:07:22.955] Recovered state of 4 nodes
[2024-04-05T15:07:22.955] Down nodes: compute03
[2024-04-05T15:07:22.955] Recovered information about 0 jobs
[2024-04-05T15:07:22.955] error: _find_node_record: lookup failure for node "compute04"
[2024-04-05T15:07:22.955] build_part_bitmap: invalid node name compute04 in partition
[2024-04-05T15:07:22.955] fatal: Invalid node names in partition normal
[2024-04-05T15:12:04.818] error: chdir(/var/log): Permission denied
[2024-04-05T15:12:04.818] error: Configured MailProg is invalid
[2024-04-05T15:12:04.818] slurmctld version 22.05.11 started on cluster cluster
[2024-04-05T15:12:04.820] No memory enforcing mechanism configured.
[2024-04-05T15:12:04.842] Recovered state of 4 nodes
[2024-04-05T15:12:04.842] Down nodes: compute03
[2024-04-05T15:12:04.842] Recovered information about 0 jobs
[2024-04-05T15:12:04.842] error: _find_node_record: lookup failure for node "compute04"
[2024-04-05T15:12:04.842] build_part_bitmap: invalid node name compute04 in partition
[2024-04-05T15:12:04.842] fatal: Invalid node names in partition normal
[2024-04-05T15:12:41.340] error: chdir(/var/log): Permission denied
[2024-04-05T15:12:41.340] error: Configured MailProg is invalid
[2024-04-05T15:12:41.340] slurmctld version 22.05.11 started on cluster cluster
[2024-04-05T15:12:41.341] No memory enforcing mechanism configured.
[2024-04-05T15:12:41.363] Recovered state of 4 nodes
[2024-04-05T15:12:41.363] Down nodes: compute03
[2024-04-05T15:12:41.363] Recovered information about 0 jobs
[2024-04-05T15:12:41.363] error: _find_node_record: lookup failure for node "compute04"
[2024-04-05T15:12:41.363] build_part_bitmap: invalid node name compute04 in partition
[2024-04-05T15:12:41.363] fatal: Invalid node names in partition normal
[2024-04-05T15:20:08.009] error: chdir(/var/log): Permission denied
[2024-04-05T15:20:08.009] error: Configured MailProg is invalid
[2024-04-05T15:20:08.009] slurmctld version 22.05.11 started on cluster cluster
[2024-04-05T15:20:08.011] No memory enforcing mechanism configured.
[2024-04-05T15:20:08.033] Recovered state of 4 nodes
[2024-04-05T15:20:08.033] Down nodes: compute03
[2024-04-05T15:20:08.033] Recovered information about 0 jobs
[2024-04-05T15:20:08.033] select/cons_res: part_data_create_array: select/cons_res: preparing for 4 partitions
[2024-04-05T15:20:08.033] Recovered state of 0 reservations
[2024-04-05T15:20:08.033] read_slurm_conf: backup_controller not specified
[2024-04-05T15:20:08.033] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-04-05T15:20:08.033] select/cons_res: part_data_create_array: select/cons_res: preparing for 4 partitions
[2024-04-05T15:20:08.033] Running as primary controller
[2024-04-05T15:20:08.033] No parameter for mcs plugin, default values set
[2024-04-05T15:20:08.033] mcs: MCSParameters = (null). ondemand set.
[2024-04-05T15:20:12.049] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-04-05T15:20:12.049] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-04-05T15:20:12.050] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-04-05T15:20:13.044] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-04-05T15:20:43.646] Terminate signal (SIGINT or SIGTERM) received
[2024-04-05T15:20:43.712] Saving all slurm state
[2024-04-05T15:20:43.742] error: chdir(/var/log): Permission denied
[2024-04-05T15:20:43.742] error: Configured MailProg is invalid
[2024-04-05T15:20:43.742] slurmctld version 22.05.11 started on cluster cluster
[2024-04-05T15:20:43.744] No memory enforcing mechanism configured.
[2024-04-05T15:20:43.745] Recovered state of 5 nodes
[2024-04-05T15:20:43.745] Down nodes: compute03
[2024-04-05T15:20:43.745] Recovered information about 0 jobs
[2024-04-05T15:20:43.745] select/cons_res: part_data_create_array: select/cons_res: preparing for 4 partitions
[2024-04-05T15:20:43.745] Recovered state of 0 reservations
[2024-04-05T15:20:43.745] read_slurm_conf: backup_controller not specified
[2024-04-05T15:20:43.745] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-04-05T15:20:43.745] select/cons_res: part_data_create_array: select/cons_res: preparing for 4 partitions
[2024-04-05T15:20:43.745] Running as primary controller
[2024-04-05T15:20:43.745] No parameter for mcs plugin, default values set
[2024-04-05T15:20:43.745] mcs: MCSParameters = (null). ondemand set.
[2024-04-05T15:20:47.760] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-04-05T15:20:47.760] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-04-05T15:20:47.761] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-04-05T15:20:48.757] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-04-05T15:25:43.434] error: Nodes compute04 not responding
[2024-04-05T15:30:43.132] error: Nodes compute04 not responding
[2024-04-05T15:35:43.858] error: Nodes compute04 not responding
[2024-04-05T15:40:43.593] error: Nodes compute04 not responding
[2024-04-05T15:45:43.298] error: Nodes compute04 not responding
[2024-04-05T15:50:43.022] error: Nodes compute04 not responding
[2024-04-05T15:52:27.295] error: Node compute01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-04-05T15:52:27.295] error: Node compute02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-04-05T15:52:27.296] error: Node compute00 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2024-04-05T15:55:43.734] error: Nodes compute04 not responding
[2024-04-05T16:00:43.434] error: Nodes compute04 not responding
[2024-04-05T16:05:43.136] error: Nodes compute04 not responding
[2024-04-05T16:10:43.811] error: Nodes compute04 not responding
[2024-04-05T16:11:28.048] Terminate signal (SIGINT or SIGTERM) received
[2024-04-05T16:11:28.126] Saving all slurm state
[2024-04-08T13:34:32.870] error: chdir(/var/log): Permission denied
[2024-04-08T13:34:32.881] error: Configured MailProg is invalid
[2024-04-08T13:34:32.882] slurmctld version 22.05.11 started on cluster cluster
[2024-04-08T13:34:33.068] No memory enforcing mechanism configured.
[2024-04-08T13:34:33.096] Recovered state of 5 nodes
[2024-04-08T13:34:33.096] Down nodes: compute03
[2024-04-08T13:34:33.096] Recovered information about 0 jobs
[2024-04-08T13:34:33.096] select/cons_res: part_data_create_array: select/cons_res: preparing for 4 partitions
[2024-04-08T13:34:33.106] Recovered state of 0 reservations
[2024-04-08T13:34:33.106] read_slurm_conf: backup_controller not specified
[2024-04-08T13:34:33.106] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure
[2024-04-08T13:34:33.106] select/cons_res: part_data_create_array: select/cons_res: preparing for 4 partitions
[2024-04-08T13:34:33.106] Running as primary controller
[2024-04-08T13:34:33.107] No parameter for mcs plugin, default values set
[2024-04-08T13:34:33.107] mcs: MCSParameters = (null). ondemand set.
[2024-04-08T13:35:33.244] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-04-08T13:36:17.341] error: Nodes compute[00-02] not responding, setting DOWN
[2024-04-08T13:39:33.777] error: Nodes compute04 not responding
[2024-04-08T13:44:33.437] error: Nodes compute04 not responding
[2024-04-08T13:49:33.119] error: Nodes compute04 not responding
[2024-04-08T13:54:33.795] error: Nodes compute04 not responding
[2024-04-08T13:59:33.467] error: Nodes compute04 not responding
[2024-04-08T14:04:33.129] error: Nodes compute04 not responding
[2024-04-08T14:09:33.810] error: Nodes compute04 not responding
[2024-04-08T14:14:33.482] error: Nodes compute04 not responding
[2024-04-08T14:19:33.150] error: Nodes compute04 not responding
[2024-04-08T14:24:33.893] error: Nodes compute04 not responding
[2024-04-08T14:29:33.591] error: Nodes compute04 not responding
[2024-04-08T14:34:33.346] error: Nodes compute04 not responding
[2024-04-08T14:39:33.045] error: Nodes compute04 not responding
[2024-04-08T14:44:33.710] error: Nodes compute04 not responding
[2024-04-08T14:49:33.402] error: Nodes compute04 not responding
[2024-04-08T14:54:33.100] error: Nodes compute04 not responding
[2024-04-08T14:59:33.797] error: Nodes compute04 not responding
[2024-04-08T15:04:33.498] error: Nodes compute04 not responding
[2024-04-08T15:09:33.206] error: Nodes compute04 not responding
[2024-04-08T15:14:33.914] error: Nodes compute04 not responding
[2024-04-08T15:19:33.607] error: Nodes compute04 not responding
[2024-04-08T15:24:33.321] error: Nodes compute04 not responding
[2024-04-08T15:29:33.037] error: Nodes compute04 not responding
[2024-04-08T15:34:33.735] error: Nodes compute04 not responding
[2024-04-08T15:39:33.455] error: Nodes compute04 not responding
[2024-04-08T15:44:33.143] error: Nodes compute04 not responding
[2024-04-08T15:49:33.829] error: Nodes compute04 not responding
[2024-04-08T15:54:33.510] error: Nodes compute04 not responding
[2024-04-08T15:59:33.194] error: Nodes compute04 not responding
[2024-04-08T16:04:33.869] error: Nodes compute04 not responding
[2024-04-08T16:09:33.545] error: Nodes compute04 not responding
[2024-04-08T16:14:33.223] error: Nodes compute04 not responding
[2024-04-08T16:15:42.964] Node compute02 now responding
[2024-04-08T16:15:42.964] validate_node_specs: Node compute02 unexpectedly rebooted boot_time=1712598209 last response=1712347747
[2024-04-08T16:15:42.965] Node compute01 now responding
[2024-04-08T16:15:42.966] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1712598211 last response=1712347747
[2024-04-08T16:15:42.967] Node compute00 now responding
[2024-04-08T16:15:42.967] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1712598141 last response=1712347747
[2024-04-08T16:15:42.993] Node compute04 now responding
[2024-04-08T16:17:36.252] update_node: node compute00 state set to IDLE
[2024-04-08T16:17:36.659] Node compute00 now responding
[2024-04-08T16:17:40.690] update_node: node compute01 state set to IDLE
[2024-04-08T16:17:41.669] Node compute01 now responding
[2024-04-08T16:17:44.441] update_node: node compute02 state set to IDLE
[2024-04-08T16:17:44.675] Node compute02 now responding
[2024-04-08T16:19:33.911] error: Nodes compute04 not responding
[2024-04-08T16:27:44.003] error: Nodes compute02 not responding, setting DOWN
[2024-04-08T16:53:06.467] Node compute02 now responding
[2024-04-08T16:53:06.467] validate_node_specs: Node compute02 unexpectedly rebooted boot_time=1712607851 last response=1712607664
[2024-04-08T16:53:06.468] Node compute03 now responding
[2024-04-08T16:53:06.468] validate_node_specs: Node compute03 unexpectedly rebooted boot_time=1712609271 last response=1712334006
[2024-04-08T16:53:09.515] update_node: node compute02 state set to IDLE
[2024-04-08T16:53:10.467] Node compute02 now responding
[2024-04-08T16:53:13.061] update_node: node compute03 state set to IDLE
[2024-04-08T16:53:13.473] Node compute03 now responding
[2024-04-08T17:44:33.434] error: Nodes compute[03-04] not responding
[2024-04-08T17:46:33.724] error: Nodes compute[03-04] not responding, setting DOWN
[2024-04-09T07:23:35.205] _slurm_rpc_submit_batch_job: JobId=433 InitPrio=4294901759 usec=135
[2024-04-09T07:25:33.079] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=433 uid 1041
[2024-04-09T07:25:42.359] _slurm_rpc_submit_batch_job: JobId=434 InitPrio=4294901758 usec=129
[2024-04-09T07:26:26.449] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=434 uid 1041
[2024-04-09T07:26:34.051] _slurm_rpc_submit_batch_job: JobId=435 InitPrio=4294901757 usec=116
[2024-04-09T07:30:18.409] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=435 uid 1041
[2024-04-09T07:30:25.514] _slurm_rpc_submit_batch_job: JobId=436 InitPrio=4294901756 usec=145
[2024-04-09T07:30:25.975] sched: Allocate JobId=436 NodeList=compute00 #CPUs=1 Partition=express
[2024-04-09T07:30:26.054] _job_complete: JobId=436 WEXITSTATUS 0
[2024-04-09T07:30:26.054] _job_complete: JobId=436 done
[2024-04-09T07:33:00.511] _slurm_rpc_submit_batch_job: JobId=437 InitPrio=4294901755 usec=137
[2024-04-09T07:33:01.136] sched/backfill: _start_job: Started JobId=437 in express on compute00
[2024-04-09T07:33:01.206] _job_complete: JobId=437 WEXITSTATUS 0
[2024-04-09T07:33:01.206] _job_complete: JobId=437 done
[2024-04-09T07:34:33.902] _slurm_rpc_submit_batch_job: JobId=438 InitPrio=4294901754 usec=142
[2024-04-09T07:34:34.144] sched/backfill: _start_job: Started JobId=438 in express on compute00
[2024-04-09T07:34:34.215] _job_complete: JobId=438 WEXITSTATUS 0
[2024-04-09T07:34:34.215] _job_complete: JobId=438 done
[2024-04-09T07:36:26.345] _slurm_rpc_submit_batch_job: JobId=439 InitPrio=4294901753 usec=144
[2024-04-09T07:36:26.681] sched: Allocate JobId=439 NodeList=compute00 #CPUs=2 Partition=express
[2024-04-09T07:36:26.754] _job_complete: JobId=439 WEXITSTATUS 0
[2024-04-09T07:36:26.754] _job_complete: JobId=439 done
[2024-04-09T07:36:37.458] _slurm_rpc_submit_batch_job: JobId=440 InitPrio=4294901752 usec=138
[2024-04-09T07:36:38.703] sched: Allocate JobId=440 NodeList=compute00 #CPUs=2 Partition=express
[2024-04-09T07:36:38.772] _job_complete: JobId=440 WEXITSTATUS 0
[2024-04-09T07:36:38.772] _job_complete: JobId=440 done
[2024-04-09T07:36:45.697] _slurm_rpc_submit_batch_job: JobId=441 InitPrio=4294901751 usec=139
[2024-04-09T07:36:45.717] sched: Allocate JobId=441 NodeList=compute00 #CPUs=2 Partition=express
[2024-04-09T07:36:45.788] _job_complete: JobId=441 WEXITSTATUS 0
[2024-04-09T07:36:45.788] _job_complete: JobId=441 done
[2024-04-09T07:37:07.613] _slurm_rpc_submit_batch_job: JobId=442 InitPrio=4294901750 usec=152
[2024-04-09T07:37:07.757] sched: Allocate JobId=442 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:37:07.841] _job_complete: JobId=442 WEXITSTATUS 0
[2024-04-09T07:37:07.841] _job_complete: JobId=442 done
[2024-04-09T07:37:14.648] _slurm_rpc_submit_batch_job: JobId=443 InitPrio=4294901749 usec=140
[2024-04-09T07:37:14.770] sched: Allocate JobId=443 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:37:14.839] _job_complete: JobId=443 WEXITSTATUS 0
[2024-04-09T07:37:14.839] _job_complete: JobId=443 done
[2024-04-09T07:39:51.640] _slurm_rpc_submit_batch_job: JobId=444 InitPrio=4294901748 usec=135
[2024-04-09T07:39:52.091] sched: Allocate JobId=444 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:39:52.165] _job_complete: JobId=444 WEXITSTATUS 0
[2024-04-09T07:39:52.165] _job_complete: JobId=444 done
[2024-04-09T07:39:57.170] _slurm_rpc_submit_batch_job: JobId=445 InitPrio=4294901747 usec=148
[2024-04-09T07:39:58.101] sched: Allocate JobId=445 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:39:58.172] _job_complete: JobId=445 WEXITSTATUS 0
[2024-04-09T07:39:58.173] _job_complete: JobId=445 done
[2024-04-09T07:42:41.276] _slurm_rpc_submit_batch_job: JobId=446 InitPrio=4294901746 usec=147
[2024-04-09T07:42:41.419] sched: Allocate JobId=446 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:42:41.503] _job_complete: JobId=446 WEXITSTATUS 0
[2024-04-09T07:42:41.503] _job_complete: JobId=446 done
[2024-04-09T07:42:45.771] _slurm_rpc_submit_batch_job: JobId=447 InitPrio=4294901745 usec=175
[2024-04-09T07:42:46.428] sched: Allocate JobId=447 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:42:46.500] _job_complete: JobId=447 WEXITSTATUS 0
[2024-04-09T07:42:46.500] _job_complete: JobId=447 done
[2024-04-09T07:43:15.396] _slurm_rpc_submit_batch_job: JobId=448 InitPrio=4294901744 usec=134
[2024-04-09T07:43:15.485] sched: Allocate JobId=448 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:43:15.533] _job_complete: JobId=448 WEXITSTATUS 0
[2024-04-09T07:43:15.533] _job_complete: JobId=448 done
[2024-04-09T07:43:23.396] _slurm_rpc_submit_batch_job: JobId=449 InitPrio=4294901743 usec=136
[2024-04-09T07:43:23.501] sched: Allocate JobId=449 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:43:23.569] _job_complete: JobId=449 WEXITSTATUS 0
[2024-04-09T07:43:23.569] _job_complete: JobId=449 done
[2024-04-09T07:43:28.004] _slurm_rpc_submit_batch_job: JobId=450 InitPrio=4294901742 usec=136
[2024-04-09T07:43:28.510] sched: Allocate JobId=450 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:43:28.580] _job_complete: JobId=450 WEXITSTATUS 0
[2024-04-09T07:43:28.580] _job_complete: JobId=450 done
[2024-04-09T07:54:15.492] _slurm_rpc_submit_batch_job: JobId=451 InitPrio=4294901741 usec=161
[2024-04-09T07:54:15.911] sched: Allocate JobId=451 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:54:15.984] _job_complete: JobId=451 WEXITSTATUS 0
[2024-04-09T07:54:15.984] _job_complete: JobId=451 done
[2024-04-09T07:54:19.807] _slurm_rpc_submit_batch_job: JobId=452 InitPrio=4294901740 usec=134
[2024-04-09T07:54:19.920] sched: Allocate JobId=452 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:54:19.989] _job_complete: JobId=452 WEXITSTATUS 0
[2024-04-09T07:54:19.989] _job_complete: JobId=452 done
[2024-04-09T07:55:39.240] _slurm_rpc_submit_batch_job: JobId=453 InitPrio=4294901739 usec=145
[2024-04-09T07:55:39.276] sched/backfill: _start_job: Started JobId=453 in express on compute00
[2024-04-09T07:55:39.323] _job_complete: JobId=453 WEXITSTATUS 0
[2024-04-09T07:55:39.323] _job_complete: JobId=453 done
[2024-04-09T07:55:44.365] _slurm_rpc_submit_batch_job: JobId=454 InitPrio=4294901738 usec=141
[2024-04-09T07:55:45.128] sched: Allocate JobId=454 NodeList=compute00 #CPUs=2 Partition=express
[2024-04-09T07:55:45.198] _job_complete: JobId=454 WEXITSTATUS 0
[2024-04-09T07:55:45.198] _job_complete: JobId=454 done
[2024-04-09T07:55:59.121] _slurm_rpc_submit_batch_job: JobId=455 InitPrio=4294901737 usec=133
[2024-04-09T07:55:59.160] sched: Allocate JobId=455 NodeList=compute00 #CPUs=3 Partition=express
[2024-04-09T07:55:59.208] _job_complete: JobId=455 WEXITSTATUS 0
[2024-04-09T07:55:59.208] _job_complete: JobId=455 done
[2024-04-09T07:59:02.424] _slurm_rpc_submit_batch_job: JobId=456 InitPrio=4294901736 usec=134
[2024-04-09T07:59:02.563] sched: Allocate JobId=456 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:59:02.650] _job_complete: JobId=456 WEXITSTATUS 0
[2024-04-09T07:59:02.650] _job_complete: JobId=456 done
[2024-04-09T07:59:08.453] _slurm_rpc_submit_batch_job: JobId=457 InitPrio=4294901735 usec=139
[2024-04-09T07:59:08.577] sched: Allocate JobId=457 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:59:08.647] _job_complete: JobId=457 WEXITSTATUS 0
[2024-04-09T07:59:08.647] _job_complete: JobId=457 done
[2024-04-09T07:59:12.979] _slurm_rpc_submit_batch_job: JobId=458 InitPrio=4294901734 usec=135
[2024-04-09T07:59:13.588] sched: Allocate JobId=458 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T07:59:13.657] _job_complete: JobId=458 WEXITSTATUS 0
[2024-04-09T07:59:13.657] _job_complete: JobId=458 done
[2024-04-09T07:59:24.476] _slurm_rpc_submit_batch_job: JobId=459 InitPrio=4294901733 usec=134
[2024-04-09T07:59:24.611] sched: Allocate JobId=459 NodeList=compute00 #CPUs=8 Partition=express
[2024-04-09T07:59:24.704] _job_complete: JobId=459 WEXITSTATUS 0
[2024-04-09T07:59:24.704] _job_complete: JobId=459 done
[2024-04-09T07:59:33.000] _slurm_rpc_submit_batch_job: JobId=460 InitPrio=4294901732 usec=136
[2024-04-09T07:59:33.293] sched/backfill: _start_job: Started JobId=460 in express on compute00
[2024-04-09T07:59:33.363] _job_complete: JobId=460 WEXITSTATUS 0
[2024-04-09T07:59:33.363] _job_complete: JobId=460 done
[2024-04-09T08:04:59.684] _slurm_rpc_submit_batch_job: JobId=461 InitPrio=4294901731 usec=135
[2024-04-09T08:05:00.428] sched: Allocate JobId=461 NodeList=compute00 #CPUs=6 Partition=express
[2024-04-09T08:05:00.497] _job_complete: JobId=461 WEXITSTATUS 0
[2024-04-09T08:05:00.497] _job_complete: JobId=461 done
[2024-04-09T08:05:40.272] _slurm_rpc_submit_batch_job: JobId=462 InitPrio=4294901730 usec=130
[2024-04-09T08:05:40.329] sched/backfill: _start_job: Started JobId=462 in express on compute00
[2024-04-09T08:05:40.377] _job_complete: JobId=462 WEXITSTATUS 0
[2024-04-09T08:05:40.377] _job_complete: JobId=462 done
[2024-04-09T08:07:03.854] _slurm_rpc_submit_batch_job: JobId=463 InitPrio=4294901729 usec=149
[2024-04-09T08:07:04.711] sched: Allocate JobId=463 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T08:07:04.781] _job_complete: JobId=463 WEXITSTATUS 0
[2024-04-09T08:07:04.781] _job_complete: JobId=463 done
[2024-04-09T08:07:36.029] _slurm_rpc_submit_batch_job: JobId=464 InitPrio=4294901728 usec=134
[2024-04-09T08:07:38.786] sched: Allocate JobId=464 NodeList=compute00 #CPUs=4 Partition=express
[2024-04-09T08:07:38.836] _job_complete: JobId=464 WEXITSTATUS 0
[2024-04-09T08:07:38.837] _job_complete: JobId=464 done
[2024-04-09T08:11:18.532] sched: _slurm_rpc_allocate_resources JobId=465 NodeList=(null) usec=139
[2024-04-09T08:11:29.983] _job_complete: JobId=465 WTERMSIG 126
[2024-04-09T08:11:29.983] _job_complete: JobId=465 cancelled by interactive user
[2024-04-09T08:11:29.983] _job_complete: JobId=465 done
[2024-04-09T08:11:29.983] _slurm_rpc_complete_job_allocation: JobId=465 error Job/step already completing or completed
[2024-04-09T08:11:50.723] sched: _slurm_rpc_allocate_resources JobId=466 NodeList=compute00 usec=156
[2024-04-09T08:12:12.235] _job_complete: JobId=466 WEXITSTATUS 0
[2024-04-09T08:12:12.235] _job_complete: JobId=466 done
[2024-04-09T08:12:18.280] sched: _slurm_rpc_allocate_resources JobId=467 NodeList=compute00 usec=158
[2024-04-09T08:12:33.902] _job_complete: JobId=467 WEXITSTATUS 0
[2024-04-09T08:12:33.902] _job_complete: JobId=467 done
[2024-04-09T08:12:42.997] sched: _slurm_rpc_allocate_resources JobId=468 NodeList=compute00 usec=151
[2024-04-09T08:13:08.294] _job_complete: JobId=468 WEXITSTATUS 0
[2024-04-09T08:13:08.294] _job_complete: JobId=468 done
[2024-04-09T08:13:41.643] _slurm_rpc_submit_batch_job: JobId=469 InitPrio=4294901723 usec=136
[2024-04-09T08:13:42.541] sched: Allocate JobId=469 NodeList=compute00 #CPUs=8 Partition=express
[2024-04-09T08:13:42.998] _job_complete: JobId=469 WEXITSTATUS 0
[2024-04-09T08:13:42.999] _job_complete: JobId=469 done
[2024-04-09T08:13:47.511] _slurm_rpc_submit_batch_job: JobId=470 InitPrio=4294901722 usec=152
[2024-04-09T08:13:47.548] sched: Allocate JobId=470 NodeList=compute00 #CPUs=8 Partition=express
[2024-04-09T08:13:47.984] _job_complete: JobId=470 WEXITSTATUS 0
[2024-04-09T08:13:47.985] _job_complete: JobId=470 done
[2024-04-09T08:13:51.448] _slurm_rpc_submit_batch_job: JobId=471 InitPrio=4294901721 usec=130
[2024-04-09T08:13:51.554] sched: Allocate JobId=471 NodeList=compute00 #CPUs=8 Partition=express
[2024-04-09T08:13:51.992] _job_complete: JobId=471 WEXITSTATUS 0
[2024-04-09T08:13:51.992] _job_complete: JobId=471 done
[2024-04-09T09:13:30.689] Node compute03 now responding
[2024-04-09T09:13:30.689] validate_node_specs: Node compute03 unexpectedly rebooted boot_time=1712612530 last response=1712612393
[2024-04-09T09:13:47.972] update_node: node compute03 state set to IDLE
[2024-04-09T09:13:48.441] Node compute03 now responding
[2024-04-09T10:21:04.857] _slurm_rpc_submit_batch_job: JobId=472 InitPrio=4294901720 usec=128
[2024-04-09T10:21:07.538] _slurm_rpc_submit_batch_job: JobId=473 InitPrio=4294901719 usec=133
[2024-04-09T10:21:12.092] _slurm_rpc_submit_batch_job: JobId=474 InitPrio=4294901718 usec=129
[2024-04-09T10:22:05.458] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=473 uid 1030
[2024-04-09T10:22:05.866] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=473 uid 1041
[2024-04-09T10:22:05.866] error: Security violation, REQUEST_KILL_JOB RPC for JobId=473 from uid 1041
[2024-04-09T10:22:05.866] _slurm_rpc_kill_job: job_str_signal() uid=1041 JobId=473 sig=9 returned: Access/permission denied
[2024-04-09T10:22:18.205] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=472 uid 1041
[2024-04-09T10:22:36.651] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=474 uid 1009
[2024-04-09T10:22:53.577] _slurm_rpc_submit_batch_job: JobId=475 InitPrio=4294901717 usec=152
[2024-04-09T10:22:54.241] sched: Allocate JobId=475 NodeList=compute00 #CPUs=1 Partition=normal
[2024-04-09T10:22:54.312] _job_complete: JobId=475 WEXITSTATUS 0
[2024-04-09T10:22:54.312] _job_complete: JobId=475 done
[2024-04-09T10:23:01.269] _slurm_rpc_submit_batch_job: JobId=476 InitPrio=4294901716 usec=150
[2024-04-09T10:23:02.251] sched: Allocate JobId=476 NodeList=compute00 #CPUs=1 Partition=normal
[2024-04-09T10:23:02.319] _job_complete: JobId=476 WEXITSTATUS 0
[2024-04-09T10:23:02.319] _job_complete: JobId=476 done
[2024-04-09T10:24:17.177] _slurm_rpc_submit_batch_job: JobId=477 InitPrio=4294901715 usec=132
[2024-04-09T10:24:37.400] _slurm_rpc_submit_batch_job: JobId=478 InitPrio=4294901714 usec=145
[2024-04-09T10:24:38.403] sched: Allocate JobId=478 NodeList=compute00 #CPUs=1 Partition=normal
[2024-04-09T10:24:38.453] _job_complete: JobId=478 WEXITSTATUS 0
[2024-04-09T10:24:38.454] _job_complete: JobId=478 done
[2024-04-09T10:24:40.510] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=474 uid 1009
[2024-04-09T10:25:13.622] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=477 uid 1037
[2024-04-09T10:25:15.218] _slurm_rpc_submit_batch_job: JobId=479 InitPrio=4294901713 usec=154
[2024-04-09T10:25:15.455] sched: Allocate JobId=479 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:25:15.523] _job_complete: JobId=479 WEXITSTATUS 0
[2024-04-09T10:25:15.523] _job_complete: JobId=479 done
[2024-04-09T10:25:51.754] _slurm_rpc_submit_batch_job: JobId=480 InitPrio=4294901712 usec=152
[2024-04-09T10:25:52.504] sched: Allocate JobId=480 NodeList=compute00 #CPUs=1 Partition=normal
[2024-04-09T10:25:52.583] _job_complete: JobId=480 WEXITSTATUS 0
[2024-04-09T10:25:52.583] _job_complete: JobId=480 done
[2024-04-09T10:26:22.436] _slurm_rpc_submit_batch_job: JobId=481 InitPrio=4294901711 usec=156
[2024-04-09T10:26:22.546] sched: Allocate JobId=481 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:26:22.607] _job_complete: JobId=481 WEXITSTATUS 0
[2024-04-09T10:26:22.607] _job_complete: JobId=481 done
[2024-04-09T10:26:26.583] _slurm_rpc_submit_batch_job: JobId=482 InitPrio=4294901710 usec=152
[2024-04-09T10:26:27.553] sched: Allocate JobId=482 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:26:27.626] _job_complete: JobId=482 WEXITSTATUS 0
[2024-04-09T10:26:27.626] _job_complete: JobId=482 done
[2024-04-09T10:26:30.335] _slurm_rpc_submit_batch_job: JobId=483 InitPrio=4294901709 usec=152
[2024-04-09T10:26:30.558] sched: Allocate JobId=483 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:26:30.630] _job_complete: JobId=483 WEXITSTATUS 0
[2024-04-09T10:26:30.630] _job_complete: JobId=483 done
[2024-04-09T10:27:00.509] _slurm_rpc_submit_batch_job: JobId=484 InitPrio=4294901708 usec=156
[2024-04-09T10:27:00.601] sched: Allocate JobId=484 NodeList=compute00 #CPUs=8 Partition=normal
[2024-04-09T10:27:00.651] _job_complete: JobId=484 WEXITSTATUS 0
[2024-04-09T10:27:00.651] _job_complete: JobId=484 done
[2024-04-09T10:27:05.321] _slurm_rpc_submit_batch_job: JobId=485 InitPrio=4294901707 usec=137
[2024-04-09T10:27:05.607] sched: Allocate JobId=485 NodeList=compute00 #CPUs=8 Partition=normal
[2024-04-09T10:27:05.679] _job_complete: JobId=485 WEXITSTATUS 0
[2024-04-09T10:27:05.679] _job_complete: JobId=485 done
[2024-04-09T10:27:22.472] _slurm_rpc_submit_batch_job: JobId=486 InitPrio=4294901706 usec=141
[2024-04-09T10:27:22.631] sched: Allocate JobId=486 NodeList=compute00 #CPUs=16 Partition=normal
[2024-04-09T10:27:22.722] _job_complete: JobId=486 WEXITSTATUS 0
[2024-04-09T10:27:22.722] _job_complete: JobId=486 done
[2024-04-09T10:27:58.610] _build_node_list: No nodes satisfy JobId=487 requirements in partition normal
[2024-04-09T10:27:58.610] _slurm_rpc_submit_batch_job: Requested node configuration is not available
[2024-04-09T10:28:14.050] _build_node_list: No nodes satisfy JobId=488 requirements in partition normal
[2024-04-09T10:28:14.050] _slurm_rpc_submit_batch_job: Requested node configuration is not available
[2024-04-09T10:30:59.034] _slurm_rpc_submit_batch_job: JobId=489 InitPrio=4294901703 usec=156
[2024-04-09T10:30:59.957] sched: Allocate JobId=489 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:31:00.035] _job_complete: JobId=489 WEXITSTATUS 0
[2024-04-09T10:31:00.035] _job_complete: JobId=489 done
[2024-04-09T10:31:07.258] _slurm_rpc_submit_batch_job: JobId=490 InitPrio=4294901702 usec=153
[2024-04-09T10:31:07.967] sched: Allocate JobId=490 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:31:08.038] _job_complete: JobId=490 WEXITSTATUS 0
[2024-04-09T10:31:08.038] _job_complete: JobId=490 done
[2024-04-09T10:31:35.170] _slurm_rpc_submit_batch_job: JobId=491 InitPrio=4294901701 usec=144
[2024-04-09T10:31:35.994] sched/backfill: _start_job: Started JobId=491 in normal on compute00
[2024-04-09T10:31:36.065] _job_complete: JobId=491 WEXITSTATUS 0
[2024-04-09T10:31:36.065] _job_complete: JobId=491 done
[2024-04-09T10:31:39.540] _slurm_rpc_submit_batch_job: JobId=492 InitPrio=4294901700 usec=144
[2024-04-09T10:31:40.005] sched: Allocate JobId=492 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:31:40.057] _job_complete: JobId=492 WEXITSTATUS 0
[2024-04-09T10:31:40.057] _job_complete: JobId=492 done
[2024-04-09T10:31:44.806] _slurm_rpc_submit_batch_job: JobId=493 InitPrio=4294901699 usec=145
[2024-04-09T10:31:45.011] sched: Allocate JobId=493 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:31:45.061] _job_complete: JobId=493 WEXITSTATUS 0
[2024-04-09T10:31:45.061] _job_complete: JobId=493 done
[2024-04-09T10:32:14.868] _slurm_rpc_submit_batch_job: JobId=494 InitPrio=4294901698 usec=154
[2024-04-09T10:32:15.051] sched: Allocate JobId=494 NodeList=compute00 #CPUs=1 Partition=normal
[2024-04-09T10:32:15.117] _job_complete: JobId=494 WEXITSTATUS 0
[2024-04-09T10:32:15.117] _job_complete: JobId=494 done
[2024-04-09T10:32:37.505] _slurm_rpc_submit_batch_job: JobId=495 InitPrio=4294901697 usec=155
[2024-04-09T10:32:38.075] sched: Allocate JobId=495 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:32:38.148] _job_complete: JobId=495 WEXITSTATUS 0
[2024-04-09T10:32:38.148] _job_complete: JobId=495 done
[2024-04-09T10:40:41.134] _slurm_rpc_submit_batch_job: JobId=496 InitPrio=4294901696 usec=142
[2024-04-09T10:40:41.746] sched: Allocate JobId=496 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:40:41.818] _job_complete: JobId=496 WEXITSTATUS 0
[2024-04-09T10:40:41.819] _job_complete: JobId=496 done
[2024-04-09T10:40:54.064] _slurm_rpc_submit_batch_job: JobId=497 InitPrio=4294901695 usec=154
[2024-04-09T10:40:54.761] sched: Allocate JobId=497 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:40:54.844] _job_complete: JobId=497 WEXITSTATUS 0
[2024-04-09T10:40:54.844] _job_complete: JobId=497 done
[2024-04-09T10:41:10.719] _slurm_rpc_submit_batch_job: JobId=498 InitPrio=4294901694 usec=155
[2024-04-09T10:41:10.780] sched: Allocate JobId=498 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:41:10.830] _job_complete: JobId=498 WEXITSTATUS 0
[2024-04-09T10:41:10.830] _job_complete: JobId=498 done
[2024-04-09T10:41:15.443] _slurm_rpc_submit_batch_job: JobId=499 InitPrio=4294901693 usec=144
[2024-04-09T10:41:15.785] sched: Allocate JobId=499 NodeList=compute00 #CPUs=10 Partition=normal
[2024-04-09T10:41:15.881] _job_complete: JobId=499 WEXITSTATUS 0
[2024-04-09T10:41:15.881] _job_complete: JobId=499 done
[2024-04-09T10:41:19.397] _slurm_rpc_submit_batch_job: JobId=500 InitPrio=4294901692 usec=147
[2024-04-09T10:41:19.789] sched: Allocate JobId=500 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:41:19.861] _job_complete: JobId=500 WEXITSTATUS 0
[2024-04-09T10:41:19.861] _job_complete: JobId=500 done
[2024-04-09T10:41:21.251] _slurm_rpc_submit_batch_job: JobId=501 InitPrio=4294901691 usec=151
[2024-04-09T10:41:22.488] _slurm_rpc_submit_batch_job: JobId=502 InitPrio=4294901690 usec=158
[2024-04-09T10:41:22.792] sched: Allocate JobId=501 NodeList=compute00 #CPUs=10 Partition=normal
[2024-04-09T10:41:22.792] sched: Allocate JobId=502 NodeList=compute01 #CPUs=10 Partition=normal
[2024-04-09T10:41:22.873] _job_complete: JobId=502 WEXITSTATUS 0
[2024-04-09T10:41:22.873] _job_complete: JobId=502 done
[2024-04-09T10:41:22.881] _job_complete: JobId=501 WEXITSTATUS 0
[2024-04-09T10:41:22.881] _job_complete: JobId=501 done
[2024-04-09T10:41:25.093] _slurm_rpc_submit_batch_job: JobId=503 InitPrio=4294901689 usec=148
[2024-04-09T10:41:25.795] sched: Allocate JobId=503 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:41:25.866] _job_complete: JobId=503 WEXITSTATUS 0
[2024-04-09T10:41:25.866] _job_complete: JobId=503 done
[2024-04-09T10:41:37.381] _slurm_rpc_submit_batch_job: JobId=504 InitPrio=4294901688 usec=155
[2024-04-09T10:41:38.812] sched: Allocate JobId=504 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-09T10:41:38.882] _job_complete: JobId=504 WEXITSTATUS 0
[2024-04-09T10:41:38.882] _job_complete: JobId=504 done
[2024-04-09T10:41:44.823] _slurm_rpc_submit_batch_job: JobId=505 InitPrio=4294901687 usec=147
[2024-04-09T10:41:45.820] sched: Allocate JobId=505 NodeList=compute00 #CPUs=16 Partition=normal
[2024-04-09T10:41:45.903] _job_complete: JobId=505 WEXITSTATUS 0
[2024-04-09T10:41:45.903] _job_complete: JobId=505 done
[2024-04-10T13:43:02.645] _slurm_rpc_submit_batch_job: JobId=506 InitPrio=4294901686 usec=133
[2024-04-10T13:43:03.145] sched: Allocate JobId=506 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-10T13:43:03.220] _job_complete: JobId=506 WEXITSTATUS 0
[2024-04-10T13:43:03.220] _job_complete: JobId=506 done
[2024-04-10T13:43:15.050] _slurm_rpc_submit_batch_job: JobId=507 InitPrio=4294901685 usec=140
[2024-04-10T13:43:15.166] sched: Allocate JobId=507 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-10T13:43:15.222] _job_complete: JobId=507 WEXITSTATUS 0
[2024-04-10T13:43:15.222] _job_complete: JobId=507 done
[2024-04-10T13:43:56.821] _slurm_rpc_submit_batch_job: JobId=508 InitPrio=4294901684 usec=143
[2024-04-10T13:43:57.231] sched: Allocate JobId=508 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-10T13:43:57.305] _job_complete: JobId=508 WEXITSTATUS 0
[2024-04-10T13:43:57.305] _job_complete: JobId=508 done
[2024-04-10T13:46:06.016] _slurm_rpc_submit_batch_job: JobId=509 InitPrio=4294901683 usec=144
[2024-04-10T13:46:06.498] sched: Allocate JobId=509 NodeList=compute00 #CPUs=1 Partition=normal
[2024-04-10T13:46:06.567] _job_complete: JobId=509 WEXITSTATUS 0
[2024-04-10T13:46:06.567] _job_complete: JobId=509 done
[2024-04-10T13:46:22.024] _slurm_rpc_submit_batch_job: JobId=510 InitPrio=4294901682 usec=153
[2024-04-10T13:46:22.524] sched: Allocate JobId=510 NodeList=compute00 #CPUs=1 Partition=normal
[2024-04-10T13:46:22.593] _job_complete: JobId=510 WEXITSTATUS 0
[2024-04-10T13:46:22.593] _job_complete: JobId=510 done
[2024-04-10T13:46:35.291] _slurm_rpc_submit_batch_job: JobId=511 InitPrio=4294901681 usec=157
[2024-04-10T13:46:35.545] sched: Allocate JobId=511 NodeList=compute00 #CPUs=1 Partition=normal
[2024-04-10T13:46:35.614] _job_complete: JobId=511 WEXITSTATUS 0
[2024-04-10T13:46:35.614] _job_complete: JobId=511 done
[2024-04-10T13:47:08.383] _slurm_rpc_submit_batch_job: JobId=512 InitPrio=4294901680 usec=146
[2024-04-10T13:47:08.592] sched: Allocate JobId=512 NodeList=compute00 #CPUs=4 Partition=normal
[2024-04-10T13:47:08.665] _job_complete: JobId=512 WEXITSTATUS 0
[2024-04-10T13:47:08.665] _job_complete: JobId=512 done
[2024-04-10T13:47:32.476] _slurm_rpc_submit_batch_job: JobId=513 InitPrio=4294901679 usec=170
[2024-04-10T13:47:32.630] sched: Allocate JobId=513 NodeList=compute00 #CPUs=8 Partition=normal
[2024-04-10T13:47:32.707] _job_complete: JobId=513 WEXITSTATUS 0
[2024-04-10T13:47:32.707] _job_complete: JobId=513 done
[2024-04-10T13:48:56.576] _slurm_rpc_submit_batch_job: JobId=514 InitPrio=4294901678 usec=139
[2024-04-10T13:48:56.795] sched: Allocate JobId=514 NodeList=compute00 #CPUs=8 Partition=normal
[2024-04-10T13:48:56.866] _job_complete: JobId=514 WEXITSTATUS 0
[2024-04-10T13:48:56.866] _job_complete: JobId=514 done
[2024-04-10T13:49:59.279] _slurm_rpc_submit_batch_job: JobId=515 InitPrio=4294901677 usec=157
[2024-04-10T13:49:59.942] sched: Allocate JobId=515 NodeList=compute00 #CPUs=8 Partition=normal
[2024-04-10T13:50:00.013] _job_complete: JobId=515 WEXITSTATUS 0
[2024-04-10T13:50:00.013] _job_complete: JobId=515 done
[2024-04-10T13:55:35.335] _slurm_rpc_submit_batch_job: JobId=516 InitPrio=4294901676 usec=135
[2024-04-10T13:55:35.571] sched: Allocate JobId=516 NodeList=compute00 #CPUs=10 Partition=normal
[2024-04-10T13:55:35.655] _job_complete: JobId=516 WEXITSTATUS 0
[2024-04-10T13:55:35.655] _job_complete: JobId=516 done
[2024-04-10T13:55:48.644] _slurm_rpc_submit_batch_job: JobId=517 InitPrio=4294901675 usec=133
[2024-04-10T13:55:49.597] sched: Allocate JobId=517 NodeList=compute00 #CPUs=10 Partition=normal
[2024-04-10T13:55:49.683] _job_complete: JobId=517 WEXITSTATUS 0
[2024-04-10T13:55:49.683] _job_complete: JobId=517 done
[2024-04-12T08:25:57.272] sched: _slurm_rpc_allocate_resources JobId=518 NodeList=(null) usec=131
[2024-04-12T08:26:05.887] _job_complete: JobId=518 WTERMSIG 126
[2024-04-12T08:26:05.887] _job_complete: JobId=518 cancelled by interactive user
[2024-04-12T08:26:05.888] _job_complete: JobId=518 done
[2024-04-12T08:26:05.888] _slurm_rpc_complete_job_allocation: JobId=518 error Job/step already completing or completed
[2024-04-12T08:28:17.896] _slurm_rpc_submit_batch_job: JobId=519 InitPrio=4294901673 usec=140
[2024-04-12T08:30:22.333] _slurm_rpc_submit_batch_job: JobId=520 InitPrio=4294901672 usec=132
[2024-04-12T08:34:22.511] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=520 uid 1034
[2024-04-12T08:34:25.442] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=519 uid 1034
[2024-04-12T08:34:33.167] _slurm_rpc_submit_batch_job: JobId=521 InitPrio=4294901671 usec=119
[2024-04-12T08:36:11.734] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=521 uid 1034
[2024-04-12T08:36:22.568] _slurm_rpc_submit_batch_job: JobId=522 InitPrio=4294901670 usec=121
[2024-04-12T08:36:46.025] _slurm_rpc_submit_batch_job: JobId=523 InitPrio=4294901669 usec=117
[2024-04-12T08:36:54.662] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=522 uid 1034
[2024-04-12T08:36:56.212] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=523 uid 1034
[2024-04-12T08:37:36.674] _slurm_rpc_submit_batch_job: JobId=524 InitPrio=4294901668 usec=117
[2024-04-12T08:39:00.538] _slurm_rpc_submit_batch_job: JobId=525 InitPrio=4294901667 usec=138
[2024-04-12T08:39:00.947] sched: Allocate JobId=525 NodeList=compute00 #CPUs=2 Partition=normal
[2024-04-12T08:39:16.767] _job_complete: JobId=525 WEXITSTATUS 0
[2024-04-12T08:39:16.767] _job_complete: JobId=525 done
[2024-04-12T08:51:23.643] _slurm_rpc_submit_batch_job: JobId=526 InitPrio=4294901666 usec=143
[2024-04-12T08:51:24.130] sched/backfill: _start_job: Started JobId=526 in normal on compute00
[2024-04-12T08:51:44.883] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=524 uid 1034
[2024-04-12T08:51:45.019] _job_complete: JobId=526 WEXITSTATUS 0
[2024-04-12T08:51:45.019] _job_complete: JobId=526 done
[2024-04-12T08:51:47.482] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=526 uid 1034
[2024-04-12T14:11:19.028] agent msg_type=1008 ran for 25 seconds
[2024-04-12T14:16:12.850] error: A node ping cycle took more than 100 seconds. Node RPC requests like ping, register status, health check and/or accounting gather update are triggered less frequently than configured. Either many nodes are non-responsive or one of SlurmdTimeout, HealthCheckInterval, JobAcctGatherFrequency, ExtSensorsFreq should be increased.
[2024-04-12T14:17:14.771] error: slurm_msg_sendto: address:port=10.10.10.103:6818 msg_type=1008: Unexpected missing socket error
[2024-04-12T14:17:14.907] error: slurm_msg_sendto: address:port=10.10.10.100:6818 msg_type=1008: Unexpected missing socket error
[2024-04-12T14:17:14.907] error: slurm_msg_sendto: address:port=10.10.10.102:6818 msg_type=1008: Unexpected missing socket error
[2024-04-12T14:17:14.907] error: slurm_msg_sendto: address:port=10.10.10.101:6818 msg_type=1008: Unexpected missing socket error
[2024-04-12T14:17:15.403] agent msg_type=1008 ran for 172 seconds
[2024-04-12T14:19:54.816] error: A node ping cycle took more than 100 seconds. Node RPC requests like ping, register status, health check and/or accounting gather update are triggered less frequently than configured. Either many nodes are non-responsive or one of SlurmdTimeout, HealthCheckInterval, JobAcctGatherFrequency, ExtSensorsFreq should be increased.
[2024-04-12T14:22:58.792] error: Nodes compute[00-03] not responding
[2024-04-12T14:23:50.201] error: slurm_msg_sendto: address:port=10.10.10.103:6818 msg_type=1008: Unexpected missing socket error
[2024-04-12T14:23:50.219] error: slurm_msg_sendto: address:port=10.10.10.102:6818 msg_type=1008: Unexpected missing socket error
[2024-04-12T14:23:50.219] error: slurm_msg_sendto: address:port=10.10.10.100:6818 msg_type=1008: Unexpected missing socket error
[2024-04-12T14:23:50.222] error: slurm_msg_sendto: address:port=10.10.10.101:6818 msg_type=1008: Unexpected missing socket error
[2024-04-12T14:23:51.099] agent msg_type=1008 ran for 373 seconds
[2024-04-12T14:23:51.748] error: Nodes compute[00-03] not responding, setting DOWN
[2024-04-12T14:47:11.628] Node compute03 now responding
[2024-04-12T14:47:11.628] node compute03 returned to service
[2024-04-12T14:47:11.628] Node compute02 now responding
[2024-04-12T14:47:11.628] node compute02 returned to service
[2024-04-12T14:47:11.629] Node compute01 now responding
[2024-04-12T14:47:11.629] node compute01 returned to service
[2024-04-12T14:47:11.629] Node compute00 now responding
[2024-04-12T14:47:11.629] node compute00 returned to service
